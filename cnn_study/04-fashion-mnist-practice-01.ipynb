{"metadata":{"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30716,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Keras fashion mnist dataset을 다운로드\n* 5만개의 학습용, 1만개의 테스트용 grayscale image array를 다운로드","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras.datasets import fashion_mnist\n\n# 전체 6만개 데이터 중, 5만개는 학습 데이터용, 1만개는 테스트 데이터용으로 분리\n(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()\n# image size는 28x28의 grayscale 2차원 데이터\nprint(\"train dataset shape:\", train_images.shape, train_labels.shape)\nprint(\"test dataset shape:\", test_images.shape, test_labels.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### MNIST image array 시각화","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n%matplotlib inline\n\nplt.imshow(train_images[0], cmap='gray')\nplt.title(train_labels[0])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_images[0, :, :], train_labels[0]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n%matplotlib inline \n\nclass_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat','Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n\ndef show_images(images, labels, ncols=8):\n    figure, axs = plt.subplots(figsize=(22, 6), nrows=1, ncols=ncols)\n    for i in range(ncols):\n        axs[i].imshow(images[i], cmap='gray')\n        axs[i].set_title(class_names[labels[i]])\n        \nshow_images(train_images[:8], train_labels[:8], ncols=8)\nshow_images(train_images[8:16], train_labels[8:16], ncols=8)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 데이터 전처리 수행. \n* 0 ~ 255 사이의 픽셀값을 0 ~ 1 사이 값으로 변환. \n* array type은 float 32","metadata":{}},{"cell_type":"code","source":"(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()\n\ndef get_preprocessed_data(images, labels):\n    \n    # 학습과 테스트 이미지 array를 0~1 사이값으로 scale 및 float32 형 변형. \n    images = np.array(images/255.0, dtype=np.float32)\n    labels = np.array(labels, dtype=np.float32)\n    \n    return images, labels\n\ntrain_images, train_labels = get_preprocessed_data(train_images, train_labels)\ntest_images, test_labels = get_preprocessed_data(test_images, test_labels)\n\nprint(\"train dataset shape:\", train_images.shape, train_labels.shape)\nprint(\"test dataset shape:\", test_images.shape, test_labels.shape)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_images[0]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Dense Layer를 기반으로 모델을 생성","metadata":{}},{"cell_type":"code","source":"INPUT_SIZE = 28","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.layers import Dense, Flatten\nfrom tensorflow.keras.models import Sequential\n\n#케라스의 레이어(케라스가 쉽다고 말하는 이유이다)\nmodel = Sequential([\n    Flatten(input_shape=(INPUT_SIZE, INPUT_SIZE)),  #28x28 입력이 들어오고 flatten에 의해 784개의 1열이 만들어진다.\n    Dense(100, activation='relu'),   #Dense Layer 1번을 생성하고 뉴런 100개를 만든다.(78,400개의 가중치가 생김)\n    Dense(30, activation='relu'),    #Dense Layer 2번을 생성하고 뉴런 30개를 만든다.(3,000개의 가중치)\n    Dense(10, activation='softmax')  #\n])\n\nmodel.summary()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 모델의 Loss와 Optimizer 설정하고 학습 수행\n* loss는 categorical_crossentropy로, optimizer는 Adam으로 설정\n* categorical_crossentropy면 타켓값(레이블)이 반드시 one hot encoding이 되어야한다.\n* one hot encoding을 않쓰고 사용할거라면 sparse_categorical_crossentropy를 사용해야 한다.\n\n원-핫 인코딩 (One-Hot Encoding)\n원-핫 인코딩은 클래스 레이블을 벡터 형태로 변환합니다. 이 벡터는 해당 클래스의 위치만 1이고 나머지는 모두 0입니다. 예를 들어, 클래스가 3개일 때:\n클래스 0: [1, 0, 0]\n클래스 1: [0, 1, 0]\n클래스 2: [0, 0, 1]\n원-핫 인코딩을 사용하면 각 레이블이 벡터로 변환되므로 categorical_crossentropy 손실 함수를 사용할 수 있습니다.\n\n스파스 인코딩 (Sparse Encoding)\n스파스 인코딩에서는 클래스 레이블을 정수 형태로 표현합니다. 즉, 각 클래스는 단일 정수로 표시됩니다. 예를 들어, 클래스가 3개일 때:\n클래스 0: 0\n클래스 1: 1\n클래스 2: 2\n스파스 인코딩을 사용하면 각 레이블이 단일 정수로 표시되므로 sparse_categorical_crossentropy 손실 함수를 사용할 수 있습니다.\n\n\n\n\n* categorical crossentropy를 위해서 Label을 OHE 로 변경","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.losses import CategoricalCrossentropy\nfrom tensorflow.keras.metrics import Accuracy\n\nmodel.compile(optimizer=Adam(0.001), loss='categorical_crossentropy', metrics=['accuracy'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.utils import to_categorical\n\ntrain_oh_labels = to_categorical(train_labels)  #categorical_crossentropy이므로 반드시 one hot encoding\ntest_oh_labels = to_categorical(test_labels)    #categorical_crossentropy이므로 반드시 one hot encoding\n\nprint(train_oh_labels.shape, test_oh_labels.shape)\n\n#one hot encoding을 했기 때문에 결과값이 (60000, 10) (10000, 10) 처럼 2차원으로 나오게 된다.","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_images.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#모델 학습 시작\nhistory = model.fit(x=train_images, y=train_oh_labels, batch_size=32, epochs=20, verbose=1)\n#모델 학습이 끝나면 history라는 객체를 통해 accuracy, loss를 반환한다.","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(history.history['loss'])\nprint(history.history['accuracy'])\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 테스트 데이터를 기반으로 Label 값 예측\n* model.predict()를 이용하여 label값 예측\n* predict()의 인자로 입력되는 feature array는 학습의 feature array와 shape가 동일해야함. \n* fit() 시 3차원(28x28 2차원 array가 여러개 존재) array 입력 했으므로 predict()도 동일한 3차원 데이터 입력\n* 특히 한건만 predict() 할때도 3차원 데이터여야 함. 이를 위해 expand_dims()로 2차원 image 배열을 3차원으로 변경","metadata":{}},{"cell_type":"code","source":"test_images.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pred_proba[0]\n#결과 :\n#array([4.8651878e-09, 3.7216765e-09, 9.6669983e-10, 3.4128553e-07,\n#      1.5375153e-09, 5.4103624e-05, 1.4692477e-08, 1.8135931e-03,\n#      2.8841464e-06, 9.9812907e-01], dtype=float32)\n#10개 중에 가장 높은 확률을 기록한 것은 9.9812907e-01이다.\n#['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat','Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pred_proba = model.predict(test_images) #test_images : 예측할 Image Array\nprint(pred_proba.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#케라스에서도 차원은 항상 일치를 시켜줘야한다.\ntest_images.shape\n#결과 : (10000, 28, 28)\n\n#이미지 1건만 입력해서 결과를 확인할 경우\ntest_images[0].shape\n#결과 : (28, 28)\n#2차원이므로 3차원 변경을 해줘야 한다.\n\nnp.expand_dims(test_images[0], axis=0).shape\n#결과 : (1, 28, 28)\n#맨앞에 차원을 하나 추가하려면 axis=0, 다음에 추가하려면 axis=1, 맨 뒤에 추가하려면 axis=-1","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"np.expand_dims(test_images[0], axis=0).shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pred_proba = model.predict(np.expand_dims(test_images[0], axis=0)) #여기서 model은 학습이 된 모델을 뜻한다.\nprint('softmax output:', pred_proba)\npred = np.argmax(np.squeeze(pred_proba))  #위에서 차원을 강제로 하나 늘렸으니 np.squeeze를 써서 강제로 하나를 줄인다.\nprint('predicted class value:', pred)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat','Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\nprint('target class value:', test_labels[0], 'predicted class value:', pred)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 테스트 데이터 세트로 모델 성능 검증","metadata":{}},{"cell_type":"code","source":"model.evaluate(test_images, test_oh_labels, batch_size=64)\n#학습을 one hot encoding으로 했기 때문에 검증할 때도 one hot encoding된 것을(test_oh_labels) 넣어주어야 한다.","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 검증 데이터 세트를 이용하여 학습 수행. \n* 일반적으로 fit() 수행시 별도의 검증 데이터 세트를 이용하여 학습 시 과적합(Overfitting)이 발생하는지 모니터링\n* fit()을 수행하면 iteration을 반복하기 때문에 중간에 하이퍼파라미터 변경(예: Learning Rate)등의 작업이 어려움. \n* fit() iteration시 여러 작업을 하기 위해 Callback 객체를 가짐. \n* 검증 데이터 세트를 fit() 시 적용하여 과적합이나 더이상 검증 데이터 성능이 좋아 지지 않을 때 Callback을 사용하여 Learning Rate 보정 작업등을 수행 가능","metadata":{}},{"cell_type":"code","source":"import numpy as np \nimport pandas as pd\nfrom tensorflow.keras.datasets import fashion_mnist\n\n(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()\n\ndef get_preprocessed_data(images, labels):\n    \n    # 학습과 테스트 이미지 array를 0~1 사이값으로 scale 및 float32 형 변형. \n    images = np.array(images/255.0, dtype=np.float32)\n    labels = np.array(labels, dtype=np.float32)\n    \n    return images, labels\n\ntrain_images, train_labels = get_preprocessed_data(train_images, train_labels)\ntest_images, test_labels = get_preprocessed_data(test_images, test_labels)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom tensorflow.keras.utils import to_categorical\n\n# 기존 학습 데이터를 다시 학습과 검증 데이터 세트로 분리\n#test_size=0.15 이면 학습 데이터에서 15%를 떼어 검증 세트로 분리하겠다는 뜻이다.\ntr_images, val_images, tr_labels, val_labels = train_test_split(train_images, train_labels, test_size=0.15, random_state=2021)\nprint('train과 validation shape:', tr_images.shape, tr_labels.shape, val_images.shape, val_labels.shape)\n\n# OHE 적용\ntr_oh_labels = to_categorical(tr_labels)\nval_oh_labels = to_categorical(val_labels)\n\nprint('after OHE:', tr_oh_labels.shape, val_oh_labels.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 검증 데이터 세트를 적용하여 학습 수행. ","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras.layers import Dense, Flatten\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.optimizers import Adam\n\nINPUT_SIZE = 28\nmodel = Sequential([\n    Flatten(input_shape=(INPUT_SIZE, INPUT_SIZE)),\n    Dense(100, activation='relu'),\n    Dense(30, activation='relu'),\n    Dense(10, activation='softmax')\n])\n\nmodel.compile(optimizer=Adam(0.001), loss='categorical_crossentropy', metrics=['accuracy'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#validation_data=(피쳐, 타겟)\nhistory = model.fit(x=tr_images, y=tr_oh_labels, batch_size=128, validation_data=(val_images, val_oh_labels), \n                    epochs=20, verbose=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(history.history['loss'])\nprint(history.history['accuracy'])\nprint(history.history['val_loss'])\nprint(history.history['val_accuracy'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n%matplotlib inline\n\nplt.plot(history.history['accuracy'], label='train')\nplt.plot(history.history['val_accuracy'], label='valid')\nplt.legend()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Functional API\n\n* Functional API가 가장 중요하다.\n* Sequential API는 딥러닝을 취미로 하는 수준의 가장 쉬운 정도이다.","metadata":{}},{"cell_type":"code","source":"# Sequential Model을 이용하여 Keras 모델 생성\n# 케라스 초창기에 모델을 만드는 방법.\nfrom tensorflow.keras.layers import Dense, Flatten\nfrom tensorflow.keras.models import Sequential\n\nINPUT_SIZE = 28\n\nmodel = Sequential([\n    Flatten(input_shape=(INPUT_SIZE, INPUT_SIZE)),\n    Dense(100, activation='relu'),\n    Dense(30, activation='relu'),\n    Dense(10, activation='softmax')\n])\n\nmodel.summary()\n\nmodel1 = Sequential()\nmodel1.add(Flatten(input_shape=(INPUT_SIZE, INPUT_SIZE)))\nmodel1.add(Dense(100, activation='relu'))\nmodel1.add(Dense(30, activation='relu'))\nmodel1.add(Dense(10, activation='softmax'))\n\nmodel1.summary()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.layers import Input, Flatten, Dense\n\n#Functional API를 사용하기 위해서 Model을 import 해주어야 한다.\nfrom tensorflow.keras.models import Model\n\n#Functional API에는 반드시 Input이 들어가야 한다.\ninput_tensor = Input(shape=(INPUT_SIZE, INPUT_SIZE))\n\n#2차원을 깔아뭉게는 Layer를 만들고자 Flatten 객체를 만듭니다.\n#Flatten(생성자 파라미터)(인자 : 데이터)\n#즉, 데이터를 입력받아서  Flatten 객체를 통해 n행 1열의 데이터를 만든다.\nx = Flatten()(input_tensor)\n\n#Dense Layer를 만들고 x(Flatten Data)를 입력한다.\n#뉴런의 갯수는 100개이고 활성화 함수는 relu를 사용한다.\nx = Dense(100, activation='relu')(x)\n#뉴런의 갯수는 30개이고 활성화 함수는 relu를 사용한다.\nx = Dense(30, activation='relu')(x)\n#뉴런의 갯수는 10개이고 활성화 함수는 softmax를 사용한다.\noutput = Dense(10, activation='softmax')(x)\n\n#Model객체에 입력은 input_tensor(입력데이터)이고 출력은 output(출력데이터)이다.\nmodel = Model(inputs=input_tensor, outputs=output)\n\nmodel.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Custom한 Dense Layer 생성하기","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras.layers import Layer, Input\nfrom tensorflow.keras.models import Model\nimport tensorflow as tf\n\n#Custom한 Dense Layer을 만드는 방법\n#내부 구조\n#상속 : tf.keras.layers.Layer\n#생성 메소드 : \n#빌드 : \n#콜 : \nclass CustomDense(tf.keras.layers.Layer):\n    # CustomDense 객체 생성시 입력되는 초기화 parameter 처리\n    def __init__(self, units=32):\n        super(CustomDense, self).__init__()\n        self.units = units\n\n    def build(self, input_shape):\n        self.w = self.add_weight(\n            shape=(input_shape[-1], self.units),\n            initializer=\"random_normal\",\n            trainable=True,\n        )\n        self.b = self.add_weight(\n            shape=(self.units,), initializer=\"random_normal\", trainable=True\n        )\n        \n    # CustomDense 객체에 callable로 입력된 입력 데이터 처리. \n    def call(self, inputs):\n        return tf.matmul(inputs, self.w) + self.b\n\n# input 값을 4개의 원소를 가지는 1차원으로 생성. \ninputs = Input((4,))\n\n# 10개의 unit을 가지는 CustomDense 객체를 생성 후 callable로 inputs값 입력 \noutputs = CustomDense(10)(inputs) #10은 생성 메소드의 units가 되고 inputs은 call의 inputs로 간다.\n\n# inputs와 outputs로 model 생성. \nmodel = Model(inputs, outputs)\nmodel.summary()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Functional API는 객체 생성 부분과 Callable 인자 입력 부분을 별도로 수행해도 무방. ","metadata":{}},{"cell_type":"code","source":"inputs = Input((4,))\n# 10개의 unit을 가지는 CustomDense 객체를 생성 후 callable로 inputs값 입력 \nmy_layer = CustomDense(10)\noutputs = my_layer(inputs)\n\n# inputs와 outputs로 model 생성. \nmodel = Model(inputs, outputs)\nmodel.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Sequential Model 생성은 단지 Functional API Layer들을 iteration 하면서 연결한 것을 model로 만든 것임","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras.models import Sequential\n\nmodel = Sequential([Input((4,)),\n                   CustomDense(10),\n                   CustomDense(8), \n                   tf.keras.layers.ReLU()])\nmodel.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Sequential Model을 Functional 객체를 For loop 반복 호출하여 작성.","metadata":{}},{"cell_type":"code","source":"layers_list = [Input((4,)), CustomDense(10), CustomDense(8), tf.keras.layers.ReLU()]\n\nfor index, layer in enumerate(layers_list):\n        print(index, layer)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"layers_list = [Input((4,)), CustomDense(10), CustomDense(8), tf.keras.layers.ReLU()]\n\ninputs = None\ncallable_inputs = None\noutputs = None\n# layers_list에 있는 Functional 객체를 iteration 수행하면서 적용. \nfor index, layer in enumerate(layers_list):\n    # layers_list의 첫번째 인자는 Input 간주. \n    if index == 0:\n        inputs = layer\n        callable_inputs = layer\n    # Functional 객체에 callable 인자로 callable_inputs를 입력하고 반환 결과 값을 다시 callable_inputs로 할당.     \n    else: \n        callable_inputs = layer(callable_inputs)\n    \noutputs = callable_inputs\nmodel = Model(inputs, outputs)\nmodel.summary()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 앞에서 생성한 로직들을 함수화 \n* Functional API로 모델 만들기\n* pixel값 1 ~ 255를 0 ~ 1사이값 Float 32로 만들기\n* One Hot Encoding Label에 적용하기\n* 학습과 검증 데이터로 나누기.\n* compile, 학습/예측/평가","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras.layers import Layer, Input, Dense, Flatten\nfrom tensorflow.keras.models import Model\nimport tensorflow as tf\n\nINPUT_SIZE = 28\n\ndef create_model() :\n    input_tensor = Input(shape=(INPUT_SIZE, INPUT_SIZE))\n    x = Flatten()(input_tensor)\n    x = Dense(100, activation = 'relu')(x)\n    x = Dense(30, activation = 'relu')(x)\n    output = Dense(10, activation = 'softmax')(x)\n    \n    #input에 들어가는 데이터는 여러개일 수 있다(사진, 텍스트, 정형데이터 등)\n    #Output으로 나오는 데이터도 여러개일 수 있다.(주차장에서 주차구획, 식별기둥, 전용구획 등)\n    #따라서 inputs, outputs이다.\n    model = Model(inputs=input_tensor, outputs=output)\n    return model\n\nmodel = create_model()\nmodel.summary()","metadata":{"execution":{"iopub.status.busy":"2024-06-11T09:31:15.224773Z","iopub.execute_input":"2024-06-11T09:31:15.225183Z","iopub.status.idle":"2024-06-11T09:31:15.272372Z","shell.execute_reply.started":"2024-06-11T09:31:15.225152Z","shell.execute_reply":"2024-06-11T09:31:15.271575Z"},"trusted":true},"execution_count":27,"outputs":[{"output_type":"display_data","data":{"text/plain":"\u001b[1mModel: \"functional_19\"\u001b[0m\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_19\"</span>\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n│ input_layer_9 (\u001b[38;5;33mInputLayer\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m)         │             \u001b[38;5;34m0\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ flatten_9 (\u001b[38;5;33mFlatten\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m784\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_27 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m)            │        \u001b[38;5;34m78,500\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_28 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m)             │         \u001b[38;5;34m3,030\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_29 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)             │           \u001b[38;5;34m310\u001b[0m │\n└─────────────────────────────────┴────────────────────────┴───────────────┘\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n│ input_layer_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>)         │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ flatten_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">784</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_27 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">78,500</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_28 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">3,030</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_29 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">310</span> │\n└─────────────────────────────────┴────────────────────────┴───────────────┘\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Total params: \u001b[0m\u001b[38;5;34m81,840\u001b[0m (319.69 KB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">81,840</span> (319.69 KB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m81,840\u001b[0m (319.69 KB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">81,840</span> (319.69 KB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n</pre>\n"},"metadata":{}}]},{"cell_type":"code","source":"import numpy as np\nfrom tensorflow.keras.utils import to_categorical\nfrom sklearn.model_selection import train_test_split\n\n#데이터 전처리를 수행하는 함수\ndef get_preprocessed_data(images, labels): #images로 0~255 사이의 gray scale 값이 들어옴, \n    # 입력받은 이미지 데이터(0~255)를 0 ~ 1사이값으로 조정 후 float32로 변경하는 함수\n    images = np.array(images/255.0, dtype=np.float32)\n    #label의 type을 float32로 변경하는 함수\n    labels = np.array(labels, dtype=np.float32)\n    \n    return images, labels\n\n#OHE 적용 \ndef get_preprocessed_ohe(images, labels):\n    #전처리가 끝난 images와 labels를 불러옴\n    images, labels = get_preprocessed_data(images, labels)\n    #labels에 대해 to_categorical 함수를 실행해서 one hot encoding을 수행\n    oh_labels = to_categorical(labels)\n    \n    return images, oh_labels\n\n#학습/검증/테스트 데이터 세트에 전처리 및 OHE 적용한 뒤 반환 \n#검증데이터 세트를 활용해서 학습을 하는게 일반적인 로직임.\n#이유는 오버피팅도 확인을 해야하고, 차후 콜백을 걸 때 검증 데이터 세트에 거는게 일반적인 로직이다.\ndef get_train_valid_test_set(train_images, train_labels, test_images, test_labels, valid_size=0.15, random_state=2021) :\n    #valid_size : 검증 데이터의 비율 (기본값은 0.15, 즉 15%).\n    #random_state : 데이터 분할 시 랜덤성을 제어하기 위한 시드 값 (기본값은 2021).\n    \n    #학습 이미지와 라벨, 테스트 이미지와 라벨에 대해 one hot encoding을 수행.\n    train_images, train_oh_labels = get_preprocessed_ohe(train_images, train_labels)\n    test_images, test_oh_labels = get_preprocessed_ohe(test_images, test_labels)\n    \n    #train_test_split 함수는 scikit-learn 라이브러리의 함수로, 데이터를 학습용과 검증용으로 나누는 기능을 합니다.\n    #학습용 세트 전체(데이터 및 라벨)(train_images, train_oh_labels)에서 valid_size만큼을 떼서 검증용 세트로 만든다.\n    #이 때 세트 전체에서 랜덤으로 검증 데이터가 뽑히는데 이 계수가 2021이다.\n    #나눠진 데이터 세트 중 학습용 데이터 세트는(tr_images, val_images)이고\n    #떼어진 검증용 데이터 세트는 (tr_oh_labels, val_oh_labels)이다.\n    tr_images, val_images, tr_oh_labels, val_oh_labels = train_test_split(train_images, train_oh_labels, test_size=valid_size, random_state=random_state)\n    \n    #최종적으로 학습용 데이터 세트(이미지, 라벨), 검증용 데이터 세트(이미지, 라벨), 테스트용 데이터 세트(이미지, 라벨)를 반환한다.\n    return(tr_images, tr_oh_labels), (val_images, val_oh_labels), (test_images, test_oh_labels)\n\n#이제 기초 작업은 모두 완료가 됨.","metadata":{"execution":{"iopub.status.busy":"2024-06-11T09:31:16.295767Z","iopub.execute_input":"2024-06-11T09:31:16.296593Z","iopub.status.idle":"2024-06-11T09:31:16.306129Z","shell.execute_reply.started":"2024-06-11T09:31:16.296561Z","shell.execute_reply":"2024-06-11T09:31:16.305096Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.datasets import fashion_mnist\n#이제 데이터 셋을 import해서 get_train_valid_test_set 함수를 돌리면 모델을 수행하기 위해 필요한 학습, 검증, 테스트 데이터셋이 모두 만들어진다.\n\n# Fashion MNIST 데이터 로딩\n(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()\nprint(train_images.shape, train_labels.shape, test_images.shape, test_labels.shape)\n\n# 전처리 적용하여 학습/검증/데이터 세트 생성. \n(tr_images, tr_oh_labels), (val_images, val_oh_labels), (test_images, test_oh_labels) = \\\nget_train_valid_test_set(train_images, train_labels, test_images, test_labels, valid_size=0.15, random_state=2021)\n\nprint(tr_images.shape, tr_oh_labels.shape, val_images.shape, val_oh_labels.shape, test_images.shape, test_oh_labels.shape)","metadata":{"execution":{"iopub.status.busy":"2024-06-11T09:31:19.867672Z","iopub.execute_input":"2024-06-11T09:31:19.868009Z","iopub.status.idle":"2024-06-11T09:31:20.544358Z","shell.execute_reply.started":"2024-06-11T09:31:19.867983Z","shell.execute_reply":"2024-06-11T09:31:20.543390Z"},"trusted":true},"execution_count":29,"outputs":[{"name":"stdout","text":"(60000, 28, 28) (60000,) (10000, 28, 28) (10000,)\n(51000, 28, 28) (51000, 10) (9000, 28, 28) (9000, 10) (10000, 28, 28) (10000, 10)\n","output_type":"stream"}]},{"cell_type":"code","source":"from tensorflow.keras.optimizers import Adam\n\n# Model 생성 및 optimizer, loss, metric 적용\nmodel = create_model()\nmodel.summary()\n\n#model.compile 함수는 모델을 학습할 수 있도록 설정하는 역할을 한다.\n#optimizer: 모델의 가중치를 업데이트하는 방법을 결정한다. Adam을 사용하고 learning rate(학습률)는 0.001로 한다.\n#loss: 모델이 학습할 때 최소화하려고 하는 손실 함수입니다. 데이터셋을 one hot encoding 했으므로 categorical_crossentropy를 사용한다.\n#metrics: 모델의 성능을 평가할 때 사용할 지표입니다. 다양한 성능 평가 지표 중 정확도(accuracy)를 사용합니다.\nmodel.compile(optimizer=Adam(0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n\n\n","metadata":{"execution":{"iopub.status.busy":"2024-06-11T09:31:23.904763Z","iopub.execute_input":"2024-06-11T09:31:23.905399Z","iopub.status.idle":"2024-06-11T09:31:23.953072Z","shell.execute_reply.started":"2024-06-11T09:31:23.905371Z","shell.execute_reply":"2024-06-11T09:31:23.952374Z"},"trusted":true},"execution_count":30,"outputs":[{"output_type":"display_data","data":{"text/plain":"\u001b[1mModel: \"functional_21\"\u001b[0m\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_21\"</span>\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n│ input_layer_10 (\u001b[38;5;33mInputLayer\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m)         │             \u001b[38;5;34m0\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ flatten_10 (\u001b[38;5;33mFlatten\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m784\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_30 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m)            │        \u001b[38;5;34m78,500\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_31 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m)             │         \u001b[38;5;34m3,030\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_32 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)             │           \u001b[38;5;34m310\u001b[0m │\n└─────────────────────────────────┴────────────────────────┴───────────────┘\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n│ input_layer_10 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>)         │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ flatten_10 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">784</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_30 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">78,500</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_31 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">3,030</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_32 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">310</span> │\n└─────────────────────────────────┴────────────────────────┴───────────────┘\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Total params: \u001b[0m\u001b[38;5;34m81,840\u001b[0m (319.69 KB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">81,840</span> (319.69 KB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m81,840\u001b[0m (319.69 KB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">81,840</span> (319.69 KB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n</pre>\n"},"metadata":{}}]},{"cell_type":"code","source":"# 학습 수행.\nhistory = model.fit(x=tr_images, y=tr_oh_labels, batch_size=128, epochs=20, validation_data=(val_images, val_oh_labels))\n#x = 모델에 입력으로 들어갈 데이터\n#y = 모델이 예측해야하는 타겟(라벨) 데이터\n#학습데이터 51,000개를 한방에 집어넣고 연산을 하면 gpu 메모리가 딸리게 된다.\n#따라서 데이터를 끊어서 조금씩 집어넣는다.(batch_size=32면 -> 데이터셋에서 32*28*28 만큼씩만 끊어서 모델에 입력한다.)\n#그러면 1,594번을 수행해야 모든 데이터셋이 한번 학습이 된 것이고 이 것을 Iteration, batch라고 한다.\n#epochs는 모든 데이터셋을 몇 번 학습시킬 것인지를 뜻한다. 3이면 전체 데이터셋을 총 3번 학습시킨다는 것이다.\n#validation_data = 검증 데이터를 지정한다. 학습 중에 모델의 성능을 평가하는 데 사용된다.\n#학습 중 각 epoch가 끝날 때마다 이 데이터를 사용하여 모델의 성능을 평가한다.\n","metadata":{"execution":{"iopub.status.busy":"2024-06-11T09:31:29.676086Z","iopub.execute_input":"2024-06-11T09:31:29.676458Z","iopub.status.idle":"2024-06-11T09:31:50.316242Z","shell.execute_reply.started":"2024-06-11T09:31:29.676422Z","shell.execute_reply":"2024-06-11T09:31:50.315465Z"},"trusted":true},"execution_count":31,"outputs":[{"name":"stdout","text":"Epoch 1/20\n\u001b[1m 92/399\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5372 - loss: 1.3726","output_type":"stream"},{"name":"stderr","text":"W0000 00:00:1718098291.993601     132 graph_launch.cc:671] Fallback to op-by-op mode because memset node breaks graph update\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m399/399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7049 - loss: 0.8697","output_type":"stream"},{"name":"stderr","text":"W0000 00:00:1718098293.704866     132 graph_launch.cc:671] Fallback to op-by-op mode because memset node breaks graph update\nW0000 00:00:1718098294.425181     134 graph_launch.cc:671] Fallback to op-by-op mode because memset node breaks graph update\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m399/399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 8ms/step - accuracy: 0.7051 - loss: 0.8690 - val_accuracy: 0.8329 - val_loss: 0.4608\nEpoch 2/20\n\u001b[1m 92/399\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8416 - loss: 0.4451","output_type":"stream"},{"name":"stderr","text":"W0000 00:00:1718098295.017069     133 graph_launch.cc:671] Fallback to op-by-op mode because memset node breaks graph update\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m399/399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8480 - loss: 0.4305 - val_accuracy: 0.8520 - val_loss: 0.4053\nEpoch 3/20\n\u001b[1m399/399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8661 - loss: 0.3773 - val_accuracy: 0.8598 - val_loss: 0.3880\nEpoch 4/20\n\u001b[1m399/399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8750 - loss: 0.3483 - val_accuracy: 0.8647 - val_loss: 0.3716\nEpoch 5/20\n\u001b[1m399/399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8812 - loss: 0.3302 - val_accuracy: 0.8736 - val_loss: 0.3551\nEpoch 6/20\n\u001b[1m399/399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8880 - loss: 0.3125 - val_accuracy: 0.8750 - val_loss: 0.3435\nEpoch 7/20\n\u001b[1m399/399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8897 - loss: 0.3018 - val_accuracy: 0.8788 - val_loss: 0.3312\nEpoch 8/20\n\u001b[1m399/399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8921 - loss: 0.2915 - val_accuracy: 0.8770 - val_loss: 0.3386\nEpoch 9/20\n\u001b[1m399/399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8974 - loss: 0.2822 - val_accuracy: 0.8700 - val_loss: 0.3633\nEpoch 10/20\n\u001b[1m399/399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8968 - loss: 0.2785 - val_accuracy: 0.8781 - val_loss: 0.3362\nEpoch 11/20\n\u001b[1m399/399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9029 - loss: 0.2645 - val_accuracy: 0.8832 - val_loss: 0.3242\nEpoch 12/20\n\u001b[1m399/399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9048 - loss: 0.2606 - val_accuracy: 0.8752 - val_loss: 0.3476\nEpoch 13/20\n\u001b[1m399/399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9056 - loss: 0.2555 - val_accuracy: 0.8836 - val_loss: 0.3319\nEpoch 14/20\n\u001b[1m399/399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9078 - loss: 0.2486 - val_accuracy: 0.8841 - val_loss: 0.3311\nEpoch 15/20\n\u001b[1m399/399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9137 - loss: 0.2340 - val_accuracy: 0.8840 - val_loss: 0.3297\nEpoch 16/20\n\u001b[1m399/399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9144 - loss: 0.2337 - val_accuracy: 0.8867 - val_loss: 0.3254\nEpoch 17/20\n\u001b[1m399/399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9169 - loss: 0.2272 - val_accuracy: 0.8887 - val_loss: 0.3153\nEpoch 18/20\n\u001b[1m399/399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9178 - loss: 0.2195 - val_accuracy: 0.8869 - val_loss: 0.3355\nEpoch 19/20\n\u001b[1m399/399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9198 - loss: 0.2173 - val_accuracy: 0.8853 - val_loss: 0.3300\nEpoch 20/20\n\u001b[1m399/399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9207 - loss: 0.2127 - val_accuracy: 0.8811 - val_loss: 0.3388\n","output_type":"stream"}]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n%matplotlib inline\n\ndef show_history(history):\n    plt.plot(history.history['accuracy'], label='train')\n    plt.plot(history.history['val_accuracy'], label='valid')\n    plt.legend()\n    \nshow_history(history)\n\n#학습을 많이 할수록 train 정확도와 valid 정확도의 gap은 벌어질 수 밖에 없다. 자연스러운 현상이다.","metadata":{"execution":{"iopub.status.busy":"2024-06-11T09:31:51.230574Z","iopub.execute_input":"2024-06-11T09:31:51.231607Z","iopub.status.idle":"2024-06-11T09:31:51.606349Z","shell.execute_reply.started":"2024-06-11T09:31:51.231561Z","shell.execute_reply":"2024-06-11T09:31:51.605278Z"},"trusted":true},"execution_count":32,"outputs":[{"output_type":"display_data","data":{"text/plain":"<Figure size 640x480 with 1 Axes>","image/png":"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"},"metadata":{}}]},{"cell_type":"code","source":"# 테스트 데이터 세트로 모델 성능 검증\nmodel.evaluate(test_images, test_oh_labels, batch_size=32, verbose=1)\n\n#Result\n#[0.6657008528709412, 0.8776999711990356]\n#손실률은 0.66이고 모델 정확도는 87.76%이다.","metadata":{"execution":{"iopub.status.busy":"2024-06-11T09:31:52.943642Z","iopub.execute_input":"2024-06-11T09:31:52.944006Z","iopub.status.idle":"2024-06-11T09:31:53.913501Z","shell.execute_reply.started":"2024-06-11T09:31:52.943978Z","shell.execute_reply":"2024-06-11T09:31:53.912445Z"},"trusted":true},"execution_count":33,"outputs":[{"name":"stdout","text":"\u001b[1m111/313\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8801 - loss: 0.3378","output_type":"stream"},{"name":"stderr","text":"W0000 00:00:1718098313.276189     133 graph_launch.cc:671] Fallback to op-by-op mode because memset node breaks graph update\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8756 - loss: 0.3546\n","output_type":"stream"},{"execution_count":33,"output_type":"execute_result","data":{"text/plain":"[0.35885465145111084, 0.8751000165939331]"},"metadata":{}}]},{"cell_type":"markdown","source":"### Callback ","metadata":{}},{"cell_type":"markdown","source":"#### ModelCheckpoint(filepath, monitor='val_loss', verbose=0, save_best_only=False, save_weights_only=False, mode='auto', period=1)\n* 특정 조건에 맞춰서 모델을 파일로 저장(중간중간에 저장하는 것이 좋다)\n - 학습 중에 메모리 부족 등으로 다운되면 처음부터 다시 학습을 시켜야 한다.\n - 처음에는 오래걸리지만 중간에 저장해두면 나중에 시간을 단축시킬 수 있다.\n* filepath: filepath는 (on_epoch_end에서 전달되는) epoch의 값과 logs의 키로 채워진 이름 형식 옵션을 가질 수 있음.\n예를 들어 filepath가 weights.{epoch:02d}-{val_loss:.2f}.hdf5라면, 파일 이름에 세대 번호와 검증 손실을 넣어 모델의 체크포인트가 저장 \n* monitor: 모니터할 지표(loss 또는 평가 지표) (학습데이터에는 하지 않는다.-오버피팅 문제 때문)\n* save_best_only: 가장 좋은 성능을 나타내는 모델만 저장할 여부\n* save_weights_only: Weights만 저장할 지 여부 (가급적 true로 저장하는게 좋다.)\n* mode: {auto, min, max} 중 하나. monitor 지표가 감소해야 좋을 경우 min, 증가해야 좋을 경우 max, auto는 monitor 이름에서 자동으로 유추.\n* min이면 min max면 max 확실하게 지정하는 것이 좋다.","metadata":{}},{"cell_type":"code","source":"!pwd","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.callbacks import ModelCheckpoint\n\nmodel = create_model()\nmodel.compile(optimizer=Adam(0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n\n# TF 버전업 변경 사항\n# ModelCheckpoint의 save_weights_only=True 시 checkpoint 모델 파일명은 .weights.h5로 끝나는 형태가 되어야 함. False시에는 .keras로 끝나야 함. \n#  period는 save_freq로 변경됨. save_freq는 \"epoch\" 또는 정수값이 올 수 있는데, 정수값의 경우 epoch가 아니라 학습 batch 반복 횟수이며 적용시 오류 위험 \nmcp_cb = ModelCheckpoint(filepath='/kaggle/working/checkpoint.{epoch:02d}-{val_loss:.2f}.weights.h5', monitor='val_loss', \n                         save_best_only=True, save_weights_only=True, mode='min', save_freq=\"epoch\", verbose=1)\nhistory = model.fit(x=tr_images, y=tr_oh_labels, batch_size=128, epochs=10, validation_data=(val_images, val_oh_labels),\n                   callbacks=[mcp_cb])","metadata":{"execution":{"iopub.status.busy":"2024-06-11T09:56:18.645730Z","iopub.execute_input":"2024-06-11T09:56:18.646646Z","iopub.status.idle":"2024-06-11T09:56:30.204801Z","shell.execute_reply.started":"2024-06-11T09:56:18.646604Z","shell.execute_reply":"2024-06-11T09:56:30.204017Z"},"trusted":true},"execution_count":47,"outputs":[{"name":"stdout","text":"Epoch 1/10\n\u001b[1m 91/399\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5498 - loss: 1.4059","output_type":"stream"},{"name":"stderr","text":"W0000 00:00:1718099780.503758     135 graph_launch.cc:671] Fallback to op-by-op mode because memset node breaks graph update\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m399/399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7085 - loss: 0.8879","output_type":"stream"},{"name":"stderr","text":"W0000 00:00:1718099781.785073     134 graph_launch.cc:671] Fallback to op-by-op mode because memset node breaks graph update\nW0000 00:00:1718099782.265666     133 graph_launch.cc:671] Fallback to op-by-op mode because memset node breaks graph update\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 1: val_loss improved from inf to 0.47742, saving model to /kaggle/working/checkpoint.01-0.48.weights.h5\n\u001b[1m399/399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - accuracy: 0.7087 - loss: 0.8873 - val_accuracy: 0.8303 - val_loss: 0.4774\nEpoch 2/10\n\u001b[1m 60/399\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8387 - loss: 0.4576","output_type":"stream"},{"name":"stderr","text":"W0000 00:00:1718099782.602627     134 graph_launch.cc:671] Fallback to op-by-op mode because memset node breaks graph update\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m396/399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8483 - loss: 0.4363\nEpoch 2: val_loss improved from 0.47742 to 0.41726, saving model to /kaggle/working/checkpoint.02-0.42.weights.h5\n\u001b[1m399/399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8483 - loss: 0.4361 - val_accuracy: 0.8522 - val_loss: 0.4173\nEpoch 3/10\n\u001b[1m397/399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8645 - loss: 0.3866\nEpoch 3: val_loss improved from 0.41726 to 0.38741, saving model to /kaggle/working/checkpoint.03-0.39.weights.h5\n\u001b[1m399/399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8645 - loss: 0.3866 - val_accuracy: 0.8593 - val_loss: 0.3874\nEpoch 4/10\n\u001b[1m376/399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8725 - loss: 0.3619\nEpoch 4: val_loss improved from 0.38741 to 0.37760, saving model to /kaggle/working/checkpoint.04-0.38.weights.h5\n\u001b[1m399/399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8724 - loss: 0.3618 - val_accuracy: 0.8633 - val_loss: 0.3776\nEpoch 5/10\n\u001b[1m396/399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8805 - loss: 0.3326\nEpoch 5: val_loss improved from 0.37760 to 0.34715, saving model to /kaggle/working/checkpoint.05-0.35.weights.h5\n\u001b[1m399/399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8804 - loss: 0.3327 - val_accuracy: 0.8763 - val_loss: 0.3472\nEpoch 6/10\n\u001b[1m392/399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8806 - loss: 0.3289\nEpoch 6: val_loss did not improve from 0.34715\n\u001b[1m399/399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8806 - loss: 0.3288 - val_accuracy: 0.8709 - val_loss: 0.3544\nEpoch 7/10\n\u001b[1m390/399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8865 - loss: 0.3122\nEpoch 7: val_loss improved from 0.34715 to 0.34715, saving model to /kaggle/working/checkpoint.07-0.35.weights.h5\n\u001b[1m399/399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8865 - loss: 0.3122 - val_accuracy: 0.8732 - val_loss: 0.3471\nEpoch 8/10\n\u001b[1m390/399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8914 - loss: 0.2997\nEpoch 8: val_loss improved from 0.34715 to 0.32834, saving model to /kaggle/working/checkpoint.08-0.33.weights.h5\n\u001b[1m399/399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8914 - loss: 0.2997 - val_accuracy: 0.8823 - val_loss: 0.3283\nEpoch 9/10\n\u001b[1m395/399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8929 - loss: 0.2887\nEpoch 9: val_loss did not improve from 0.32834\n\u001b[1m399/399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8929 - loss: 0.2887 - val_accuracy: 0.8779 - val_loss: 0.3388\nEpoch 10/10\n\u001b[1m397/399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8957 - loss: 0.2812\nEpoch 10: val_loss improved from 0.32834 to 0.32134, saving model to /kaggle/working/checkpoint.10-0.32.weights.h5\n\u001b[1m399/399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8958 - loss: 0.2812 - val_accuracy: 0.8843 - val_loss: 0.3213\n","output_type":"stream"}]},{"cell_type":"code","source":"!ls -lia\n!rm -rf weight*\n!ls -lia\n","metadata":{"execution":{"iopub.status.busy":"2024-06-11T09:39:39.065069Z","iopub.execute_input":"2024-06-11T09:39:39.065465Z","iopub.status.idle":"2024-06-11T09:39:42.241353Z","shell.execute_reply.started":"2024-06-11T09:39:39.065429Z","shell.execute_reply":"2024-06-11T09:39:42.240080Z"},"trusted":true},"execution_count":43,"outputs":[{"name":"stdout","text":"total 12\n   131078 drwxr-xr-x 3 root root 4096 Jun 11 09:39 .\n322054256 drwxr-xr-x 5 root root 4096 Jun 11 06:25 ..\n   131079 drwxr-xr-x 2 root root 4096 Jun 11 06:25 .virtual_documents\ntotal 12\n   131078 drwxr-xr-x 3 root root 4096 Jun 11 09:39 .\n322054256 drwxr-xr-x 5 root root 4096 Jun 11 06:25 ..\n   131079 drwxr-xr-x 2 root root 4096 Jun 11 06:25 .virtual_documents\n","output_type":"stream"}]},{"cell_type":"markdown","source":"#### ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=10, verbose=0, mode='auto', min_delta=0.0001, cooldown=0, min_lr=0)\n* 특정 epochs 횟수동안 성능이 개선 되지 않을 시 Learning rate를 동적으로 감소 시킴 \n* monitor: 모니터할 지표(loss 또는 평가 지표) \n* factor: 학습 속도를 줄일 인수. new_lr = lr * factor \n* patience: Learing Rate를 줄이기 전에 monitor할 epochs 횟수. \n* mode: {auto, min, max} 중 하나. monitor 지표가 감소해야 좋을 경우 min, 증가해야 좋을 경우 max, auto는 monitor 이름에서 유추. ","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras.callbacks import ReduceLROnPlateau\n\nmodel = create_model()\nmodel.compile(optimizer=Adam(0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n\nrlr_cb = ReduceLROnPlateau(monitor='val_loss', factor=0.3, patience=3, mode='min', verbose=1)\nhistory = model.fit(x=tr_images, y=tr_oh_labels, batch_size=128, epochs=30, validation_data=(val_images, val_oh_labels),\n                   callbacks=[rlr_cb])","metadata":{"execution":{"iopub.status.busy":"2024-06-11T09:56:38.093095Z","iopub.execute_input":"2024-06-11T09:56:38.093822Z","iopub.status.idle":"2024-06-11T09:57:06.763548Z","shell.execute_reply.started":"2024-06-11T09:56:38.093788Z","shell.execute_reply":"2024-06-11T09:57:06.762778Z"},"trusted":true},"execution_count":48,"outputs":[{"name":"stdout","text":"Epoch 1/30\n\u001b[1m 91/399\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5778 - loss: 1.3209","output_type":"stream"},{"name":"stderr","text":"W0000 00:00:1718099799.921484     135 graph_launch.cc:671] Fallback to op-by-op mode because memset node breaks graph update\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m399/399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7238 - loss: 0.8368","output_type":"stream"},{"name":"stderr","text":"W0000 00:00:1718099801.201188     132 graph_launch.cc:671] Fallback to op-by-op mode because memset node breaks graph update\nW0000 00:00:1718099802.042098     132 graph_launch.cc:671] Fallback to op-by-op mode because memset node breaks graph update\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m399/399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - accuracy: 0.7240 - loss: 0.8362 - val_accuracy: 0.8431 - val_loss: 0.4432 - learning_rate: 0.0010\nEpoch 2/30\n\u001b[1m 91/399\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8522 - loss: 0.4256","output_type":"stream"},{"name":"stderr","text":"W0000 00:00:1718099802.368364     134 graph_launch.cc:671] Fallback to op-by-op mode because memset node breaks graph update\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m399/399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8523 - loss: 0.4171 - val_accuracy: 0.8572 - val_loss: 0.3958 - learning_rate: 0.0010\nEpoch 3/30\n\u001b[1m399/399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8657 - loss: 0.3727 - val_accuracy: 0.8631 - val_loss: 0.3751 - learning_rate: 0.0010\nEpoch 4/30\n\u001b[1m399/399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8750 - loss: 0.3506 - val_accuracy: 0.8584 - val_loss: 0.3849 - learning_rate: 0.0010\nEpoch 5/30\n\u001b[1m399/399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8828 - loss: 0.3254 - val_accuracy: 0.8733 - val_loss: 0.3416 - learning_rate: 0.0010\nEpoch 6/30\n\u001b[1m399/399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8897 - loss: 0.3008 - val_accuracy: 0.8751 - val_loss: 0.3438 - learning_rate: 0.0010\nEpoch 7/30\n\u001b[1m399/399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8921 - loss: 0.2947 - val_accuracy: 0.8803 - val_loss: 0.3262 - learning_rate: 0.0010\nEpoch 8/30\n\u001b[1m399/399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8972 - loss: 0.2813 - val_accuracy: 0.8814 - val_loss: 0.3244 - learning_rate: 0.0010\nEpoch 9/30\n\u001b[1m399/399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8977 - loss: 0.2728 - val_accuracy: 0.8829 - val_loss: 0.3272 - learning_rate: 0.0010\nEpoch 10/30\n\u001b[1m399/399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9016 - loss: 0.2672 - val_accuracy: 0.8830 - val_loss: 0.3223 - learning_rate: 0.0010\nEpoch 11/30\n\u001b[1m399/399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9028 - loss: 0.2650 - val_accuracy: 0.8867 - val_loss: 0.3135 - learning_rate: 0.0010\nEpoch 12/30\n\u001b[1m399/399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9048 - loss: 0.2532 - val_accuracy: 0.8852 - val_loss: 0.3116 - learning_rate: 0.0010\nEpoch 13/30\n\u001b[1m399/399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9106 - loss: 0.2430 - val_accuracy: 0.8878 - val_loss: 0.3082 - learning_rate: 0.0010\nEpoch 14/30\n\u001b[1m399/399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9115 - loss: 0.2398 - val_accuracy: 0.8803 - val_loss: 0.3280 - learning_rate: 0.0010\nEpoch 15/30\n\u001b[1m399/399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9161 - loss: 0.2304 - val_accuracy: 0.8720 - val_loss: 0.3552 - learning_rate: 0.0010\nEpoch 16/30\n\u001b[1m389/399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9143 - loss: 0.2312\nEpoch 16: ReduceLROnPlateau reducing learning rate to 0.0003000000142492354.\n\u001b[1m399/399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9144 - loss: 0.2311 - val_accuracy: 0.8867 - val_loss: 0.3186 - learning_rate: 0.0010\nEpoch 17/30\n\u001b[1m399/399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9261 - loss: 0.1995 - val_accuracy: 0.8941 - val_loss: 0.2991 - learning_rate: 3.0000e-04\nEpoch 18/30\n\u001b[1m399/399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9293 - loss: 0.1984 - val_accuracy: 0.8940 - val_loss: 0.3037 - learning_rate: 3.0000e-04\nEpoch 19/30\n\u001b[1m399/399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9297 - loss: 0.1907 - val_accuracy: 0.8938 - val_loss: 0.3034 - learning_rate: 3.0000e-04\nEpoch 20/30\n\u001b[1m394/399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9330 - loss: 0.1860\nEpoch 20: ReduceLROnPlateau reducing learning rate to 9.000000427477062e-05.\n\u001b[1m399/399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9330 - loss: 0.1861 - val_accuracy: 0.8918 - val_loss: 0.3065 - learning_rate: 3.0000e-04\nEpoch 21/30\n\u001b[1m399/399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9334 - loss: 0.1867 - val_accuracy: 0.8948 - val_loss: 0.2988 - learning_rate: 9.0000e-05\nEpoch 22/30\n\u001b[1m399/399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9342 - loss: 0.1818 - val_accuracy: 0.8944 - val_loss: 0.3005 - learning_rate: 9.0000e-05\nEpoch 23/30\n\u001b[1m399/399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9364 - loss: 0.1784 - val_accuracy: 0.8950 - val_loss: 0.2999 - learning_rate: 9.0000e-05\nEpoch 24/30\n\u001b[1m388/399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9363 - loss: 0.1773\nEpoch 24: ReduceLROnPlateau reducing learning rate to 2.700000040931627e-05.\n\u001b[1m399/399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9363 - loss: 0.1772 - val_accuracy: 0.8936 - val_loss: 0.3025 - learning_rate: 9.0000e-05\nEpoch 25/30\n\u001b[1m399/399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9377 - loss: 0.1742 - val_accuracy: 0.8943 - val_loss: 0.3003 - learning_rate: 2.7000e-05\nEpoch 26/30\n\u001b[1m399/399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9385 - loss: 0.1712 - val_accuracy: 0.8944 - val_loss: 0.3002 - learning_rate: 2.7000e-05\nEpoch 27/30\n\u001b[1m395/399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9383 - loss: 0.1721\nEpoch 27: ReduceLROnPlateau reducing learning rate to 8.100000013655517e-06.\n\u001b[1m399/399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9383 - loss: 0.1721 - val_accuracy: 0.8943 - val_loss: 0.3003 - learning_rate: 2.7000e-05\nEpoch 28/30\n\u001b[1m399/399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9382 - loss: 0.1730 - val_accuracy: 0.8941 - val_loss: 0.3001 - learning_rate: 8.1000e-06\nEpoch 29/30\n\u001b[1m399/399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9372 - loss: 0.1712 - val_accuracy: 0.8944 - val_loss: 0.3001 - learning_rate: 8.1000e-06\nEpoch 30/30\n\u001b[1m382/399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9374 - loss: 0.1729\nEpoch 30: ReduceLROnPlateau reducing learning rate to 2.429999949526973e-06.\n\u001b[1m399/399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9375 - loss: 0.1728 - val_accuracy: 0.8940 - val_loss: 0.3000 - learning_rate: 8.1000e-06\n","output_type":"stream"}]},{"cell_type":"markdown","source":"#### EarlyStopping(monitor='val_loss', min_delta=0, patience=0, verbose=0, mode='auto', baseline=None, restore_best_weights=False)\n* 특정 epochs 동안 성능이 개선되지 않을 시 학습을 조기에 중단\n* monitor: 모니터할 지표(loss 또는 평가 지표) \n* patience: Early Stopping 적용 전에 monitor할 epochs 횟수. \n* mode: {auto, min, max} 중 하나. monitor 지표가 감소해야 좋을 경우 min, 증가해야 좋을 경우 max, auto는 monitor 이름에서 유추. ","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras.callbacks import EarlyStopping\n\nmodel = create_model()\nmodel.compile(optimizer=Adam(0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n\nely_cb = EarlyStopping(monitor='val_loss', patience=3, mode='min', verbose=1)\nhistory = model.fit(x=tr_images, y=tr_oh_labels, batch_size=128, epochs=30, validation_data=(val_images, val_oh_labels),\n                   callbacks=[ely_cb])","metadata":{"execution":{"iopub.status.busy":"2024-06-11T09:57:07.328871Z","iopub.execute_input":"2024-06-11T09:57:07.329738Z","iopub.status.idle":"2024-06-11T09:57:19.117701Z","shell.execute_reply.started":"2024-06-11T09:57:07.329701Z","shell.execute_reply":"2024-06-11T09:57:19.116698Z"},"trusted":true},"execution_count":49,"outputs":[{"name":"stdout","text":"Epoch 1/30\n\u001b[1m 92/399\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5581 - loss: 1.3196","output_type":"stream"},{"name":"stderr","text":"W0000 00:00:1718099829.194466     134 graph_launch.cc:671] Fallback to op-by-op mode because memset node breaks graph update\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m399/399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7134 - loss: 0.8533","output_type":"stream"},{"name":"stderr","text":"W0000 00:00:1718099830.468220     133 graph_launch.cc:671] Fallback to op-by-op mode because memset node breaks graph update\nW0000 00:00:1718099830.939440     132 graph_launch.cc:671] Fallback to op-by-op mode because memset node breaks graph update\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m399/399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - accuracy: 0.7136 - loss: 0.8527 - val_accuracy: 0.8407 - val_loss: 0.4527\nEpoch 2/30\n\u001b[1m 92/399\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8463 - loss: 0.4338","output_type":"stream"},{"name":"stderr","text":"W0000 00:00:1718099831.271164     134 graph_launch.cc:671] Fallback to op-by-op mode because memset node breaks graph update\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m399/399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8488 - loss: 0.4291 - val_accuracy: 0.8543 - val_loss: 0.4056\nEpoch 3/30\n\u001b[1m399/399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8678 - loss: 0.3754 - val_accuracy: 0.8604 - val_loss: 0.3846\nEpoch 4/30\n\u001b[1m399/399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8717 - loss: 0.3537 - val_accuracy: 0.8701 - val_loss: 0.3527\nEpoch 5/30\n\u001b[1m399/399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8813 - loss: 0.3300 - val_accuracy: 0.8729 - val_loss: 0.3467\nEpoch 6/30\n\u001b[1m399/399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8850 - loss: 0.3168 - val_accuracy: 0.8681 - val_loss: 0.3628\nEpoch 7/30\n\u001b[1m399/399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8916 - loss: 0.2950 - val_accuracy: 0.8816 - val_loss: 0.3275\nEpoch 8/30\n\u001b[1m399/399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8946 - loss: 0.2890 - val_accuracy: 0.8751 - val_loss: 0.3475\nEpoch 9/30\n\u001b[1m399/399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8977 - loss: 0.2799 - val_accuracy: 0.8744 - val_loss: 0.3414\nEpoch 10/30\n\u001b[1m399/399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9003 - loss: 0.2712 - val_accuracy: 0.8796 - val_loss: 0.3396\nEpoch 10: early stopping\n","output_type":"stream"}]},{"cell_type":"code","source":"!rm weigh*","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping\n\nmodel = create_model()\nmodel.compile(optimizer=Adam(0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n\n# TF 버전업 변경 사항\n# ModelCheckpoint의 save_weights_only=True 시 checkpoint 모델 파일명은 .weights.h5로 끝나는 형태가 되어야 함. False시에는 .keras로 끝나야 함. \n#  period는 save_freq로 변경됨. save_freq는 \"epoch\" 또는 정수값이 올 수 있는데, 정수값의 경우 epoch가 아니라 학습 batch 반복 횟수이며 적용시 오류 위험 \nmcp_cb = ModelCheckpoint(filepath='/kaggle/working/checkpoint.{epoch:02d}-{val_loss:.2f}.weights.h5', monitor='val_loss', \n                         save_best_only=True, save_weights_only=True, mode='min', save_freq=\"epoch\", verbose=0)\nrlr_cb = ReduceLROnPlateau(monitor='val_loss', factor=0.3, patience=5, mode='min', verbose=1)\nely_cb = EarlyStopping(monitor='val_loss', patience=7, mode='min', verbose=1)\n\nhistory = model.fit(x=tr_images, y=tr_oh_labels, batch_size=128, epochs=40, validation_data=(val_images, val_oh_labels),\n                   callbacks=[mcp_cb, rlr_cb, ely_cb])","metadata":{"execution":{"iopub.status.busy":"2024-06-11T09:57:30.543491Z","iopub.execute_input":"2024-06-11T09:57:30.544445Z","iopub.status.idle":"2024-06-11T09:57:57.558039Z","shell.execute_reply.started":"2024-06-11T09:57:30.544411Z","shell.execute_reply":"2024-06-11T09:57:57.557098Z"},"trusted":true},"execution_count":50,"outputs":[{"name":"stdout","text":"Epoch 1/40\n\u001b[1m 87/399\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5195 - loss: 1.4277","output_type":"stream"},{"name":"stderr","text":"W0000 00:00:1718099852.374000     134 graph_launch.cc:671] Fallback to op-by-op mode because memset node breaks graph update\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m399/399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7013 - loss: 0.8871","output_type":"stream"},{"name":"stderr","text":"W0000 00:00:1718099853.685457     135 graph_launch.cc:671] Fallback to op-by-op mode because memset node breaks graph update\nW0000 00:00:1718099854.162389     134 graph_launch.cc:671] Fallback to op-by-op mode because memset node breaks graph update\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m399/399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - accuracy: 0.7016 - loss: 0.8864 - val_accuracy: 0.8258 - val_loss: 0.4804 - learning_rate: 0.0010\nEpoch 2/40\n\u001b[1m 58/399\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8444 - loss: 0.4545","output_type":"stream"},{"name":"stderr","text":"W0000 00:00:1718099854.509551     132 graph_launch.cc:671] Fallback to op-by-op mode because memset node breaks graph update\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m399/399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8503 - loss: 0.4307 - val_accuracy: 0.8479 - val_loss: 0.4184 - learning_rate: 0.0010\nEpoch 3/40\n\u001b[1m399/399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8642 - loss: 0.3749 - val_accuracy: 0.8638 - val_loss: 0.3831 - learning_rate: 0.0010\nEpoch 4/40\n\u001b[1m399/399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8739 - loss: 0.3517 - val_accuracy: 0.8708 - val_loss: 0.3595 - learning_rate: 0.0010\nEpoch 5/40\n\u001b[1m399/399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8817 - loss: 0.3276 - val_accuracy: 0.8700 - val_loss: 0.3668 - learning_rate: 0.0010\nEpoch 6/40\n\u001b[1m399/399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8871 - loss: 0.3136 - val_accuracy: 0.8668 - val_loss: 0.3673 - learning_rate: 0.0010\nEpoch 7/40\n\u001b[1m399/399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8910 - loss: 0.2989 - val_accuracy: 0.8818 - val_loss: 0.3329 - learning_rate: 0.0010\nEpoch 8/40\n\u001b[1m399/399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8978 - loss: 0.2850 - val_accuracy: 0.8808 - val_loss: 0.3253 - learning_rate: 0.0010\nEpoch 9/40\n\u001b[1m399/399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8985 - loss: 0.2721 - val_accuracy: 0.8767 - val_loss: 0.3400 - learning_rate: 0.0010\nEpoch 10/40\n\u001b[1m399/399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9029 - loss: 0.2627 - val_accuracy: 0.8841 - val_loss: 0.3221 - learning_rate: 0.0010\nEpoch 11/40\n\u001b[1m399/399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9073 - loss: 0.2515 - val_accuracy: 0.8820 - val_loss: 0.3242 - learning_rate: 0.0010\nEpoch 12/40\n\u001b[1m399/399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9078 - loss: 0.2484 - val_accuracy: 0.8844 - val_loss: 0.3294 - learning_rate: 0.0010\nEpoch 13/40\n\u001b[1m399/399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9099 - loss: 0.2422 - val_accuracy: 0.8831 - val_loss: 0.3226 - learning_rate: 0.0010\nEpoch 14/40\n\u001b[1m399/399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9129 - loss: 0.2348 - val_accuracy: 0.8883 - val_loss: 0.3081 - learning_rate: 0.0010\nEpoch 15/40\n\u001b[1m399/399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9133 - loss: 0.2304 - val_accuracy: 0.8831 - val_loss: 0.3243 - learning_rate: 0.0010\nEpoch 16/40\n\u001b[1m399/399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9169 - loss: 0.2271 - val_accuracy: 0.8861 - val_loss: 0.3172 - learning_rate: 0.0010\nEpoch 17/40\n\u001b[1m399/399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9209 - loss: 0.2135 - val_accuracy: 0.8887 - val_loss: 0.3148 - learning_rate: 0.0010\nEpoch 18/40\n\u001b[1m399/399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9219 - loss: 0.2106 - val_accuracy: 0.8872 - val_loss: 0.3187 - learning_rate: 0.0010\nEpoch 19/40\n\u001b[1m388/399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9255 - loss: 0.2001\nEpoch 19: ReduceLROnPlateau reducing learning rate to 0.0003000000142492354.\n\u001b[1m399/399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9254 - loss: 0.2003 - val_accuracy: 0.8912 - val_loss: 0.3140 - learning_rate: 0.0010\nEpoch 20/40\n\u001b[1m399/399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9349 - loss: 0.1821 - val_accuracy: 0.8897 - val_loss: 0.3123 - learning_rate: 3.0000e-04\nEpoch 21/40\n\u001b[1m399/399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9351 - loss: 0.1746 - val_accuracy: 0.8966 - val_loss: 0.3047 - learning_rate: 3.0000e-04\nEpoch 22/40\n\u001b[1m399/399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9374 - loss: 0.1722 - val_accuracy: 0.8959 - val_loss: 0.3058 - learning_rate: 3.0000e-04\nEpoch 23/40\n\u001b[1m399/399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9390 - loss: 0.1707 - val_accuracy: 0.8946 - val_loss: 0.3065 - learning_rate: 3.0000e-04\nEpoch 24/40\n\u001b[1m399/399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9385 - loss: 0.1700 - val_accuracy: 0.8958 - val_loss: 0.3054 - learning_rate: 3.0000e-04\nEpoch 25/40\n\u001b[1m399/399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9412 - loss: 0.1663 - val_accuracy: 0.8926 - val_loss: 0.3095 - learning_rate: 3.0000e-04\nEpoch 26/40\n\u001b[1m390/399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9419 - loss: 0.1611\nEpoch 26: ReduceLROnPlateau reducing learning rate to 9.000000427477062e-05.\n\u001b[1m399/399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9419 - loss: 0.1612 - val_accuracy: 0.8921 - val_loss: 0.3112 - learning_rate: 3.0000e-04\nEpoch 27/40\n\u001b[1m399/399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9474 - loss: 0.1522 - val_accuracy: 0.8936 - val_loss: 0.3061 - learning_rate: 9.0000e-05\nEpoch 28/40\n\u001b[1m399/399\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9476 - loss: 0.1490 - val_accuracy: 0.8946 - val_loss: 0.3066 - learning_rate: 9.0000e-05\nEpoch 28: early stopping\n","output_type":"stream"}]},{"cell_type":"code","source":"!ls -lia","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}