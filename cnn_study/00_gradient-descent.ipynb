{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30698,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-05-18T06:09:13.846969Z","iopub.execute_input":"2024-05-18T06:09:13.847463Z","iopub.status.idle":"2024-05-18T06:09:14.395642Z","shell.execute_reply.started":"2024-05-18T06:09:13.847425Z","shell.execute_reply":"2024-05-18T06:09:14.394645Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"### 보스턴 주택 가격 데이터 세트를 Peceptron 기반에서 학습 및 테스트하기 위한 데이터 로드\n* 사이킷런에서 보스턴 주택 가격 데이터 세트를 로드하고 이를 DataFrame으로 생성","metadata":{}},{"cell_type":"code","source":"!pip install scikit-learn==1.0.2","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import sklearn\n\nprint(sklearn.__version__)","metadata":{"execution":{"iopub.status.busy":"2024-05-18T06:09:19.712509Z","iopub.execute_input":"2024-05-18T06:09:19.713237Z","iopub.status.idle":"2024-05-18T06:09:19.720753Z","shell.execute_reply.started":"2024-05-18T06:09:19.713195Z","shell.execute_reply":"2024-05-18T06:09:19.719199Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"1.0.2\n","output_type":"stream"}]},{"cell_type":"code","source":"from sklearn.datasets import load_boston\n\nboston = load_boston()\nprint(load_boston())","metadata":{"execution":{"iopub.status.busy":"2024-05-18T06:09:21.840515Z","iopub.execute_input":"2024-05-18T06:09:21.841030Z","iopub.status.idle":"2024-05-18T06:09:21.869850Z","shell.execute_reply.started":"2024-05-18T06:09:21.840990Z","shell.execute_reply":"2024-05-18T06:09:21.868559Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"{'data': array([[6.3200e-03, 1.8000e+01, 2.3100e+00, ..., 1.5300e+01, 3.9690e+02,\n        4.9800e+00],\n       [2.7310e-02, 0.0000e+00, 7.0700e+00, ..., 1.7800e+01, 3.9690e+02,\n        9.1400e+00],\n       [2.7290e-02, 0.0000e+00, 7.0700e+00, ..., 1.7800e+01, 3.9283e+02,\n        4.0300e+00],\n       ...,\n       [6.0760e-02, 0.0000e+00, 1.1930e+01, ..., 2.1000e+01, 3.9690e+02,\n        5.6400e+00],\n       [1.0959e-01, 0.0000e+00, 1.1930e+01, ..., 2.1000e+01, 3.9345e+02,\n        6.4800e+00],\n       [4.7410e-02, 0.0000e+00, 1.1930e+01, ..., 2.1000e+01, 3.9690e+02,\n        7.8800e+00]]), 'target': array([24. , 21.6, 34.7, 33.4, 36.2, 28.7, 22.9, 27.1, 16.5, 18.9, 15. ,\n       18.9, 21.7, 20.4, 18.2, 19.9, 23.1, 17.5, 20.2, 18.2, 13.6, 19.6,\n       15.2, 14.5, 15.6, 13.9, 16.6, 14.8, 18.4, 21. , 12.7, 14.5, 13.2,\n       13.1, 13.5, 18.9, 20. , 21. , 24.7, 30.8, 34.9, 26.6, 25.3, 24.7,\n       21.2, 19.3, 20. , 16.6, 14.4, 19.4, 19.7, 20.5, 25. , 23.4, 18.9,\n       35.4, 24.7, 31.6, 23.3, 19.6, 18.7, 16. , 22.2, 25. , 33. , 23.5,\n       19.4, 22. , 17.4, 20.9, 24.2, 21.7, 22.8, 23.4, 24.1, 21.4, 20. ,\n       20.8, 21.2, 20.3, 28. , 23.9, 24.8, 22.9, 23.9, 26.6, 22.5, 22.2,\n       23.6, 28.7, 22.6, 22. , 22.9, 25. , 20.6, 28.4, 21.4, 38.7, 43.8,\n       33.2, 27.5, 26.5, 18.6, 19.3, 20.1, 19.5, 19.5, 20.4, 19.8, 19.4,\n       21.7, 22.8, 18.8, 18.7, 18.5, 18.3, 21.2, 19.2, 20.4, 19.3, 22. ,\n       20.3, 20.5, 17.3, 18.8, 21.4, 15.7, 16.2, 18. , 14.3, 19.2, 19.6,\n       23. , 18.4, 15.6, 18.1, 17.4, 17.1, 13.3, 17.8, 14. , 14.4, 13.4,\n       15.6, 11.8, 13.8, 15.6, 14.6, 17.8, 15.4, 21.5, 19.6, 15.3, 19.4,\n       17. , 15.6, 13.1, 41.3, 24.3, 23.3, 27. , 50. , 50. , 50. , 22.7,\n       25. , 50. , 23.8, 23.8, 22.3, 17.4, 19.1, 23.1, 23.6, 22.6, 29.4,\n       23.2, 24.6, 29.9, 37.2, 39.8, 36.2, 37.9, 32.5, 26.4, 29.6, 50. ,\n       32. , 29.8, 34.9, 37. , 30.5, 36.4, 31.1, 29.1, 50. , 33.3, 30.3,\n       34.6, 34.9, 32.9, 24.1, 42.3, 48.5, 50. , 22.6, 24.4, 22.5, 24.4,\n       20. , 21.7, 19.3, 22.4, 28.1, 23.7, 25. , 23.3, 28.7, 21.5, 23. ,\n       26.7, 21.7, 27.5, 30.1, 44.8, 50. , 37.6, 31.6, 46.7, 31.5, 24.3,\n       31.7, 41.7, 48.3, 29. , 24. , 25.1, 31.5, 23.7, 23.3, 22. , 20.1,\n       22.2, 23.7, 17.6, 18.5, 24.3, 20.5, 24.5, 26.2, 24.4, 24.8, 29.6,\n       42.8, 21.9, 20.9, 44. , 50. , 36. , 30.1, 33.8, 43.1, 48.8, 31. ,\n       36.5, 22.8, 30.7, 50. , 43.5, 20.7, 21.1, 25.2, 24.4, 35.2, 32.4,\n       32. , 33.2, 33.1, 29.1, 35.1, 45.4, 35.4, 46. , 50. , 32.2, 22. ,\n       20.1, 23.2, 22.3, 24.8, 28.5, 37.3, 27.9, 23.9, 21.7, 28.6, 27.1,\n       20.3, 22.5, 29. , 24.8, 22. , 26.4, 33.1, 36.1, 28.4, 33.4, 28.2,\n       22.8, 20.3, 16.1, 22.1, 19.4, 21.6, 23.8, 16.2, 17.8, 19.8, 23.1,\n       21. , 23.8, 23.1, 20.4, 18.5, 25. , 24.6, 23. , 22.2, 19.3, 22.6,\n       19.8, 17.1, 19.4, 22.2, 20.7, 21.1, 19.5, 18.5, 20.6, 19. , 18.7,\n       32.7, 16.5, 23.9, 31.2, 17.5, 17.2, 23.1, 24.5, 26.6, 22.9, 24.1,\n       18.6, 30.1, 18.2, 20.6, 17.8, 21.7, 22.7, 22.6, 25. , 19.9, 20.8,\n       16.8, 21.9, 27.5, 21.9, 23.1, 50. , 50. , 50. , 50. , 50. , 13.8,\n       13.8, 15. , 13.9, 13.3, 13.1, 10.2, 10.4, 10.9, 11.3, 12.3,  8.8,\n        7.2, 10.5,  7.4, 10.2, 11.5, 15.1, 23.2,  9.7, 13.8, 12.7, 13.1,\n       12.5,  8.5,  5. ,  6.3,  5.6,  7.2, 12.1,  8.3,  8.5,  5. , 11.9,\n       27.9, 17.2, 27.5, 15. , 17.2, 17.9, 16.3,  7. ,  7.2,  7.5, 10.4,\n        8.8,  8.4, 16.7, 14.2, 20.8, 13.4, 11.7,  8.3, 10.2, 10.9, 11. ,\n        9.5, 14.5, 14.1, 16.1, 14.3, 11.7, 13.4,  9.6,  8.7,  8.4, 12.8,\n       10.5, 17.1, 18.4, 15.4, 10.8, 11.8, 14.9, 12.6, 14.1, 13. , 13.4,\n       15.2, 16.1, 17.8, 14.9, 14.1, 12.7, 13.5, 14.9, 20. , 16.4, 17.7,\n       19.5, 20.2, 21.4, 19.9, 19. , 19.1, 19.1, 20.1, 19.9, 19.6, 23.2,\n       29.8, 13.8, 13.3, 16.7, 12. , 14.6, 21.4, 23. , 23.7, 25. , 21.8,\n       20.6, 21.2, 19.1, 20.6, 15.2,  7. ,  8.1, 13.6, 20.1, 21.8, 24.5,\n       23.1, 19.7, 18.3, 21.2, 17.5, 16.8, 22.4, 20.6, 23.9, 22. , 11.9]), 'feature_names': array(['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD',\n       'TAX', 'PTRATIO', 'B', 'LSTAT'], dtype='<U7'), 'DESCR': \".. _boston_dataset:\\n\\nBoston house prices dataset\\n---------------------------\\n\\n**Data Set Characteristics:**  \\n\\n    :Number of Instances: 506 \\n\\n    :Number of Attributes: 13 numeric/categorical predictive. Median Value (attribute 14) is usually the target.\\n\\n    :Attribute Information (in order):\\n        - CRIM     per capita crime rate by town\\n        - ZN       proportion of residential land zoned for lots over 25,000 sq.ft.\\n        - INDUS    proportion of non-retail business acres per town\\n        - CHAS     Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)\\n        - NOX      nitric oxides concentration (parts per 10 million)\\n        - RM       average number of rooms per dwelling\\n        - AGE      proportion of owner-occupied units built prior to 1940\\n        - DIS      weighted distances to five Boston employment centres\\n        - RAD      index of accessibility to radial highways\\n        - TAX      full-value property-tax rate per $10,000\\n        - PTRATIO  pupil-teacher ratio by town\\n        - B        1000(Bk - 0.63)^2 where Bk is the proportion of black people by town\\n        - LSTAT    % lower status of the population\\n        - MEDV     Median value of owner-occupied homes in $1000's\\n\\n    :Missing Attribute Values: None\\n\\n    :Creator: Harrison, D. and Rubinfeld, D.L.\\n\\nThis is a copy of UCI ML housing dataset.\\nhttps://archive.ics.uci.edu/ml/machine-learning-databases/housing/\\n\\n\\nThis dataset was taken from the StatLib library which is maintained at Carnegie Mellon University.\\n\\nThe Boston house-price data of Harrison, D. and Rubinfeld, D.L. 'Hedonic\\nprices and the demand for clean air', J. Environ. Economics & Management,\\nvol.5, 81-102, 1978.   Used in Belsley, Kuh & Welsch, 'Regression diagnostics\\n...', Wiley, 1980.   N.B. Various transformations are used in the table on\\npages 244-261 of the latter.\\n\\nThe Boston house-price data has been used in many machine learning papers that address regression\\nproblems.   \\n     \\n.. topic:: References\\n\\n   - Belsley, Kuh & Welsch, 'Regression diagnostics: Identifying Influential Data and Sources of Collinearity', Wiley, 1980. 244-261.\\n   - Quinlan,R. (1993). Combining Instance-Based and Model-Based Learning. In Proceedings on the Tenth International Conference of Machine Learning, 236-243, University of Massachusetts, Amherst. Morgan Kaufmann.\\n\", 'filename': 'boston_house_prices.csv', 'data_module': 'sklearn.datasets.data'}\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function load_boston is deprecated; `load_boston` is deprecated in 1.0 and will be removed in 1.2.\n\n    The Boston housing prices dataset has an ethical problem. You can refer to\n    the documentation of this function for further details.\n\n    The scikit-learn maintainers therefore strongly discourage the use of this\n    dataset unless the purpose of the code is to study and educate about\n    ethical issues in data science and machine learning.\n\n    In this special case, you can fetch the dataset from the original\n    source::\n\n        import pandas as pd\n        import numpy as np\n\n\n        data_url = \"http://lib.stat.cmu.edu/datasets/boston\"\n        raw_df = pd.read_csv(data_url, sep=\"\\s+\", skiprows=22, header=None)\n        data = np.hstack([raw_df.values[::2, :], raw_df.values[1::2, :2]])\n        target = raw_df.values[1::2, 2]\n\n    Alternative datasets include the California housing dataset (i.e.\n    :func:`~sklearn.datasets.fetch_california_housing`) and the Ames housing\n    dataset. You can load the datasets as follows::\n\n        from sklearn.datasets import fetch_california_housing\n        housing = fetch_california_housing()\n\n    for the California housing dataset and::\n\n        from sklearn.datasets import fetch_openml\n        housing = fetch_openml(name=\"house_prices\", as_frame=True)\n\n    for the Ames housing dataset.\n    \n  warnings.warn(msg, category=FutureWarning)\n","output_type":"stream"}]},{"cell_type":"code","source":"bostonDF = pd.DataFrame(boston.data, columns=boston.feature_names)\nbostonDF['PRICE'] = boston.target\nprint(bostonDF.shape)\nbostonDF.head()","metadata":{"execution":{"iopub.status.busy":"2024-05-18T06:09:25.590168Z","iopub.execute_input":"2024-05-18T06:09:25.590694Z","iopub.status.idle":"2024-05-18T06:09:25.638068Z","shell.execute_reply.started":"2024-05-18T06:09:25.590639Z","shell.execute_reply":"2024-05-18T06:09:25.636870Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"(506, 14)\n","output_type":"stream"},{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"      CRIM    ZN  INDUS  CHAS    NOX     RM   AGE     DIS  RAD    TAX  \\\n0  0.00632  18.0   2.31   0.0  0.538  6.575  65.2  4.0900  1.0  296.0   \n1  0.02731   0.0   7.07   0.0  0.469  6.421  78.9  4.9671  2.0  242.0   \n2  0.02729   0.0   7.07   0.0  0.469  7.185  61.1  4.9671  2.0  242.0   \n3  0.03237   0.0   2.18   0.0  0.458  6.998  45.8  6.0622  3.0  222.0   \n4  0.06905   0.0   2.18   0.0  0.458  7.147  54.2  6.0622  3.0  222.0   \n\n   PTRATIO       B  LSTAT  PRICE  \n0     15.3  396.90   4.98   24.0  \n1     17.8  396.90   9.14   21.6  \n2     17.8  392.83   4.03   34.7  \n3     18.7  394.63   2.94   33.4  \n4     18.7  396.90   5.33   36.2  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>CRIM</th>\n      <th>ZN</th>\n      <th>INDUS</th>\n      <th>CHAS</th>\n      <th>NOX</th>\n      <th>RM</th>\n      <th>AGE</th>\n      <th>DIS</th>\n      <th>RAD</th>\n      <th>TAX</th>\n      <th>PTRATIO</th>\n      <th>B</th>\n      <th>LSTAT</th>\n      <th>PRICE</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.00632</td>\n      <td>18.0</td>\n      <td>2.31</td>\n      <td>0.0</td>\n      <td>0.538</td>\n      <td>6.575</td>\n      <td>65.2</td>\n      <td>4.0900</td>\n      <td>1.0</td>\n      <td>296.0</td>\n      <td>15.3</td>\n      <td>396.90</td>\n      <td>4.98</td>\n      <td>24.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.02731</td>\n      <td>0.0</td>\n      <td>7.07</td>\n      <td>0.0</td>\n      <td>0.469</td>\n      <td>6.421</td>\n      <td>78.9</td>\n      <td>4.9671</td>\n      <td>2.0</td>\n      <td>242.0</td>\n      <td>17.8</td>\n      <td>396.90</td>\n      <td>9.14</td>\n      <td>21.6</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.02729</td>\n      <td>0.0</td>\n      <td>7.07</td>\n      <td>0.0</td>\n      <td>0.469</td>\n      <td>7.185</td>\n      <td>61.1</td>\n      <td>4.9671</td>\n      <td>2.0</td>\n      <td>242.0</td>\n      <td>17.8</td>\n      <td>392.83</td>\n      <td>4.03</td>\n      <td>34.7</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.03237</td>\n      <td>0.0</td>\n      <td>2.18</td>\n      <td>0.0</td>\n      <td>0.458</td>\n      <td>6.998</td>\n      <td>45.8</td>\n      <td>6.0622</td>\n      <td>3.0</td>\n      <td>222.0</td>\n      <td>18.7</td>\n      <td>394.63</td>\n      <td>2.94</td>\n      <td>33.4</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.06905</td>\n      <td>0.0</td>\n      <td>2.18</td>\n      <td>0.0</td>\n      <td>0.458</td>\n      <td>7.147</td>\n      <td>54.2</td>\n      <td>6.0622</td>\n      <td>3.0</td>\n      <td>222.0</td>\n      <td>18.7</td>\n      <td>396.90</td>\n      <td>5.33</td>\n      <td>36.2</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"### Weight와 Bias의 Update 값을 계산하는 함수 생성.\n* w1은 RM(방의 계수) 피처의 Weight 값\n* w2는 LSTAT(하위계층 비율) 피처의 Weight 값\n* bias는 Bias\n* N은 입력 데이터 건수\n![](https://raw.githubusercontent.com/chulminkw/CNN_PG/main/utils/images/Weight_update.png)\n","metadata":{}},{"cell_type":"code","source":"# gradient_descent()함수에서 반복적으로 호출되면서 update될 weight/bias 값을 계산하는 함수. \n# rm은 RM(방 개수), lstat(하위계층 비율), target은 PRICE임. 전체 array가 다 입력됨. \n# 반환 값은 weight와 bias가 update되어야 할 값과 Mean Squared Error 값을 loss로 반환.\ndef get_update_weights_value(bias, w1, w2, rm, lstat, target, learning_rate=0.01):\n    \n    #Number of data\n    N = len(target)\n    #Predicted Value\n    predicted = bias + w1 * rm + w2 * lstat\n    #Difference between actual and predicted values\n    diff = target - predicted\n\n    #w1, w2의 업데이트값(-n * (dLoss(w)/dw1))\n    #.T는 세로 행렬을 가로 행렬로 변환하기 위해 수행한다. 행렬 곱 ㅡ * ㅣ 을 하기 위해서.\n    w1_update = -(2/N)*learning_rate*(np.dot(rm.T, diff))\n    w2_update = -(2/N)*learning_rate*(np.dot(lstat.T, diff))\n\n    #Set to obtain bias based on array.\n    #bias도 행렬로 만들어 주기 위해서 ones 함수를 사용한다.\n    #Np.ones((3,2)) -> [[1. 1.], [1. 1.], [1. 1.]]\n    #Np.ones((N,)) -> [1, 1, 1, ...N개... , 1]\n    bias_factors = np.ones((N,))\n    \n    #bias의 업데이트값(-n * (dLoss(bias)/dbias))\n    #.T는 세로 행렬을 가로 행렬로 변환하기 위해 수행한다. 행렬 곱 ㅡ * ㅣ 을 하기 위해서.\n    bias_update = -(2/N)*learning_rate*(np.dot(bias_factors.T, diff))\n\n    #회귀오류측정을 위해 모든 오차의 제곱을 해서 그 평균을 산출한다.\n    mse_loss = np.mean(np.square(diff))\n    \n    return bias_update, w1_update, w2_update, mse_loss","metadata":{"execution":{"iopub.status.busy":"2024-05-18T06:09:39.685896Z","iopub.execute_input":"2024-05-18T06:09:39.686377Z","iopub.status.idle":"2024-05-18T06:09:39.696987Z","shell.execute_reply.started":"2024-05-18T06:09:39.686343Z","shell.execute_reply":"2024-05-18T06:09:39.695677Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"### Gradient Descent 를 적용하는 함수 생성\n* iter_epochs 수만큼 반복적으로 get_update_weights_value()를 호출하여 update될 weight/bias값을 구한 뒤 Weight/Bias를 Update적용. ","metadata":{}},{"cell_type":"code","source":"# RM, LSTAT feature array와 PRICE target array를 입력 받아서 iter_epochs수만큼 반복적으로 Weight와 Bias를 update적용. \ndef gradient_descent(features, target, iter_epochs=1000, verbose=True):\n    # w1, w2는 numpy array 연산을 위해 1차원 array로 변환하되 초기 값은 0으로 설정\n    # bias도 1차원 array로 변환하되 초기 값은 1로 설정. \n    w1 = np.zeros((1,)) # np연산의 기본 데이터타입은 float이므로 [0.]이 출력된다.\n    w2 = np.zeros((1,))\n    bias = np.zeros((1, ))\n    print('최초 w1, w2, bias:', w1, w2, bias)\n    \n    # learning_rate와 RM, LSTAT 피처 지정. 호출 시 numpy array형태로 RM과 LSTAT으로 된 2차원 feature가 입력됨.\n    learning_rate = 0.01\n    rm = features[:, 0]\n    lstat = features[:, 1]\n    #features는 Numpy 배열이고 2차원 배열입니다.\n    #features[0,1]은 1행 2열의 값입니다.\n    #features[:,0]에서 :는 모든 행을 뜻합니다. 즉, 1열의 모든 행 값을 의미합니다.\n    #features = np.array(\n    #[[1,2]\n    #[3,4]\n    #[5,6]])\n    #features[0,1] = 2\n    #features[:,0] = [1,3,5]\n    #위에 코드에서는 1열의 도믄 행 값을 rm이라 칭하고 2열의 모든 행 값을 lstat라 칭한다는 것을 의미한다.\n    \n    # iter_epochs 수만큼 반복하면서 weight와 bias update 수행. \n    for i in range(iter_epochs):\n        # weight/bias update 값 계산 \n        bias_update, w1_update, w2_update, mse_loss = get_update_weights_value(bias, w1, w2, rm, lstat, target, learning_rate)\n        # weight/bias의 update 적용. \n        w1 = w1 - w1_update\n        w2 = w2 - w2_update\n        bias = bias - bias_update\n        if verbose:\n            print('Epoch:', i+1,'/', iter_epochs)\n            print('w1:', w1, 'w2:', w2, 'bias:', bias, 'loss:', mse_loss)\n        \n    return w1, w2, bias","metadata":{"execution":{"iopub.status.busy":"2024-05-18T06:09:45.543815Z","iopub.execute_input":"2024-05-18T06:09:45.544299Z","iopub.status.idle":"2024-05-18T06:09:45.556875Z","shell.execute_reply.started":"2024-05-18T06:09:45.544266Z","shell.execute_reply":"2024-05-18T06:09:45.555409Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"### Gradient Descent 적용\n* 신경망은 데이터를 정규화/표준화 작업을 미리 선행해 주어야 함. \n* 이를 위해 사이킷런의 MinMaxScaler를 이용하여 개별 feature값은 0~1사이 값으로 변환후 학습 적용.","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import MinMaxScaler\n\nscaler = MinMaxScaler()\nscaled_features = scaler.fit_transform(bostonDF[['RM', 'LSTAT']])\nprint(scaled_features[:15])","metadata":{"execution":{"iopub.status.busy":"2024-05-18T06:09:51.271048Z","iopub.execute_input":"2024-05-18T06:09:51.271510Z","iopub.status.idle":"2024-05-18T06:09:51.291079Z","shell.execute_reply.started":"2024-05-18T06:09:51.271478Z","shell.execute_reply":"2024-05-18T06:09:51.289673Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"[[0.57750527 0.08967991]\n [0.5479977  0.2044702 ]\n [0.6943859  0.06346578]\n [0.65855528 0.03338852]\n [0.68710481 0.09933775]\n [0.54972217 0.09602649]\n [0.4696302  0.29525386]\n [0.50028741 0.48068433]\n [0.39662771 0.7781457 ]\n [0.46809734 0.424117  ]\n [0.53956697 0.51655629]\n [0.46905537 0.31843267]\n [0.44606246 0.38576159]\n [0.45755892 0.18018764]\n [0.48572523 0.23537528]]\n","output_type":"stream"}]},{"cell_type":"code","source":"w1, w2, bias = gradient_descent(scaled_features, bostonDF['PRICE'].values, iter_epochs=1000, verbose=True)\nprint('##### 최종 w1, w2, bias #######')\nprint(w1, w2, bias)","metadata":{"execution":{"iopub.status.busy":"2024-05-18T06:09:54.539811Z","iopub.execute_input":"2024-05-18T06:09:54.540313Z","iopub.status.idle":"2024-05-18T06:09:55.007938Z","shell.execute_reply.started":"2024-05-18T06:09:54.540275Z","shell.execute_reply":"2024-05-18T06:09:55.006502Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"최초 w1, w2, bias: [0.] [0.] [0.]\nEpoch: 1 / 1000\nw1: [0.252369] w2: [0.10914761] bias: [0.45065613] loss: 592.1469169960474\nEpoch: 2 / 1000\nw1: [0.4982605] w2: [0.21458377] bias: [0.8890071] loss: 564.6567515182813\nEpoch: 3 / 1000\nw1: [0.73785103] w2: [0.31641055] bias: [1.315389] loss: 538.6424811965484\nEpoch: 4 / 1000\nw1: [0.97131229] w2: [0.41472723] bias: [1.73012873] loss: 514.0245946883915\nEpoch: 5 / 1000\nw1: [1.1988113] w2: [0.50963037] bias: [2.13354428] loss: 490.7278647125017\nEpoch: 6 / 1000\nw1: [1.42051052] w2: [0.60121392] bias: [2.52594493] loss: 468.6811172230454\nEpoch: 7 / 1000\nw1: [1.63656797] w2: [0.68956922] bias: [2.90763152] loss: 447.81701302090454\nEpoch: 8 / 1000\nw1: [1.84713735] w2: [0.77478516] bias: [3.27889669] loss: 428.07184113172934\nEpoch: 9 / 1000\nw1: [2.05236818] w2: [0.85694818] bias: [3.64002506] loss: 409.3853233168043\nEpoch: 10 / 1000\nw1: [2.25240586] w2: [0.93614234] bias: [3.9912935] loss: 391.700429116892\nEpoch: 11 / 1000\nw1: [2.44739187] w2: [1.01244944] bias: [4.3329713] loss: 374.9632008615359\nEpoch: 12 / 1000\nw1: [2.63746378] w2: [1.08594902] bias: [4.66532042] loss: 359.1225881068813\nEpoch: 13 / 1000\nw1: [2.82275543] w2: [1.15671846] bias: [4.98859562] loss: 344.1302909940069\nEpoch: 14 / 1000\nw1: [3.00339699] w2: [1.22483301] bias: [5.30304476] loss: 329.9406120471257\nEpoch: 15 / 1000\nw1: [3.17951508] w2: [1.29036588] bias: [5.60890888] loss: 316.510315956918\nEpoch: 16 / 1000\nw1: [3.35123288] w2: [1.35338828] bias: [5.90642246] loss: 303.79849691875256\nEpoch: 17 / 1000\nw1: [3.51867017] w2: [1.41396946] bias: [6.19581358] loss: 291.76645311874114\nEpoch: 18 / 1000\nw1: [3.68194348] w2: [1.47217679] bias: [6.47730407] loss: 280.37756798249774\nEpoch: 19 / 1000\nw1: [3.84116617] w2: [1.52807579] bias: [6.75110972] loss: 269.5971978222294\nEpoch: 20 / 1000\nw1: [3.99644848] w2: [1.58173021] bias: [7.01744043] loss: 259.3925655374141\nEpoch: 21 / 1000\nw1: [4.14789765] w2: [1.63320202] bias: [7.27650034] loss: 249.73266004289786\nEpoch: 22 / 1000\nw1: [4.29561799] w2: [1.68255154] bias: [7.52848803] loss: 240.58814111581867\nEpoch: 23 / 1000\nw1: [4.43971097] w2: [1.72983741] bias: [7.77359668] loss: 231.9312493693882\nEpoch: 24 / 1000\nw1: [4.58027526] w2: [1.77511669] bias: [8.01201415] loss: 223.7357210772968\nEpoch: 25 / 1000\nw1: [4.71740688] w2: [1.81844487] bias: [8.24392319] loss: 215.97670758738937\nEpoch: 26 / 1000\nw1: [4.85119919] w2: [1.85987591] bias: [8.46950157] loss: 208.63069907734152\nEpoch: 27 / 1000\nw1: [4.98174302] w2: [1.89946232] bias: [8.68892219] loss: 201.67545241838852\nEpoch: 28 / 1000\nw1: [5.10912671] w2: [1.93725517] bias: [8.90235322] loss: 195.0899229257653\nEpoch: 29 / 1000\nw1: [5.2334362] w2: [1.97330412] bias: [9.10995826] loss: 188.85419978643984\nEpoch: 30 / 1000\nw1: [5.35475506] w2: [2.00765749] bias: [9.31189642] loss: 182.94944496600834\nEpoch: 31 / 1000\nw1: [5.47316461] w2: [2.04036227] bias: [9.50832248] loss: 177.3578354072935\nEpoch: 32 / 1000\nw1: [5.58874391] w2: [2.07146416] bias: [9.69938699] loss: 172.06250834328898\nEpoch: 33 / 1000\nw1: [5.7015699] w2: [2.10100763] bias: [9.88523637] loss: 167.04750955664883\nEpoch: 34 / 1000\nw1: [5.8117174] w2: [2.12903592] bias: [10.06601306] loss: 162.29774442696137\nEpoch: 35 / 1000\nw1: [5.91925918] w2: [2.15559109] bias: [10.24185561] loss: 157.79893161560153\nEpoch: 36 / 1000\nw1: [6.02426604] w2: [2.18071405] bias: [10.41289877] loss: 153.5375592460489\nEpoch: 37 / 1000\nw1: [6.12680682] w2: [2.20444461] bias: [10.57927363] loss: 149.50084344521483\nEpoch: 38 / 1000\nw1: [6.22694852] w2: [2.22682145] bias: [10.74110768] loss: 145.676689118568\nEpoch: 39 / 1000\nw1: [6.32475628] w2: [2.24788225] bias: [10.89852494] loss: 142.05365283870012\nEpoch: 40 / 1000\nw1: [6.42029347] w2: [2.26766361] bias: [11.05164604] loss: 138.62090773346088\nEpoch: 41 / 1000\nw1: [6.51362172] w2: [2.28620115] bias: [11.20058832] loss: 135.3682102659228\nEpoch: 42 / 1000\nw1: [6.604801] w2: [2.30352952] bias: [11.3454659] loss: 132.28586880424686\nEpoch: 43 / 1000\nw1: [6.69388962] w2: [2.31968242] bias: [11.48638979] loss: 129.36471388500598\nEpoch: 44 / 1000\nw1: [6.7809443] w2: [2.33469263] bias: [11.62346799] loss: 126.59607007872512\nEpoch: 45 / 1000\nw1: [6.86602023] w2: [2.34859203] bias: [11.75680551] loss: 123.97172937130848\nEpoch: 46 / 1000\nw1: [6.94917108] w2: [2.36141163] bias: [11.88650453] loss: 121.48392597967882\nEpoch: 47 / 1000\nw1: [7.03044904] w2: [2.37318159] bias: [12.01266441] loss: 119.12531252435281\nEpoch: 48 / 1000\nw1: [7.1099049] w2: [2.38393125] bias: [12.13538181] loss: 116.88893748584081\nEpoch: 49 / 1000\nw1: [7.18758806] w2: [2.39368915] bias: [12.25475075] loss: 114.76822387569831\nEpoch: 50 / 1000\nw1: [7.26354655] w2: [2.40248304] bias: [12.37086268] loss: 112.75694905678353\nEpoch: 51 / 1000\nw1: [7.3378271] w2: [2.41033993] bias: [12.48380655] loss: 110.84922565080181\nEpoch: 52 / 1000\nw1: [7.41047519] w2: [2.41728606] bias: [12.59366889] loss: 109.0394834745537\nEpoch: 53 / 1000\nw1: [7.48153501] w2: [2.42334699] bias: [12.70053386] loss: 107.32245244945982\nEpoch: 54 / 1000\nw1: [7.55104957] w2: [2.42854755] bias: [12.80448331] loss: 105.69314643192288\nEpoch: 55 / 1000\nw1: [7.61906072] w2: [2.43291192] bias: [12.90559687] loss: 104.14684791491104\nEpoch: 56 / 1000\nw1: [7.68560913] w2: [2.43646359] bias: [13.00395199] loss: 102.67909355382197\nEpoch: 57 / 1000\nw1: [7.7507344] w2: [2.43922542] bias: [13.09962401] loss: 101.2856604722152\nEpoch: 58 / 1000\nw1: [7.81447501] w2: [2.44121965] bias: [13.1926862] loss: 99.9625533053925\nEpoch: 59 / 1000\nw1: [7.87686843] w2: [2.44246792] bias: [13.28320984] loss: 98.7059919420721\nEpoch: 60 / 1000\nw1: [7.93795106] w2: [2.44299125] bias: [13.37126426] loss: 97.51239992654277\nEpoch: 61 / 1000\nw1: [7.99775836] w2: [2.4428101] bias: [13.4569169] loss: 96.37839348571093\nEpoch: 62 / 1000\nw1: [8.05632477] w2: [2.44194439] bias: [13.54023334] loss: 95.30077114737183\nEpoch: 63 / 1000\nw1: [8.11368382] w2: [2.44041347] bias: [13.62127739] loss: 94.27650391784934\nEpoch: 64 / 1000\nw1: [8.16986812] w2: [2.43823616] bias: [13.70011112] loss: 93.3027259888657\nEpoch: 65 / 1000\nw1: [8.22490938] w2: [2.43543078] bias: [13.77679487] loss: 92.37672594512588\nEpoch: 66 / 1000\nw1: [8.27883846] w2: [2.43201514] bias: [13.85138738] loss: 91.49593844563863\nEpoch: 67 / 1000\nw1: [8.33168537] w2: [2.42800657] bias: [13.92394574] loss: 90.65793635324837\nEpoch: 68 / 1000\nw1: [8.3834793] w2: [2.42342191] bias: [13.99452553] loss: 89.86042328822934\nEpoch: 69 / 1000\nw1: [8.43424863] w2: [2.41827757] bias: [14.06318075] loss: 89.10122658309233\nEpoch: 70 / 1000\nw1: [8.48402099] w2: [2.41258948] bias: [14.12996399] loss: 88.37829061698743\nEpoch: 71 / 1000\nw1: [8.53282324] w2: [2.40637315] bias: [14.19492636] loss: 87.68967050925\nEpoch: 72 / 1000\nw1: [8.58068151] w2: [2.39964367] bias: [14.25811759] loss: 87.03352615273883\nEpoch: 73 / 1000\nw1: [8.62762121] w2: [2.39241569] bias: [14.31958604] loss: 86.40811656865867\nEpoch: 74 / 1000\nw1: [8.67366708] w2: [2.38470349] bias: [14.37937877] loss: 85.81179456554575\nEpoch: 75 / 1000\nw1: [8.71884315] w2: [2.37652095] bias: [14.43754153] loss: 85.24300168602738\nEpoch: 76 / 1000\nw1: [8.76317283] w2: [2.36788157] bias: [14.49411885] loss: 84.70026342585103\nEpoch: 77 / 1000\nw1: [8.80667886] w2: [2.35879847] bias: [14.54915401] loss: 84.18218471051222\nEpoch: 78 / 1000\nw1: [8.84938337] w2: [2.34928442] bias: [14.60268914] loss: 83.68744561560251\nEpoch: 79 / 1000\nw1: [8.8913079] w2: [2.33935186] bias: [14.65476519] loss: 83.21479731774544\nEpoch: 80 / 1000\nw1: [8.93247338] w2: [2.32901284] bias: [14.70542202] loss: 82.76305826369628\nEpoch: 81 / 1000\nw1: [8.97290019] w2: [2.31827914] bias: [14.75469837] loss: 82.33111054585146\nEpoch: 82 / 1000\nw1: [9.01260812] w2: [2.30716217] bias: [14.80263196] loss: 81.91789647304546\nEpoch: 83 / 1000\nw1: [9.05161647] w2: [2.29567305] bias: [14.84925943] loss: 81.52241532611386\nEpoch: 84 / 1000\nw1: [9.08994397] w2: [2.28382259] bias: [14.89461648] loss: 81.14372028826679\nEpoch: 85 / 1000\nw1: [9.12760885] w2: [2.27162131] bias: [14.93873778] loss: 80.78091554085417\nEpoch: 86 / 1000\nw1: [9.16462886] w2: [2.25907943] bias: [14.98165708] loss: 80.43315351561137\nEpoch: 87 / 1000\nw1: [9.20102124] w2: [2.24620691] bias: [15.02340721] loss: 80.09963229495428\nEpoch: 88 / 1000\nw1: [9.23680278] w2: [2.23301342] bias: [15.06402009] loss: 79.77959315234655\nEpoch: 89 / 1000\nw1: [9.2719898] w2: [2.21950836] bias: [15.10352679] loss: 79.47231822519215\nEpoch: 90 / 1000\nw1: [9.30659818] w2: [2.2057009] bias: [15.1419575] loss: 79.17712831311268\nEpoch: 91 / 1000\nw1: [9.34064336] w2: [2.19159994] bias: [15.1793416] loss: 78.89338079485358\nEpoch: 92 / 1000\nw1: [9.37414037] w2: [2.17721412] bias: [15.21570769] loss: 78.62046765742802\nEpoch: 93 / 1000\nw1: [9.40710382] w2: [2.16255188] bias: [15.25108356] loss: 78.35781363144996\nEpoch: 94 / 1000\nw1: [9.43954792] w2: [2.14762141] bias: [15.28549624] loss: 78.10487442693646\nEpoch: 95 / 1000\nw1: [9.47148651] w2: [2.13243067] bias: [15.31897204] loss: 77.86113506416437\nEpoch: 96 / 1000\nw1: [9.50293302] w2: [2.11698741] bias: [15.35153654] loss: 77.62610829446118\nEpoch: 97 / 1000\nw1: [9.53390055] w2: [2.10129916] bias: [15.38321463] loss: 77.39933310608332\nEpoch: 98 / 1000\nw1: [9.56440181] w2: [2.08537326] bias: [15.41403051] loss: 77.18037331059814\nEpoch: 99 / 1000\nw1: [9.59444918] w2: [2.06921684] bias: [15.44400772] loss: 76.96881620543178\nEpoch: 100 / 1000\nw1: [9.62405472] w2: [2.05283682] bias: [15.47316916] loss: 76.76427130847858\nEpoch: 101 / 1000\nw1: [9.65323012] w2: [2.03623996] bias: [15.50153711] loss: 76.56636916089059\nEpoch: 102 / 1000\nw1: [9.68198678] w2: [2.01943281] bias: [15.52913324] loss: 76.37476019437209\nEpoch: 103 / 1000\nw1: [9.71033578] w2: [2.00242174] bias: [15.55597862] loss: 76.18911365950495\nEpoch: 104 / 1000\nw1: [9.7382879] w2: [1.98521297] bias: [15.58209375] loss: 76.00911661181549\nEpoch: 105 / 1000\nw1: [9.76585362] w2: [1.96781253] bias: [15.60749856] loss: 75.83447295247232\nEpoch: 106 / 1000\nw1: [9.79304315] w2: [1.95022629] bias: [15.63221246] loss: 75.66490252067138\nEpoch: 107 / 1000\nw1: [9.8198664] w2: [1.93245995] bias: [15.65625431] loss: 75.50014023492336\nEpoch: 108 / 1000\nw1: [9.84633303] w2: [1.91451906] bias: [15.67964245] loss: 75.33993528060844\nEpoch: 109 / 1000\nw1: [9.87245242] w2: [1.89640903] bias: [15.70239474] loss: 75.18405034130593\nEpoch: 110 / 1000\nw1: [9.8982337] w2: [1.87813511] bias: [15.72452853] loss: 75.03226087153946\nEpoch: 111 / 1000\nw1: [9.92368575] w2: [1.8597024] bias: [15.74606072] loss: 74.88435440870717\nEpoch: 112 / 1000\nw1: [9.94881722] w2: [1.84111586] bias: [15.76700773] loss: 74.74012992208469\nEpoch: 113 / 1000\nw1: [9.97363651] w2: [1.82238034] bias: [15.78738553] loss: 74.59939719690419\nEpoch: 114 / 1000\nw1: [9.99815179] w2: [1.80350053] bias: [15.80720967] loss: 74.46197625161912\nEpoch: 115 / 1000\nw1: [10.02237101] w2: [1.78448099] bias: [15.82649527] loss: 74.32769678656659\nEpoch: 116 / 1000\nw1: [10.04630191] w2: [1.76532618] bias: [15.84525702] loss: 74.19639766233621\nEpoch: 117 / 1000\nw1: [10.06995201] w2: [1.74604041] bias: [15.86350923] loss: 74.06792640624403\nEpoch: 118 / 1000\nw1: [10.09332863] w2: [1.72662788] bias: [15.8812658] loss: 73.94213874539783\nEpoch: 119 / 1000\nw1: [10.1164389] w2: [1.70709269] bias: [15.89854028] loss: 73.81889816492102\nEpoch: 120 / 1000\nw1: [10.13928973] w2: [1.68743881] bias: [15.91534582] loss: 73.69807548997905\nEpoch: 121 / 1000\nw1: [10.16188786] w2: [1.6676701] bias: [15.93169522] loss: 73.57954849032653\nEpoch: 122 / 1000\nw1: [10.18423984] w2: [1.64779033] bias: [15.94760094] loss: 73.4632015061613\nEpoch: 123 / 1000\nw1: [10.20635206] w2: [1.62780315] bias: [15.96307508] loss: 73.34892509413729\nEpoch: 124 / 1000\nw1: [10.2282307] w2: [1.60771212] bias: [15.97812944] loss: 73.23661569245048\nEpoch: 125 / 1000\nw1: [10.24988181] w2: [1.5875207] bias: [15.99277546] loss: 73.12617530396972\nEpoch: 126 / 1000\nw1: [10.27131124] w2: [1.56723225] bias: [16.0070243] loss: 73.01751119644065\nEpoch: 127 / 1000\nw1: [10.2925247] w2: [1.54685004] bias: [16.0208868] loss: 72.91053561884254\nEpoch: 128 / 1000\nw1: [10.31352775] w2: [1.52637726] bias: [16.03437351] loss: 72.80516553302773\nEpoch: 129 / 1000\nw1: [10.33432578] w2: [1.50581699] bias: [16.04749467] loss: 72.70132235982048\nEpoch: 130 / 1000\nw1: [10.35492405] w2: [1.48517225] bias: [16.06026028] loss: 72.59893173879578\nEpoch: 131 / 1000\nw1: [10.37532767] w2: [1.46444596] bias: [16.07268004] loss: 72.4979233010015\nEpoch: 132 / 1000\nw1: [10.39554161] w2: [1.44364097] bias: [16.08476338] loss: 72.39823045392598\nEpoch: 133 / 1000\nw1: [10.4155707] w2: [1.42276004] bias: [16.09651949] loss: 72.2997901780517\nEpoch: 134 / 1000\nw1: [10.43541965] w2: [1.40180587] bias: [16.1079573] loss: 72.20254283437058\nEpoch: 135 / 1000\nw1: [10.45509303] w2: [1.38078107] bias: [16.11908549] loss: 72.10643198227032\nEpoch: 136 / 1000\nw1: [10.47459529] w2: [1.35968818] bias: [16.12991253] loss: 72.01140420723294\nEpoch: 137 / 1000\nw1: [10.49393077] w2: [1.33852969] bias: [16.14044663] loss: 71.91740895781695\nEpoch: 138 / 1000\nw1: [10.51310366] w2: [1.31730799] bias: [16.15069578] loss: 71.82439839142289\nEpoch: 139 / 1000\nw1: [10.53211808] w2: [1.29602543] bias: [16.16066776] loss: 71.73232722836889\nEpoch: 140 / 1000\nw1: [10.55097801] w2: [1.27468429] bias: [16.17037013] loss: 71.64115261382877\nEpoch: 141 / 1000\nw1: [10.56968731] w2: [1.25328677] bias: [16.17981026] loss: 71.55083398720868\nEpoch: 142 / 1000\nw1: [10.58824977] w2: [1.23183503] bias: [16.18899529] loss: 71.46133295856184\nEpoch: 143 / 1000\nw1: [10.60666905] w2: [1.21033116] bias: [16.1979322] loss: 71.37261319166198\nEpoch: 144 / 1000\nw1: [10.62494872] w2: [1.18877719] bias: [16.20662775] loss: 71.2846402933767\nEpoch: 145 / 1000\nw1: [10.64309225] w2: [1.16717511] bias: [16.21508853] loss: 71.1973817090013\nEpoch: 146 / 1000\nw1: [10.66110302] w2: [1.14552684] bias: [16.22332094] loss: 71.11080662323225\nEpoch: 147 / 1000\nw1: [10.67898431] w2: [1.12383424] bias: [16.23133122] loss: 71.02488586647578\nEpoch: 148 / 1000\nw1: [10.69673932] w2: [1.10209915] bias: [16.23912542] loss: 70.93959182620482\nEpoch: 149 / 1000\nw1: [10.71437116] w2: [1.08032332] bias: [16.24670945] loss: 70.85489836309186\nEpoch: 150 / 1000\nw1: [10.73188285] w2: [1.05850847] bias: [16.25408904] loss: 70.77078073166058\nEpoch: 151 / 1000\nw1: [10.74927734] w2: [1.03665629] bias: [16.26126976] loss: 70.68721550521256\nEpoch: 152 / 1000\nw1: [10.76655748] w2: [1.01476839] bias: [16.26825705] loss: 70.60418050479932\nEpoch: 153 / 1000\nw1: [10.78372606] w2: [0.99284636] bias: [16.27505617] loss: 70.52165473202065\nEpoch: 154 / 1000\nw1: [10.80078578] w2: [0.97089174] bias: [16.28167227] loss: 70.43961830544438\nEpoch: 155 / 1000\nw1: [10.81773929] w2: [0.94890601] bias: [16.28811033] loss: 70.3580524004514\nEpoch: 156 / 1000\nw1: [10.83458915] w2: [0.92689063] bias: [16.29437521] loss: 70.27693919232183\nEpoch: 157 / 1000\nw1: [10.85133784] w2: [0.90484703] bias: [16.30047164] loss: 70.19626180238784\nEpoch: 158 / 1000\nw1: [10.86798779] w2: [0.88277656] bias: [16.30640422] loss: 70.11600424708757\nEpoch: 159 / 1000\nw1: [10.88454137] w2: [0.86068056] bias: [16.3121774] loss: 70.03615138976394\nEpoch: 160 / 1000\nw1: [10.90100087] w2: [0.83856034] bias: [16.31779554] loss: 69.95668889506071\nEpoch: 161 / 1000\nw1: [10.91736852] w2: [0.81641716] bias: [16.32326287] loss: 69.87760318577526\nEpoch: 162 / 1000\nw1: [10.93364649] w2: [0.79425225] bias: [16.32858351] loss: 69.79888140203656\nEpoch: 163 / 1000\nw1: [10.9498369] w2: [0.77206679] bias: [16.33376144] loss: 69.72051136268215\nEpoch: 164 / 1000\nw1: [10.9659418] w2: [0.74986195] bias: [16.33880057] loss: 69.64248152871643\nEpoch: 165 / 1000\nw1: [10.9819632] w2: [0.72763886] bias: [16.34370467] loss: 69.56478096873751\nEpoch: 166 / 1000\nw1: [10.99790304] w2: [0.70539861] bias: [16.34847744] loss: 69.48739932622694\nEpoch: 167 / 1000\nw1: [11.01376321] w2: [0.68314228] bias: [16.35312245] loss: 69.41032678860182\nEpoch: 168 / 1000\nw1: [11.02954555] w2: [0.66087089] bias: [16.35764319] loss: 69.33355405793434\nEpoch: 169 / 1000\nw1: [11.04525186] w2: [0.63858545] bias: [16.36204304] loss: 69.25707232324882\nEpoch: 170 / 1000\nw1: [11.06088388] w2: [0.61628695] bias: [16.36632531] loss: 69.18087323431139\nEpoch: 171 / 1000\nw1: [11.0764433] w2: [0.59397634] bias: [16.37049319] loss: 69.10494887683197\nEpoch: 172 / 1000\nw1: [11.09193177] w2: [0.57165454] bias: [16.3745498] loss: 69.02929174900203\nEpoch: 173 / 1000\nw1: [11.10735091] w2: [0.54932245] bias: [16.37849819] loss: 68.95389473929666\nEpoch: 174 / 1000\nw1: [11.12270226] w2: [0.52698095] bias: [16.38234129] loss: 68.87875110547262\nEpoch: 175 / 1000\nw1: [11.13798736] w2: [0.50463089] bias: [16.38608198] loss: 68.80385445469781\nEpoch: 176 / 1000\nw1: [11.15320767] w2: [0.48227308] bias: [16.38972305] loss: 68.72919872475141\nEpoch: 177 / 1000\nw1: [11.16836464] w2: [0.45990835] bias: [16.39326722] loss: 68.65477816623682\nEpoch: 178 / 1000\nw1: [11.18345966] w2: [0.43753746] bias: [16.39671712] loss: 68.5805873257531\nEpoch: 179 / 1000\nw1: [11.1984941] w2: [0.41516118] bias: [16.40007533] loss: 68.50662102997276\nEpoch: 180 / 1000\nw1: [11.21346927] w2: [0.39278025] bias: [16.40334434] loss: 68.43287437057779\nEpoch: 181 / 1000\nw1: [11.22838648] w2: [0.37039538] bias: [16.40652658] loss: 68.359342690007\nEpoch: 182 / 1000\nw1: [11.24324696] w2: [0.34800726] bias: [16.40962443] loss: 68.28602156797125\nEpoch: 183 / 1000\nw1: [11.25805195] w2: [0.32561658] bias: [16.41264017] loss: 68.21290680869545\nEpoch: 184 / 1000\nw1: [11.27280263] w2: [0.30322399] bias: [16.41557605] loss: 68.13999442884756\nEpoch: 185 / 1000\nw1: [11.28750014] w2: [0.28083014] bias: [16.41843424] loss: 68.06728064611826\nEpoch: 186 / 1000\nw1: [11.30214562] w2: [0.25843564] bias: [16.42121685] loss: 67.99476186841567\nEpoch: 187 / 1000\nw1: [11.31674016] w2: [0.2360411] bias: [16.42392595] loss: 67.9224346836422\nEpoch: 188 / 1000\nw1: [11.33128482] w2: [0.2136471] bias: [16.42656354] loss: 67.8502958500224\nEpoch: 189 / 1000\nw1: [11.34578064] w2: [0.19125422] bias: [16.42913157] loss: 67.77834228695171\nEpoch: 190 / 1000\nw1: [11.36022862] w2: [0.16886301] bias: [16.43163192] loss: 67.70657106633824\nEpoch: 191 / 1000\nw1: [11.37462974] w2: [0.14647401] bias: [16.43406644] loss: 67.63497940441106\nEpoch: 192 / 1000\nw1: [11.38898496] w2: [0.12408774] bias: [16.43643693] loss: 67.56356465396982\nEpoch: 193 / 1000\nw1: [11.4032952] w2: [0.10170471] bias: [16.43874513] loss: 67.49232429705188\nEpoch: 194 / 1000\nw1: [11.41756136] w2: [0.07932542] bias: [16.44099273] loss: 67.42125593799447\nEpoch: 195 / 1000\nw1: [11.43178433] w2: [0.05695034] bias: [16.44318139] loss: 67.35035729687065\nEpoch: 196 / 1000\nw1: [11.44596495] w2: [0.03457994] bias: [16.4453127] loss: 67.2796262032789\nEpoch: 197 / 1000\nw1: [11.46010406] w2: [0.01221467] bias: [16.44738823] loss: 67.20906059046723\nEpoch: 198 / 1000\nw1: [11.47420247] w2: [-0.01014502] bias: [16.44940949] loss: 67.13865848977393\nEpoch: 199 / 1000\nw1: [11.48826096] w2: [-0.03249871] bias: [16.45137797] loss: 67.06841802536759\nEpoch: 200 / 1000\nw1: [11.5022803] w2: [-0.05484598] bias: [16.4532951] loss: 66.99833740927085\nEpoch: 201 / 1000\nw1: [11.51626124] w2: [-0.07718644] bias: [16.45516227] loss: 66.92841493665176\nEpoch: 202 / 1000\nw1: [11.5302045] w2: [-0.09951968] bias: [16.45698085] loss: 66.85864898136927\nEpoch: 203 / 1000\nw1: [11.54411078] w2: [-0.12184533] bias: [16.45875215] loss: 66.78903799175826\nEpoch: 204 / 1000\nw1: [11.55798078] w2: [-0.14416302] bias: [16.46047746] loss: 66.71958048664196\nEpoch: 205 / 1000\nw1: [11.57181516] w2: [-0.16647239] bias: [16.46215804] loss: 66.65027505155886\nEpoch: 206 / 1000\nw1: [11.58561457] w2: [-0.18877309] bias: [16.4637951] loss: 66.58112033519325\nEpoch: 207 / 1000\nw1: [11.59937965] w2: [-0.21106478] bias: [16.46538981] loss: 66.51211504599762\nEpoch: 208 / 1000\nw1: [11.613111] w2: [-0.23334714] bias: [16.46694334] loss: 66.44325794899729\nEpoch: 209 / 1000\nw1: [11.62680924] w2: [-0.25561984] bias: [16.46845681] loss: 66.3745478627671\nEpoch: 210 / 1000\nw1: [11.64047494] w2: [-0.27788257] bias: [16.46993129] loss: 66.30598365657083\nEpoch: 211 / 1000\nw1: [11.65410867] w2: [-0.30013504] bias: [16.47136785] loss: 66.23756424765483\nEpoch: 212 / 1000\nw1: [11.66771098] w2: [-0.32237695] bias: [16.47276753] loss: 66.16928859868744\nEpoch: 213 / 1000\nw1: [11.68128242] w2: [-0.34460802] bias: [16.47413131] loss: 66.1011557153361\nEpoch: 214 / 1000\nw1: [11.6948235] w2: [-0.36682797] bias: [16.47546019] loss: 66.03316464397535\nEpoch: 215 / 1000\nw1: [11.70833473] w2: [-0.38903653] bias: [16.47675509] loss: 65.96531446951785\nEpoch: 216 / 1000\nw1: [11.72181661] w2: [-0.41123345] bias: [16.47801696] loss: 65.89760431336252\nEpoch: 217 / 1000\nw1: [11.73526963] w2: [-0.43341847] bias: [16.47924668] loss: 65.8300333314531\nEpoch: 218 / 1000\nw1: [11.74869425] w2: [-0.45559136] bias: [16.48044512] loss: 65.76260071244121\nEpoch: 219 / 1000\nw1: [11.76209092] w2: [-0.47775186] bias: [16.48161314] loss: 65.69530567594838\nEpoch: 220 / 1000\nw1: [11.7754601] w2: [-0.49989976] bias: [16.48275157] loss: 65.62814747092176\nEpoch: 221 / 1000\nw1: [11.78880222] w2: [-0.52203482] bias: [16.48386119] loss: 65.56112537407823\nEpoch: 222 / 1000\nw1: [11.8021177] w2: [-0.54415684] bias: [16.4849428] loss: 65.4942386884323\nEpoch: 223 / 1000\nw1: [11.81540694] w2: [-0.5662656] bias: [16.48599716] loss: 65.42748674190352\nEpoch: 224 / 1000\nw1: [11.82867036] w2: [-0.5883609] bias: [16.487025] loss: 65.36086888599854\nEpoch: 225 / 1000\nw1: [11.84190833] w2: [-0.61044254] bias: [16.48802704] loss: 65.29438449456438\nEpoch: 226 / 1000\nw1: [11.85512124] w2: [-0.63251033] bias: [16.48900398] loss: 65.2280329626087\nEpoch: 227 / 1000\nw1: [11.86830945] w2: [-0.65456407] bias: [16.48995651] loss: 65.16181370518369\nEpoch: 228 / 1000\nw1: [11.88147332] w2: [-0.67660361] bias: [16.49088527] loss: 65.09572615632992\nEpoch: 229 / 1000\nw1: [11.89461321] w2: [-0.69862874] bias: [16.49179093] loss: 65.02976976807719\nEpoch: 230 / 1000\nw1: [11.90772945] w2: [-0.72063932] bias: [16.4926741] loss: 64.96394400949906\nEpoch: 231 / 1000\nw1: [11.92082237] w2: [-0.74263516] bias: [16.49353538] loss: 64.89824836581829\nEpoch: 232 / 1000\nw1: [11.9338923] w2: [-0.76461612] bias: [16.49437538] loss: 64.83268233756058\nEpoch: 233 / 1000\nw1: [11.94693954] w2: [-0.78658204] bias: [16.49519467] loss: 64.76724543975361\nEpoch: 234 / 1000\nw1: [11.95996442] w2: [-0.80853277] bias: [16.49599381] loss: 64.70193720116941\nEpoch: 235 / 1000\nw1: [11.97296721] w2: [-0.83046816] bias: [16.49677335] loss: 64.63675716360734\nEpoch: 236 / 1000\nw1: [11.98594821] w2: [-0.85238807] bias: [16.49753381] loss: 64.57170488121584\nEpoch: 237 / 1000\nw1: [11.99890771] w2: [-0.87429237] bias: [16.49827571] loss: 64.50677991985049\nEpoch: 238 / 1000\nw1: [12.01184598] w2: [-0.89618093] bias: [16.49899955] loss: 64.44198185646673\nEpoch: 239 / 1000\nw1: [12.02476328] w2: [-0.91805361] bias: [16.49970582] loss: 64.37731027854525\nEpoch: 240 / 1000\nw1: [12.03765988] w2: [-0.93991029] bias: [16.500395] loss: 64.31276478354827\nEpoch: 241 / 1000\nw1: [12.05053603] w2: [-0.96175085] bias: [16.50106754] loss: 64.24834497840502\nEpoch: 242 / 1000\nw1: [12.06339198] w2: [-0.98357518] bias: [16.5017239] loss: 64.18405047902498\nEpoch: 243 / 1000\nw1: [12.07622796] w2: [-1.00538317] bias: [16.5023645] loss: 64.11988090983729\nEpoch: 244 / 1000\nw1: [12.08904421] w2: [-1.0271747] bias: [16.50298979] loss: 64.05583590335493\nEpoch: 245 / 1000\nw1: [12.10184097] w2: [-1.04894967] bias: [16.50360016] loss: 63.991915099762245\nEpoch: 246 / 1000\nw1: [12.11461844] w2: [-1.07070797] bias: [16.50419603] loss: 63.928118146524795\nEpoch: 247 / 1000\nw1: [12.12737685] w2: [-1.09244952] bias: [16.50477777] loss: 63.86444469802013\nEpoch: 248 / 1000\nw1: [12.14011642] w2: [-1.11417421] bias: [16.50534578] loss: 63.800894415188324\nEpoch: 249 / 1000\nw1: [12.15283733] w2: [-1.13588195] bias: [16.50590043] loss: 63.73746696520142\nEpoch: 250 / 1000\nw1: [12.1655398] w2: [-1.15757265] bias: [16.50644206] loss: 63.674162021150615\nEpoch: 251 / 1000\nw1: [12.17822402] w2: [-1.17924623] bias: [16.50697104] loss: 63.61097926175018\nEpoch: 252 / 1000\nw1: [12.19089018] w2: [-1.2009026] bias: [16.5074877] loss: 63.547918371057364\nEpoch: 253 / 1000\nw1: [12.20353846] w2: [-1.22254168] bias: [16.50799237] loss: 63.484979038207356\nEpoch: 254 / 1000\nw1: [12.21616905] w2: [-1.24416339] bias: [16.50848538] loss: 63.42216095716239\nEpoch: 255 / 1000\nw1: [12.22878211] w2: [-1.26576767] bias: [16.50896704] loss: 63.359463826474396\nEpoch: 256 / 1000\nw1: [12.24137783] w2: [-1.28735443] bias: [16.50943765] loss: 63.29688734906047\nEpoch: 257 / 1000\nw1: [12.25395636] w2: [-1.3089236] bias: [16.50989751] loss: 63.23443123199023\nEpoch: 258 / 1000\nw1: [12.26651787] w2: [-1.33047512] bias: [16.51034691] loss: 63.17209518628465\nEpoch: 259 / 1000\nw1: [12.27906252] w2: [-1.35200893] bias: [16.51078613] loss: 63.10987892672584\nEpoch: 260 / 1000\nw1: [12.29159047] w2: [-1.37352495] bias: [16.51121545] loss: 63.04778217167676\nEpoch: 261 / 1000\nw1: [12.30410185] w2: [-1.39502314] bias: [16.51163512] loss: 62.9858046429108\nEpoch: 262 / 1000\nw1: [12.31659682] w2: [-1.41650342] bias: [16.5120454] loss: 62.9239460654505\nEpoch: 263 / 1000\nw1: [12.32907553] w2: [-1.43796575] bias: [16.51244655] loss: 62.86220616741479\nEpoch: 264 / 1000\nw1: [12.34153811] w2: [-1.45941007] bias: [16.51283882] loss: 62.800584679874525\nEpoch: 265 / 1000\nw1: [12.35398469] w2: [-1.48083633] bias: [16.51322243] loss: 62.73908133671575\nEpoch: 266 / 1000\nw1: [12.36641541] w2: [-1.50224447] bias: [16.51359762] loss: 62.67769587451019\nEpoch: 267 / 1000\nw1: [12.3788304] w2: [-1.52363445] bias: [16.51396461] loss: 62.616428032392875\nEpoch: 268 / 1000\nw1: [12.39122978] w2: [-1.54500622] bias: [16.51432363] loss: 62.55527755194609\nEpoch: 269 / 1000\nw1: [12.40361368] w2: [-1.56635974] bias: [16.51467488] loss: 62.49424417708968\nEpoch: 270 / 1000\nw1: [12.41598222] w2: [-1.58769496] bias: [16.51501858] loss: 62.43332765397722\nEpoch: 271 / 1000\nw1: [12.4283355] w2: [-1.60901184] bias: [16.51535492] loss: 62.37252773089763\nEpoch: 272 / 1000\nw1: [12.44067365] w2: [-1.63031035] bias: [16.5156841] loss: 62.311844158182176\nEpoch: 273 / 1000\nw1: [12.45299678] w2: [-1.65159043] bias: [16.5160063] loss: 62.25127668811635\nEpoch: 274 / 1000\nw1: [12.46530499] w2: [-1.67285207] bias: [16.51632173] loss: 62.190825074856534\nEpoch: 275 / 1000\nw1: [12.4775984] w2: [-1.69409522] bias: [16.51663054] loss: 62.13048907435101\nEpoch: 276 / 1000\nw1: [12.48987709] w2: [-1.71531984] bias: [16.51693293] loss: 62.070268444265295\nEpoch: 277 / 1000\nw1: [12.50214118] w2: [-1.73652591] bias: [16.51722906] loss: 62.01016294391137\nEpoch: 278 / 1000\nw1: [12.51439076] w2: [-1.7577134] bias: [16.5175191] loss: 61.95017233418076\nEpoch: 279 / 1000\nw1: [12.52662592] w2: [-1.77888227] bias: [16.5178032] loss: 61.89029637748107\nEpoch: 280 / 1000\nw1: [12.53884677] w2: [-1.8000325] bias: [16.51808153] loss: 61.83053483767602\nEpoch: 281 / 1000\nw1: [12.55105338] w2: [-1.82116406] bias: [16.51835423] loss: 61.7708874800286\nEpoch: 282 / 1000\nw1: [12.56324586] w2: [-1.84227693] bias: [16.51862146] loss: 61.71135407114721\nEpoch: 283 / 1000\nw1: [12.57542428] w2: [-1.86337108] bias: [16.51888336] loss: 61.65193437893482\nEpoch: 284 / 1000\nw1: [12.58758873] w2: [-1.88444648] bias: [16.51914007] loss: 61.592628172540586\nEpoch: 285 / 1000\nw1: [12.59973929] w2: [-1.90550312] bias: [16.51939173] loss: 61.53343522231427\nEpoch: 286 / 1000\nw1: [12.61187604] w2: [-1.92654097] bias: [16.51963847] loss: 61.4743552997629\nEpoch: 287 / 1000\nw1: [12.62399907] w2: [-1.94756002] bias: [16.51988042] loss: 61.41538817750986\nEpoch: 288 / 1000\nw1: [12.63610844] w2: [-1.96856024] bias: [16.5201177] loss: 61.356533629256006\nEpoch: 289 / 1000\nw1: [12.64820424] w2: [-1.98954162] bias: [16.52035044] loss: 61.29779142974298\nEpoch: 290 / 1000\nw1: [12.66028653] w2: [-2.01050414] bias: [16.52057876] loss: 61.23916135471831\nEpoch: 291 / 1000\nw1: [12.67235539] w2: [-2.03144779] bias: [16.52080276] loss: 61.180643180902486\nEpoch: 292 / 1000\nw1: [12.68441089] w2: [-2.05237254] bias: [16.52102258] loss: 61.12223668595771\nEpoch: 293 / 1000\nw1: [12.6964531] w2: [-2.07327839] bias: [16.5212383] loss: 61.06394164845826\nEpoch: 294 / 1000\nw1: [12.70848207] w2: [-2.09416533] bias: [16.52145005] loss: 61.00575784786245\nEpoch: 295 / 1000\nw1: [12.72049788] w2: [-2.11503333] bias: [16.52165792] loss: 60.947685064486066\nEpoch: 296 / 1000\nw1: [12.7325006] w2: [-2.13588239] bias: [16.52186202] loss: 60.88972307947717\nEpoch: 297 / 1000\nw1: [12.74449027] w2: [-2.15671251] bias: [16.52206244] loss: 60.83187167479224\nEpoch: 298 / 1000\nw1: [12.75646697] w2: [-2.17752366] bias: [16.52225928] loss: 60.77413063317353\nEpoch: 299 / 1000\nw1: [12.76843075] w2: [-2.19831584] bias: [16.52245263] loss: 60.71649973812762\nEpoch: 300 / 1000\nw1: [12.78038167] w2: [-2.21908904] bias: [16.52264258] loss: 60.65897877390512\nEpoch: 301 / 1000\nw1: [12.79231979] w2: [-2.23984325] bias: [16.52282922] loss: 60.60156752548139\nEpoch: 302 / 1000\nw1: [12.80424516] w2: [-2.26057848] bias: [16.52301264] loss: 60.54426577853821\nEpoch: 303 / 1000\nw1: [12.81615784] w2: [-2.2812947] bias: [16.52319291] loss: 60.48707331944654\nEpoch: 304 / 1000\nw1: [12.82805788] w2: [-2.30199192] bias: [16.52337012] loss: 60.42998993525001\nEpoch: 305 / 1000\nw1: [12.83994533] w2: [-2.32267012] bias: [16.52354435] loss: 60.37301541364937\nEpoch: 306 / 1000\nw1: [12.85182024] w2: [-2.34332932] bias: [16.52371567] loss: 60.316149542987695\nEpoch: 307 / 1000\nw1: [12.86368266] w2: [-2.36396949] bias: [16.52388416] loss: 60.259392112236334\nEpoch: 308 / 1000\nw1: [12.87553264] w2: [-2.38459065] bias: [16.52404989] loss: 60.20274291098164\nEpoch: 309 / 1000\nw1: [12.88737023] w2: [-2.40519278] bias: [16.52421293] loss: 60.14620172941228\nEpoch: 310 / 1000\nw1: [12.89919547] w2: [-2.42577589] bias: [16.52437335] loss: 60.089768358307275\nEpoch: 311 / 1000\nw1: [12.91100841] w2: [-2.44633997] bias: [16.52453122] loss: 60.033442589024595\nEpoch: 312 / 1000\nw1: [12.9228091] w2: [-2.46688503] bias: [16.5246866] loss: 59.977224213490395\nEpoch: 313 / 1000\nw1: [12.93459757] w2: [-2.48741106] bias: [16.52483955] loss: 59.92111302418865\nEpoch: 314 / 1000\nw1: [12.94637387] w2: [-2.50791806] bias: [16.52499013] loss: 59.865108814151505\nEpoch: 315 / 1000\nw1: [12.95813804] w2: [-2.52840603] bias: [16.52513841] loss: 59.80921137694995\nEpoch: 316 / 1000\nw1: [12.96989013] w2: [-2.54887498] bias: [16.52528444] loss: 59.75342050668501\nEpoch: 317 / 1000\nw1: [12.98163017] w2: [-2.56932491] bias: [16.52542828] loss: 59.69773599797938\nEpoch: 318 / 1000\nw1: [12.9933582] w2: [-2.58975581] bias: [16.52556999] loss: 59.642157645969526\nEpoch: 319 / 1000\nw1: [13.00507425] w2: [-2.6101677] bias: [16.52570961] loss: 59.586685246298\nEpoch: 320 / 1000\nw1: [13.01677838] w2: [-2.63056057] bias: [16.5258472] loss: 59.531318595106384\nEpoch: 321 / 1000\nw1: [13.02847061] w2: [-2.65093443] bias: [16.52598281] loss: 59.476057489028285\nEpoch: 322 / 1000\nw1: [13.04015098] w2: [-2.67128928] bias: [16.52611649] loss: 59.42090172518296\nEpoch: 323 / 1000\nw1: [13.05181953] w2: [-2.69162513] bias: [16.52624829] loss: 59.36585110116902\nEpoch: 324 / 1000\nw1: [13.06347628] w2: [-2.71194197] bias: [16.52637824] loss: 59.31090541505856\nEpoch: 325 / 1000\nw1: [13.07512128] w2: [-2.73223982] bias: [16.52650641] loss: 59.25606446539152\nEpoch: 326 / 1000\nw1: [13.08675456] w2: [-2.75251868] bias: [16.52663283] loss: 59.20132805117033\nEpoch: 327 / 1000\nw1: [13.09837615] w2: [-2.77277856] bias: [16.52675755] loss: 59.14669597185476\nEpoch: 328 / 1000\nw1: [13.10998609] w2: [-2.79301946] bias: [16.5268806] loss: 59.09216802735712\nEpoch: 329 / 1000\nw1: [13.12158439] w2: [-2.81324139] bias: [16.52700203] loss: 59.037744018037564\nEpoch: 330 / 1000\nw1: [13.13317111] w2: [-2.83344435] bias: [16.52712188] loss: 58.98342374469963\nEpoch: 331 / 1000\nw1: [13.14474625] w2: [-2.85362835] bias: [16.52724018] loss: 58.92920700858605\nEpoch: 332 / 1000\nw1: [13.15630987] w2: [-2.87379341] bias: [16.52735697] loss: 58.875093611374695\nEpoch: 333 / 1000\nw1: [13.16786198] w2: [-2.89393952] bias: [16.5274723] loss: 58.82108335517469\nEpoch: 334 / 1000\nw1: [13.17940261] w2: [-2.91406669] bias: [16.52758618] loss: 58.76717604252276\nEpoch: 335 / 1000\nw1: [13.19093179] w2: [-2.93417494] bias: [16.52769867] loss: 58.71337147637967\nEpoch: 336 / 1000\nw1: [13.20244956] w2: [-2.95426426] bias: [16.52780979] loss: 58.65966946012686\nEpoch: 337 / 1000\nw1: [13.21395593] w2: [-2.97433468] bias: [16.52791957] loss: 58.60606979756316\nEpoch: 338 / 1000\nw1: [13.22545093] w2: [-2.99438619] bias: [16.52802805] loss: 58.552572292901786\nEpoch: 339 / 1000\nw1: [13.23693459] w2: [-3.01441881] bias: [16.52813525] loss: 58.49917675076733\nEpoch: 340 / 1000\nw1: [13.24840694] w2: [-3.03443255] bias: [16.52824122] loss: 58.44588297619291\nEpoch: 341 / 1000\nw1: [13.25986801] w2: [-3.05442741] bias: [16.52834596] loss: 58.392690774617414\nEpoch: 342 / 1000\nw1: [13.2713178] w2: [-3.07440341] bias: [16.52844953] loss: 58.33959995188297\nEpoch: 343 / 1000\nw1: [13.28275636] w2: [-3.09436055] bias: [16.52855193] loss: 58.28661031423232\nEpoch: 344 / 1000\nw1: [13.2941837] w2: [-3.11429886] bias: [16.5286532] loss: 58.233721668306536\nEpoch: 345 / 1000\nw1: [13.30559985] w2: [-3.13421832] bias: [16.52875337] loss: 58.18093382114259\nEpoch: 346 / 1000\nw1: [13.31700484] w2: [-3.15411896] bias: [16.52885246] loss: 58.12824658017117\nEpoch: 347 / 1000\nw1: [13.32839868] w2: [-3.17400079] bias: [16.52895049] loss: 58.07565975321453\nEpoch: 348 / 1000\nw1: [13.33978139] w2: [-3.19386382] bias: [16.52904749] loss: 58.02317314848443\nEpoch: 349 / 1000\nw1: [13.35115301] w2: [-3.21370806] bias: [16.52914348] loss: 57.97078657458009\nEpoch: 350 / 1000\nw1: [13.36251355] w2: [-3.23353352] bias: [16.52923849] loss: 57.918499840486355\nEpoch: 351 / 1000\nw1: [13.37386303] w2: [-3.25334021] bias: [16.52933254] loss: 57.866312755571784\nEpoch: 352 / 1000\nw1: [13.38520148] w2: [-3.27312815] bias: [16.52942564] loss: 57.81422512958685\nEpoch: 353 / 1000\nw1: [13.39652892] w2: [-3.29289734] bias: [16.52951782] loss: 57.76223677266224\nEpoch: 354 / 1000\nw1: [13.40784536] w2: [-3.3126478] bias: [16.52960911] loss: 57.7103474953072\nEpoch: 355 / 1000\nw1: [13.41915083] w2: [-3.33237954] bias: [16.52969951] loss: 57.65855710840784\nEpoch: 356 / 1000\nw1: [13.43044534] w2: [-3.35209258] bias: [16.52978905] loss: 57.60686542322565\nEpoch: 357 / 1000\nw1: [13.44172893] w2: [-3.37178691] bias: [16.52987775] loss: 57.55527225139593\nEpoch: 358 / 1000\nw1: [13.4530016] w2: [-3.39146257] bias: [16.52996563] loss: 57.50377740492633\nEpoch: 359 / 1000\nw1: [13.46426337] w2: [-3.41111955] bias: [16.5300527] loss: 57.45238069619545\nEpoch: 360 / 1000\nw1: [13.47551428] w2: [-3.43075788] bias: [16.53013898] loss: 57.401081937951425\nEpoch: 361 / 1000\nw1: [13.48675432] w2: [-3.45037756] bias: [16.53022448] loss: 57.34988094331056\nEpoch: 362 / 1000\nw1: [13.49798353] w2: [-3.46997861] bias: [16.53030924] loss: 57.29877752575602\nEpoch: 363 / 1000\nw1: [13.50920192] w2: [-3.48956105] bias: [16.53039325] loss: 57.24777149913666\nEpoch: 364 / 1000\nw1: [13.52040951] w2: [-3.50912487] bias: [16.53047654] loss: 57.19686267766562\nEpoch: 365 / 1000\nw1: [13.53160631] w2: [-3.52867011] bias: [16.53055911] loss: 57.146050875919215\nEpoch: 366 / 1000\nw1: [13.54279235] w2: [-3.54819677] bias: [16.530641] loss: 57.095335908835736\nEpoch: 367 / 1000\nw1: [13.55396764] w2: [-3.56770486] bias: [16.5307222] loss: 57.044717591714296\nEpoch: 368 / 1000\nw1: [13.5651322] w2: [-3.58719441] bias: [16.53080274] loss: 56.99419574021369\nEpoch: 369 / 1000\nw1: [13.57628604] w2: [-3.60666541] bias: [16.53088262] loss: 56.94377017035137\nEpoch: 370 / 1000\nw1: [13.58742919] w2: [-3.6261179] bias: [16.53096187] loss: 56.89344069850224\nEpoch: 371 / 1000\nw1: [13.59856166] w2: [-3.64555187] bias: [16.53104048] loss: 56.84320714139771\nEpoch: 372 / 1000\nw1: [13.60968346] w2: [-3.66496735] bias: [16.53111849] loss: 56.79306931612463\nEpoch: 373 / 1000\nw1: [13.62079461] w2: [-3.68436435] bias: [16.53119589] loss: 56.743027040124296\nEpoch: 374 / 1000\nw1: [13.63189513] w2: [-3.70374288] bias: [16.53127269] loss: 56.69308013119144\nEpoch: 375 / 1000\nw1: [13.64298504] w2: [-3.72310296] bias: [16.53134892] loss: 56.643228407473266\nEpoch: 376 / 1000\nw1: [13.65406434] w2: [-3.74244461] bias: [16.53142459] loss: 56.5934716874685\nEpoch: 377 / 1000\nw1: [13.66513306] w2: [-3.76176782] bias: [16.53149969] loss: 56.54380979002649\nEpoch: 378 / 1000\nw1: [13.6761912] w2: [-3.78107263] bias: [16.53157425] loss: 56.494242534346206\nEpoch: 379 / 1000\nw1: [13.6872388] w2: [-3.80035905] bias: [16.53164827] loss: 56.444769739975406\nEpoch: 380 / 1000\nw1: [13.69827585] w2: [-3.81962709] bias: [16.53172177] loss: 56.39539122680973\nEpoch: 381 / 1000\nw1: [13.70930238] w2: [-3.83887676] bias: [16.53179474] loss: 56.346106815091844\nEpoch: 382 / 1000\nw1: [13.7203184] w2: [-3.85810808] bias: [16.53186722] loss: 56.29691632541049\nEpoch: 383 / 1000\nw1: [13.73132392] w2: [-3.87732106] bias: [16.53193919] loss: 56.247819578699755\nEpoch: 384 / 1000\nw1: [13.74231896] w2: [-3.89651573] bias: [16.53201067] loss: 56.198816396238136\nEpoch: 385 / 1000\nw1: [13.75330353] w2: [-3.91569209] bias: [16.53208168] loss: 56.1499065996478\nEpoch: 386 / 1000\nw1: [13.76427766] w2: [-3.93485016] bias: [16.53215221] loss: 56.10109001089366\nEpoch: 387 / 1000\nw1: [13.77524134] w2: [-3.95398995] bias: [16.53222228] loss: 56.05236645228269\nEpoch: 388 / 1000\nw1: [13.7861946] w2: [-3.97311148] bias: [16.53229189] loss: 56.003735746463036\nEpoch: 389 / 1000\nw1: [13.79713744] w2: [-3.99221477] bias: [16.53236106] loss: 55.95519771642328\nEpoch: 390 / 1000\nw1: [13.80806989] w2: [-4.01129983] bias: [16.53242979] loss: 55.90675218549161\nEpoch: 391 / 1000\nw1: [13.81899196] w2: [-4.03036668] bias: [16.53249808] loss: 55.85839897733516\nEpoch: 392 / 1000\nw1: [13.82990365] w2: [-4.04941532] bias: [16.53256595] loss: 55.810137915959096\nEpoch: 393 / 1000\nw1: [13.84080499] w2: [-4.06844579] bias: [16.5326334] loss: 55.76196882570598\nEpoch: 394 / 1000\nw1: [13.85169599] w2: [-4.08745808] bias: [16.53270044] loss: 55.71389153125495\nEpoch: 395 / 1000\nw1: [13.86257665] w2: [-4.10645222] bias: [16.53276707] loss: 55.665905857621084\nEpoch: 396 / 1000\nw1: [13.873447] w2: [-4.12542822] bias: [16.53283331] loss: 55.61801163015454\nEpoch: 397 / 1000\nw1: [13.88430704] w2: [-4.14438611] bias: [16.53289916] loss: 55.57020867453988\nEpoch: 398 / 1000\nw1: [13.89515679] w2: [-4.16332588] bias: [16.53296461] loss: 55.52249681679541\nEpoch: 399 / 1000\nw1: [13.90599626] w2: [-4.18224756] bias: [16.53302969] loss: 55.474875883272354\nEpoch: 400 / 1000\nw1: [13.91682547] w2: [-4.20115117] bias: [16.5330944] loss: 55.42734570065424\nEpoch: 401 / 1000\nw1: [13.92764442] w2: [-4.22003672] bias: [16.53315873] loss: 55.37990609595616\nEpoch: 402 / 1000\nw1: [13.93845312] w2: [-4.23890423] bias: [16.53322271] loss: 55.33255689652407\nEpoch: 403 / 1000\nw1: [13.9492516] w2: [-4.2577537] bias: [16.53328632] loss: 55.285297930034055\nEpoch: 404 / 1000\nw1: [13.96003987] w2: [-4.27658517] bias: [16.53334959] loss: 55.23812902449176\nEpoch: 405 / 1000\nw1: [13.97081793] w2: [-4.29539864] bias: [16.53341251] loss: 55.19105000823158\nEpoch: 406 / 1000\nw1: [13.98158579] w2: [-4.31419412] bias: [16.53347508] loss: 55.14406070991603\nEpoch: 407 / 1000\nw1: [13.99234348] w2: [-4.33297164] bias: [16.53353732] loss: 55.09716095853509\nEpoch: 408 / 1000\nw1: [14.003091] w2: [-4.35173121] bias: [16.53359923] loss: 55.050350583405496\nEpoch: 409 / 1000\nw1: [14.01382836] w2: [-4.37047285] bias: [16.5336608] loss: 55.0036294141701\nEpoch: 410 / 1000\nw1: [14.02455557] w2: [-4.38919657] bias: [16.53372206] loss: 54.95699728079718\nEpoch: 411 / 1000\nw1: [14.03527266] w2: [-4.40790239] bias: [16.53378299] loss: 54.9104540135798\nEpoch: 412 / 1000\nw1: [14.04597962] w2: [-4.42659032] bias: [16.53384361] loss: 54.863999443135164\nEpoch: 413 / 1000\nw1: [14.05667647] w2: [-4.44526039] bias: [16.53390392] loss: 54.81763340040391\nEpoch: 414 / 1000\nw1: [14.06736322] w2: [-4.4639126] bias: [16.53396392] loss: 54.7713557166495\nEpoch: 415 / 1000\nw1: [14.07803989] w2: [-4.48254697] bias: [16.53402362] loss: 54.72516622345763\nEpoch: 416 / 1000\nw1: [14.08870649] w2: [-4.50116351] bias: [16.53408302] loss: 54.679064752735414\nEpoch: 417 / 1000\nw1: [14.09936301] w2: [-4.51976226] bias: [16.53414213] loss: 54.63305113671093\nEpoch: 418 / 1000\nw1: [14.11000949] w2: [-4.53834321] bias: [16.53420094] loss: 54.587125207932466\nEpoch: 419 / 1000\nw1: [14.12064593] w2: [-4.55690638] bias: [16.53425947] loss: 54.541286799267944\nEpoch: 420 / 1000\nw1: [14.13127233] w2: [-4.5754518] bias: [16.53431771] loss: 54.49553574390424\nEpoch: 421 / 1000\nw1: [14.14188872] w2: [-4.59397948] bias: [16.53437567] loss: 54.44987187534657\nEpoch: 422 / 1000\nw1: [14.1524951] w2: [-4.61248943] bias: [16.53443335] loss: 54.40429502741787\nEpoch: 423 / 1000\nw1: [14.16309148] w2: [-4.63098167] bias: [16.53449075] loss: 54.35880503425819\nEpoch: 424 / 1000\nw1: [14.17367787] w2: [-4.64945622] bias: [16.53454789] loss: 54.31340173032401\nEpoch: 425 / 1000\nw1: [14.1842543] w2: [-4.66791308] bias: [16.53460475] loss: 54.26808495038766\nEpoch: 426 / 1000\nw1: [14.19482075] w2: [-4.68635229] bias: [16.53466135] loss: 54.22285452953672\nEpoch: 427 / 1000\nw1: [14.20537726] w2: [-4.70477385] bias: [16.53471768] loss: 54.17771030317333\nEpoch: 428 / 1000\nw1: [14.21592382] w2: [-4.72317778] bias: [16.53477376] loss: 54.13265210701367\nEpoch: 429 / 1000\nw1: [14.22646045] w2: [-4.7415641] bias: [16.53482958] loss: 54.08767977708726\nEpoch: 430 / 1000\nw1: [14.23698716] w2: [-4.75993282] bias: [16.53488514] loss: 54.0427931497364\nEpoch: 431 / 1000\nw1: [14.24750396] w2: [-4.77828395] bias: [16.53494045] loss: 53.99799206161556\nEpoch: 432 / 1000\nw1: [14.25801086] w2: [-4.79661753] bias: [16.53499551] loss: 53.953276349690725\nEpoch: 433 / 1000\nw1: [14.26850787] w2: [-4.81493355] bias: [16.53505032] loss: 53.90864585123886\nEpoch: 434 / 1000\nw1: [14.278995] w2: [-4.83323204] bias: [16.53510488] loss: 53.86410040384724\nEpoch: 435 / 1000\nw1: [14.28947226] w2: [-4.85151301] bias: [16.53515921] loss: 53.819639845412894\nEpoch: 436 / 1000\nw1: [14.29993967] w2: [-4.86977648] bias: [16.53521329] loss: 53.775264014141996\nEpoch: 437 / 1000\nw1: [14.31039723] w2: [-4.88802246] bias: [16.53526713] loss: 53.73097274854922\nEpoch: 438 / 1000\nw1: [14.32084495] w2: [-4.90625098] bias: [16.53532074] loss: 53.68676588745722\nEpoch: 439 / 1000\nw1: [14.33128285] w2: [-4.92446204] bias: [16.53537412] loss: 53.64264326999598\nEpoch: 440 / 1000\nw1: [14.34171092] w2: [-4.94265567] bias: [16.53542726] loss: 53.59860473560223\nEpoch: 441 / 1000\nw1: [14.3521292] w2: [-4.96083188] bias: [16.53548017] loss: 53.55465012401887\nEpoch: 442 / 1000\nw1: [14.36253768] w2: [-4.97899068] bias: [16.53553285] loss: 53.510779275294375\nEpoch: 443 / 1000\nw1: [14.37293637] w2: [-4.99713209] bias: [16.53558531] loss: 53.46699202978217\nEpoch: 444 / 1000\nw1: [14.38332528] w2: [-5.01525613] bias: [16.53563754] loss: 53.4232882281401\nEpoch: 445 / 1000\nw1: [14.39370443] w2: [-5.03336282] bias: [16.53568955] loss: 53.379667711329795\nEpoch: 446 / 1000\nw1: [14.40407383] w2: [-5.05145216] bias: [16.53574134] loss: 53.336130320616135\nEpoch: 447 / 1000\nw1: [14.41443348] w2: [-5.06952418] bias: [16.53579291] loss: 53.29267589756658\nEpoch: 448 / 1000\nw1: [14.4247834] w2: [-5.0875789] bias: [16.53584426] loss: 53.24930428405072\nEpoch: 449 / 1000\nw1: [14.43512359] w2: [-5.10561632] bias: [16.5358954] loss: 53.206015322239566\nEpoch: 450 / 1000\nw1: [14.44545406] w2: [-5.12363646] bias: [16.53594632] loss: 53.162808854605025\nEpoch: 451 / 1000\nw1: [14.45577483] w2: [-5.14163935] bias: [16.53599703] loss: 53.11968472391935\nEpoch: 452 / 1000\nw1: [14.46608591] w2: [-5.15962499] bias: [16.53604752] loss: 53.0766427732545\nEpoch: 453 / 1000\nw1: [14.4763873] w2: [-5.17759341] bias: [16.53609781] loss: 53.03368284598163\nEpoch: 454 / 1000\nw1: [14.48667901] w2: [-5.19554461] bias: [16.53614789] loss: 52.99080478577048\nEpoch: 455 / 1000\nw1: [14.49696106] w2: [-5.21347862] bias: [16.53619776] loss: 52.94800843658879\nEpoch: 456 / 1000\nw1: [14.50723345] w2: [-5.23139545] bias: [16.53624743] loss: 52.90529364270177\nEpoch: 457 / 1000\nw1: [14.5174962] w2: [-5.24929511] bias: [16.53629689] loss: 52.8626602486715\nEpoch: 458 / 1000\nw1: [14.52774931] w2: [-5.26717763] bias: [16.53634615] loss: 52.82010809935634\nEpoch: 459 / 1000\nw1: [14.53799279] w2: [-5.28504302] bias: [16.53639521] loss: 52.77763703991044\nEpoch: 460 / 1000\nw1: [14.54822665] w2: [-5.30289129] bias: [16.53644406] loss: 52.7352469157831\nEpoch: 461 / 1000\nw1: [14.55845091] w2: [-5.32072246] bias: [16.53649272] loss: 52.692937572718215\nEpoch: 462 / 1000\nw1: [14.56866557] w2: [-5.33853655] bias: [16.53654118] loss: 52.65070885675375\nEpoch: 463 / 1000\nw1: [14.57887064] w2: [-5.35633357] bias: [16.53658944] loss: 52.60856061422115\nEpoch: 464 / 1000\nw1: [14.58906614] w2: [-5.37411354] bias: [16.53663751] loss: 52.56649269174476\nEpoch: 465 / 1000\nw1: [14.59925206] w2: [-5.39187648] bias: [16.53668538] loss: 52.524504936241335\nEpoch: 466 / 1000\nw1: [14.60942843] w2: [-5.4096224] bias: [16.53673306] loss: 52.482597194919336\nEpoch: 467 / 1000\nw1: [14.61959525] w2: [-5.42735131] bias: [16.53678054] loss: 52.440769315278565\nEpoch: 468 / 1000\nw1: [14.62975252] w2: [-5.44506324] bias: [16.53682784] loss: 52.399021145109494\nEpoch: 469 / 1000\nw1: [14.63990027] w2: [-5.46275819] bias: [16.53687494] loss: 52.35735253249268\nEpoch: 470 / 1000\nw1: [14.65003849] w2: [-5.48043619] bias: [16.53692186] loss: 52.3157633257983\nEpoch: 471 / 1000\nw1: [14.6601672] w2: [-5.49809725] bias: [16.53696858] loss: 52.27425337368552\nEpoch: 472 / 1000\nw1: [14.67028641] w2: [-5.51574139] bias: [16.53701512] loss: 52.232822525102\nEpoch: 473 / 1000\nw1: [14.68039613] w2: [-5.53336862] bias: [16.53706147] loss: 52.19147062928334\nEpoch: 474 / 1000\nw1: [14.69049636] w2: [-5.55097896] bias: [16.53710763] loss: 52.15019753575246\nEpoch: 475 / 1000\nw1: [14.70058712] w2: [-5.56857242] bias: [16.53715362] loss: 52.109003094319135\nEpoch: 476 / 1000\nw1: [14.71066841] w2: [-5.58614902] bias: [16.53719941] loss: 52.06788715507942\nEpoch: 477 / 1000\nw1: [14.72074025] w2: [-5.60370878] bias: [16.53724502] loss: 52.02684956841506\nEpoch: 478 / 1000\nw1: [14.73080264] w2: [-5.62125172] bias: [16.53729045] loss: 51.98589018499302\nEpoch: 479 / 1000\nw1: [14.7408556] w2: [-5.63877783] bias: [16.5373357] loss: 51.945008855764875\nEpoch: 480 / 1000\nw1: [14.75089913] w2: [-5.65628716] bias: [16.53738077] loss: 51.90420543196632\nEpoch: 481 / 1000\nw1: [14.76093323] w2: [-5.6737797] bias: [16.53742566] loss: 51.863479765116566\nEpoch: 482 / 1000\nw1: [14.77095794] w2: [-5.69125548] bias: [16.53747037] loss: 51.82283170701787\nEpoch: 483 / 1000\nw1: [14.78097324] w2: [-5.70871451] bias: [16.5375149] loss: 51.782261109754934\nEpoch: 484 / 1000\nw1: [14.79097915] w2: [-5.72615681] bias: [16.53755925] loss: 51.741767825694396\nEpoch: 485 / 1000\nw1: [14.80097567] w2: [-5.74358238] bias: [16.53760342] loss: 51.70135170748429\nEpoch: 486 / 1000\nw1: [14.81096283] w2: [-5.76099126] bias: [16.53764742] loss: 51.661012608053504\nEpoch: 487 / 1000\nw1: [14.82094062] w2: [-5.77838346] bias: [16.53769124] loss: 51.62075038061126\nEpoch: 488 / 1000\nw1: [14.83090906] w2: [-5.79575898] bias: [16.53773489] loss: 51.580564878646534\nEpoch: 489 / 1000\nw1: [14.84086816] w2: [-5.81311785] bias: [16.53777836] loss: 51.540455955927584\nEpoch: 490 / 1000\nw1: [14.85081791] w2: [-5.83046008] bias: [16.53782166] loss: 51.50042346650136\nEpoch: 491 / 1000\nw1: [14.86075835] w2: [-5.84778569] bias: [16.53786479] loss: 51.46046726469302\nEpoch: 492 / 1000\nw1: [14.87068946] w2: [-5.86509469] bias: [16.53790774] loss: 51.420587205105384\nEpoch: 493 / 1000\nw1: [14.88061127] w2: [-5.8823871] bias: [16.53795052] loss: 51.38078314261835\nEpoch: 494 / 1000\nw1: [14.89052378] w2: [-5.89966294] bias: [16.53799313] loss: 51.34105493238847\nEpoch: 495 / 1000\nw1: [14.90042699] w2: [-5.91692221] bias: [16.53803557] loss: 51.30140242984834\nEpoch: 496 / 1000\nw1: [14.91032093] w2: [-5.93416495] bias: [16.53807784] loss: 51.26182549070611\nEpoch: 497 / 1000\nw1: [14.92020559] w2: [-5.95139115] bias: [16.53811993] loss: 51.22232397094492\nEpoch: 498 / 1000\nw1: [14.93008099] w2: [-5.96860084] bias: [16.53816186] loss: 51.18289772682246\nEpoch: 499 / 1000\nw1: [14.93994714] w2: [-5.98579404] bias: [16.53820362] loss: 51.14354661487034\nEpoch: 500 / 1000\nw1: [14.94980404] w2: [-6.00297075] bias: [16.53824521] loss: 51.104270491893615\nEpoch: 501 / 1000\nw1: [14.9596517] w2: [-6.020131] bias: [16.53828664] loss: 51.0650692149703\nEpoch: 502 / 1000\nw1: [14.96949014] w2: [-6.0372748] bias: [16.53832789] loss: 51.02594264145082\nEpoch: 503 / 1000\nw1: [14.97931936] w2: [-6.05440216] bias: [16.53836898] loss: 50.98689062895743\nEpoch: 504 / 1000\nw1: [14.98913937] w2: [-6.0715131] bias: [16.53840991] loss: 50.94791303538382\nEpoch: 505 / 1000\nw1: [14.99895018] w2: [-6.08860764] bias: [16.53845067] loss: 50.90900971889448\nEpoch: 506 / 1000\nw1: [15.0087518] w2: [-6.1056858] bias: [16.53849126] loss: 50.87018053792427\nEpoch: 507 / 1000\nw1: [15.01854423] w2: [-6.12274758] bias: [16.53853169] loss: 50.83142535117784\nEpoch: 508 / 1000\nw1: [15.02832749] w2: [-6.139793] bias: [16.53857195] loss: 50.792744017629175\nEpoch: 509 / 1000\nw1: [15.03810159] w2: [-6.15682209] bias: [16.53861205] loss: 50.75413639652102\nEpoch: 510 / 1000\nw1: [15.04786653] w2: [-6.17383484] bias: [16.53865198] loss: 50.71560234736441\nEpoch: 511 / 1000\nw1: [15.05762232] w2: [-6.19083129] bias: [16.53869176] loss: 50.67714172993815\nEpoch: 512 / 1000\nw1: [15.06736897] w2: [-6.20781145] bias: [16.53873137] loss: 50.638754404288306\nEpoch: 513 / 1000\nw1: [15.0771065] w2: [-6.22477532] bias: [16.53877082] loss: 50.60044023072767\nEpoch: 514 / 1000\nw1: [15.0868349] w2: [-6.24172293] bias: [16.5388101] loss: 50.56219906983531\nEpoch: 515 / 1000\nw1: [15.09655419] w2: [-6.25865429] bias: [16.53884923] loss: 50.52403078245598\nEpoch: 516 / 1000\nw1: [15.10626438] w2: [-6.27556942] bias: [16.53888819] loss: 50.48593522969968\nEpoch: 517 / 1000\nw1: [15.11596547] w2: [-6.29246834] bias: [16.53892699] loss: 50.44791227294115\nEpoch: 518 / 1000\nw1: [15.12565748] w2: [-6.30935105] bias: [16.53896563] loss: 50.40996177381929\nEpoch: 519 / 1000\nw1: [15.13534041] w2: [-6.32621757] bias: [16.53900411] loss: 50.37208359423677\nEpoch: 520 / 1000\nw1: [15.14501428] w2: [-6.34306792] bias: [16.53904244] loss: 50.33427759635942\nEpoch: 521 / 1000\nw1: [15.15467908] w2: [-6.35990212] bias: [16.5390806] loss: 50.2965436426158\nEpoch: 522 / 1000\nw1: [15.16433483] w2: [-6.37672018] bias: [16.5391186] loss: 50.25888159569668\nEpoch: 523 / 1000\nw1: [15.17398154] w2: [-6.39352211] bias: [16.53915645] loss: 50.2212913185545\nEpoch: 524 / 1000\nw1: [15.18361922] w2: [-6.41030793] bias: [16.53919414] loss: 50.18377267440296\nEpoch: 525 / 1000\nw1: [15.19324787] w2: [-6.42707766] bias: [16.53923167] loss: 50.146325526716424\nEpoch: 526 / 1000\nw1: [15.20286751] w2: [-6.4438313] bias: [16.53926904] loss: 50.10894973922947\nEpoch: 527 / 1000\nw1: [15.21247814] w2: [-6.46056889] bias: [16.53930625] loss: 50.07164517593643\nEpoch: 528 / 1000\nw1: [15.22207977] w2: [-6.47729042] bias: [16.53934331] loss: 50.03441170109081\nEpoch: 529 / 1000\nw1: [15.23167241] w2: [-6.49399593] bias: [16.53938021] loss: 49.99724917920488\nEpoch: 530 / 1000\nw1: [15.24125606] w2: [-6.51068541] bias: [16.53941696] loss: 49.96015747504911\nEpoch: 531 / 1000\nw1: [15.25083075] w2: [-6.52735889] bias: [16.53945355] loss: 49.92313645365172\nEpoch: 532 / 1000\nw1: [15.26039647] w2: [-6.54401639] bias: [16.53948998] loss: 49.8861859802982\nEpoch: 533 / 1000\nw1: [15.26995324] w2: [-6.56065791] bias: [16.53952626] loss: 49.84930592053078\nEpoch: 534 / 1000\nw1: [15.27950105] w2: [-6.57728348] bias: [16.53956238] loss: 49.81249614014796\nEpoch: 535 / 1000\nw1: [15.28903993] w2: [-6.5938931] bias: [16.53959835] loss: 49.77575650520403\nEpoch: 536 / 1000\nw1: [15.29856988] w2: [-6.61048679] bias: [16.53963416] loss: 49.73908688200855\nEpoch: 537 / 1000\nw1: [15.30809091] w2: [-6.62706458] bias: [16.53966982] loss: 49.70248713712594\nEpoch: 538 / 1000\nw1: [15.31760303] w2: [-6.64362647] bias: [16.53970533] loss: 49.665957137374875\nEpoch: 539 / 1000\nw1: [15.32710624] w2: [-6.66017247] bias: [16.53974068] loss: 49.6294967498279\nEpoch: 540 / 1000\nw1: [15.33660055] w2: [-6.67670261] bias: [16.53977588] loss: 49.59310584181093\nEpoch: 541 / 1000\nw1: [15.34608598] w2: [-6.6932169] bias: [16.53981093] loss: 49.556784280902725\nEpoch: 542 / 1000\nw1: [15.35556253] w2: [-6.70971536] bias: [16.53984582] loss: 49.52053193493443\nEpoch: 543 / 1000\nw1: [15.36503021] w2: [-6.72619799] bias: [16.53988056] loss: 49.48434867198912\nEpoch: 544 / 1000\nw1: [15.37448903] w2: [-6.74266482] bias: [16.53991515] loss: 49.44823436040126\nEpoch: 545 / 1000\nw1: [15.38393899] w2: [-6.75911585] bias: [16.53994958] loss: 49.412188868756296\nEpoch: 546 / 1000\nw1: [15.39338011] w2: [-6.77555111] bias: [16.53998387] loss: 49.376212065890144\nEpoch: 547 / 1000\nw1: [15.40281239] w2: [-6.79197061] bias: [16.540018] loss: 49.340303820888664\nEpoch: 548 / 1000\nw1: [15.41223585] w2: [-6.80837437] bias: [16.54005198] loss: 49.304464003087304\nEpoch: 549 / 1000\nw1: [15.42165048] w2: [-6.82476239] bias: [16.54008581] loss: 49.268692482070485\nEpoch: 550 / 1000\nw1: [15.43105631] w2: [-6.8411347] bias: [16.54011949] loss: 49.232989127671246\nEpoch: 551 / 1000\nw1: [15.44045333] w2: [-6.8574913] bias: [16.54015302] loss: 49.197353809970686\nEpoch: 552 / 1000\nw1: [15.44984156] w2: [-6.87383222] bias: [16.5401864] loss: 49.16178639929751\nEpoch: 553 / 1000\nw1: [15.459221] w2: [-6.89015747] bias: [16.54021963] loss: 49.12628676622761\nEpoch: 554 / 1000\nw1: [15.46859166] w2: [-6.90646707] bias: [16.54025271] loss: 49.09085478158352\nEpoch: 555 / 1000\nw1: [15.47795356] w2: [-6.92276102] bias: [16.54028564] loss: 49.055490316434\nEpoch: 556 / 1000\nw1: [15.48730669] w2: [-6.93903935] bias: [16.54031842] loss: 49.02019324209353\nEpoch: 557 / 1000\nw1: [15.49665108] w2: [-6.95530206] bias: [16.54035105] loss: 48.98496343012187\nEpoch: 558 / 1000\nw1: [15.50598671] w2: [-6.97154918] bias: [16.54038353] loss: 48.94980075232356\nEpoch: 559 / 1000\nw1: [15.51531362] w2: [-6.98778072] bias: [16.54041586] loss: 48.91470508074751\nEpoch: 560 / 1000\nw1: [15.52463179] w2: [-7.00399669] bias: [16.54044805] loss: 48.87967628768645\nEpoch: 561 / 1000\nw1: [15.53394125] w2: [-7.02019711] bias: [16.54048008] loss: 48.84471424567655\nEpoch: 562 / 1000\nw1: [15.54324199] w2: [-7.03638199] bias: [16.54051197] loss: 48.80981882749691\nEpoch: 563 / 1000\nw1: [15.55253404] w2: [-7.05255135] bias: [16.54054371] loss: 48.7749899061691\nEpoch: 564 / 1000\nw1: [15.56181739] w2: [-7.0687052] bias: [16.54057531] loss: 48.74022735495671\nEpoch: 565 / 1000\nw1: [15.57109205] w2: [-7.08484356] bias: [16.54060675] loss: 48.70553104736486\nEpoch: 566 / 1000\nw1: [15.58035804] w2: [-7.10096644] bias: [16.54063805] loss: 48.6709008571398\nEpoch: 567 / 1000\nw1: [15.58961536] w2: [-7.11707386] bias: [16.5406692] loss: 48.63633665826839\nEpoch: 568 / 1000\nw1: [15.59886401] w2: [-7.13316583] bias: [16.54070021] loss: 48.60183832497768\nEpoch: 569 / 1000\nw1: [15.60810402] w2: [-7.14924237] bias: [16.54073107] loss: 48.5674057317344\nEpoch: 570 / 1000\nw1: [15.61733538] w2: [-7.16530349] bias: [16.54076178] loss: 48.53303875324457\nEpoch: 571 / 1000\nw1: [15.6265581] w2: [-7.1813492] bias: [16.54079234] loss: 48.49873726445302\nEpoch: 572 / 1000\nw1: [15.6357722] w2: [-7.19737952] bias: [16.54082276] loss: 48.46450114054289\nEpoch: 573 / 1000\nw1: [15.64497767] w2: [-7.21339447] bias: [16.54085304] loss: 48.430330256935264\nEpoch: 574 / 1000\nw1: [15.65417454] w2: [-7.22939406] bias: [16.54088317] loss: 48.39622448928863\nEpoch: 575 / 1000\nw1: [15.6633628] w2: [-7.24537831] bias: [16.54091315] loss: 48.36218371349847\nEpoch: 576 / 1000\nw1: [15.67254246] w2: [-7.26134722] bias: [16.54094299] loss: 48.328207805696806\nEpoch: 577 / 1000\nw1: [15.68171354] w2: [-7.27730082] bias: [16.54097268] loss: 48.29429664225176\nEpoch: 578 / 1000\nw1: [15.69087605] w2: [-7.29323911] bias: [16.54100223] loss: 48.260450099767056\nEpoch: 579 / 1000\nw1: [15.70002998] w2: [-7.30916212] bias: [16.54103163] loss: 48.226668055081625\nEpoch: 580 / 1000\nw1: [15.70917535] w2: [-7.32506985] bias: [16.54106089] loss: 48.192950385269164\nEpoch: 581 / 1000\nw1: [15.71831216] w2: [-7.34096233] bias: [16.54109001] loss: 48.15929696763759\nEpoch: 582 / 1000\nw1: [15.72744043] w2: [-7.35683956] bias: [16.54111898] loss: 48.12570767972875\nEpoch: 583 / 1000\nw1: [15.73656016] w2: [-7.37270157] bias: [16.54114781] loss: 48.09218239931779\nEpoch: 584 / 1000\nw1: [15.74567136] w2: [-7.38854836] bias: [16.54117649] loss: 48.05872100441292\nEpoch: 585 / 1000\nw1: [15.75477404] w2: [-7.40437995] bias: [16.54120503] loss: 48.02532337325475\nEpoch: 586 / 1000\nw1: [15.76386821] w2: [-7.42019635] bias: [16.54123343] loss: 47.99198938431605\nEpoch: 587 / 1000\nw1: [15.77295387] w2: [-7.43599759] bias: [16.54126168] loss: 47.95871891630116\nEpoch: 588 / 1000\nw1: [15.78203103] w2: [-7.45178367] bias: [16.5412898] loss: 47.92551184814562\nEpoch: 589 / 1000\nw1: [15.7910997] w2: [-7.4675546] bias: [16.54131776] loss: 47.89236805901569\nEpoch: 590 / 1000\nw1: [15.8001599] w2: [-7.48331041] bias: [16.54134559] loss: 47.85928742830799\nEpoch: 591 / 1000\nw1: [15.80921162] w2: [-7.4990511] bias: [16.54137327] loss: 47.82626983564892\nEpoch: 592 / 1000\nw1: [15.81825487] w2: [-7.5147767] bias: [16.54140082] loss: 47.79331516089437\nEpoch: 593 / 1000\nw1: [15.82728967] w2: [-7.53048721] bias: [16.54142821] loss: 47.7604232841292\nEpoch: 594 / 1000\nw1: [15.83631602] w2: [-7.54618266] bias: [16.54145547] loss: 47.72759408566681\nEpoch: 595 / 1000\nw1: [15.84533393] w2: [-7.56186304] bias: [16.54148259] loss: 47.69482744604873\nEpoch: 596 / 1000\nw1: [15.85434341] w2: [-7.57752839] bias: [16.54150956] loss: 47.66212324604414\nEpoch: 597 / 1000\nw1: [15.86334446] w2: [-7.59317871] bias: [16.5415364] loss: 47.629481366649536\nEpoch: 598 / 1000\nw1: [15.8723371] w2: [-7.60881401] bias: [16.54156309] loss: 47.59690168908815\nEpoch: 599 / 1000\nw1: [15.88132132] w2: [-7.62443432] bias: [16.54158964] loss: 47.56438409480965\nEpoch: 600 / 1000\nw1: [15.89029715] w2: [-7.64003964] bias: [16.54161605] loss: 47.5319284654896\nEpoch: 601 / 1000\nw1: [15.89926458] w2: [-7.65563] bias: [16.54164232] loss: 47.49953468302917\nEpoch: 602 / 1000\nw1: [15.90822363] w2: [-7.6712054] bias: [16.54166845] loss: 47.467202629554514\nEpoch: 603 / 1000\nw1: [15.9171743] w2: [-7.68676586] bias: [16.54169444] loss: 47.434932187416535\nEpoch: 604 / 1000\nw1: [15.92611661] w2: [-7.70231139] bias: [16.54172029] loss: 47.402723239190315\nEpoch: 605 / 1000\nw1: [15.93505055] w2: [-7.71784201] bias: [16.541746] loss: 47.37057566767478\nEpoch: 606 / 1000\nw1: [15.94397614] w2: [-7.73335773] bias: [16.54177157] loss: 47.338489355892165\nEpoch: 607 / 1000\nw1: [15.95289339] w2: [-7.74885856] bias: [16.541797] loss: 47.30646418708776\nEpoch: 608 / 1000\nw1: [15.96180229] w2: [-7.76434453] bias: [16.54182229] loss: 47.27450004472931\nEpoch: 609 / 1000\nw1: [15.97070287] w2: [-7.77981564] bias: [16.54184744] loss: 47.24259681250667\nEpoch: 610 / 1000\nw1: [15.97959513] w2: [-7.79527191] bias: [16.54187245] loss: 47.2107543743314\nEpoch: 611 / 1000\nw1: [15.98847908] w2: [-7.81071335] bias: [16.54189732] loss: 47.17897261433632\nEpoch: 612 / 1000\nw1: [15.99735472] w2: [-7.82613998] bias: [16.54192206] loss: 47.14725141687504\nEpoch: 613 / 1000\nw1: [16.00622206] w2: [-7.84155181] bias: [16.54194665] loss: 47.11559066652163\nEpoch: 614 / 1000\nw1: [16.01508111] w2: [-7.85694886] bias: [16.54197111] loss: 47.083990248070165\nEpoch: 615 / 1000\nw1: [16.02393189] w2: [-7.87233113] bias: [16.54199543] loss: 47.052450046534254\nEpoch: 616 / 1000\nw1: [16.03277439] w2: [-7.88769865] bias: [16.54201961] loss: 47.02096994714667\nEpoch: 617 / 1000\nw1: [16.04160862] w2: [-7.90305142] bias: [16.54204365] loss: 46.98954983535897\nEpoch: 618 / 1000\nw1: [16.05043459] w2: [-7.91838947] bias: [16.54206756] loss: 46.958189596840974\nEpoch: 619 / 1000\nw1: [16.05925232] w2: [-7.9337128] bias: [16.54209133] loss: 46.92688911748046\nEpoch: 620 / 1000\nw1: [16.0680618] w2: [-7.94902143] bias: [16.54211496] loss: 46.89564828338265\nEpoch: 621 / 1000\nw1: [16.07686305] w2: [-7.96431537] bias: [16.54213845] loss: 46.86446698086989\nEpoch: 622 / 1000\nw1: [16.08565607] w2: [-7.97959464] bias: [16.5421618] loss: 46.83334509648116\nEpoch: 623 / 1000\nw1: [16.09444088] w2: [-7.99485925] bias: [16.54218502] loss: 46.8022825169717\nEpoch: 624 / 1000\nw1: [16.10321747] w2: [-8.01010922] bias: [16.5422081] loss: 46.77127912931256\nEpoch: 625 / 1000\nw1: [16.11198586] w2: [-8.02534456] bias: [16.54223105] loss: 46.74033482069027\nEpoch: 626 / 1000\nw1: [16.12074606] w2: [-8.04056528] bias: [16.54225386] loss: 46.70944947850632\nEpoch: 627 / 1000\nw1: [16.12949806] w2: [-8.0557714] bias: [16.54227653] loss: 46.67862299037686\nEpoch: 628 / 1000\nw1: [16.13824189] w2: [-8.07096293] bias: [16.54229906] loss: 46.6478552441322\nEpoch: 629 / 1000\nw1: [16.14697755] w2: [-8.08613988] bias: [16.54232146] loss: 46.617146127816454\nEpoch: 630 / 1000\nw1: [16.15570504] w2: [-8.10130228] bias: [16.54234373] loss: 46.5864955296871\nEpoch: 631 / 1000\nw1: [16.16442437] w2: [-8.11645012] bias: [16.54236585] loss: 46.55590333821462\nEpoch: 632 / 1000\nw1: [16.17313556] w2: [-8.13158344] bias: [16.54238785] loss: 46.52536944208202\nEpoch: 633 / 1000\nw1: [16.18183861] w2: [-8.14670223] bias: [16.5424097] loss: 46.49489373018452\nEpoch: 634 / 1000\nw1: [16.19053352] w2: [-8.16180652] bias: [16.54243142] loss: 46.46447609162908\nEpoch: 635 / 1000\nw1: [16.19922031] w2: [-8.17689632] bias: [16.54245301] loss: 46.43411641573401\nEpoch: 636 / 1000\nw1: [16.20789898] w2: [-8.19197164] bias: [16.54247446] loss: 46.40381459202857\nEpoch: 637 / 1000\nw1: [16.21656954] w2: [-8.20703249] bias: [16.54249578] loss: 46.37357051025257\nEpoch: 638 / 1000\nw1: [16.225232] w2: [-8.2220789] bias: [16.54251696] loss: 46.343384060356\nEpoch: 639 / 1000\nw1: [16.23388636] w2: [-8.23711087] bias: [16.54253801] loss: 46.31325513249854\nEpoch: 640 / 1000\nw1: [16.24253264] w2: [-8.25212841] bias: [16.54255892] loss: 46.28318361704929\nEpoch: 641 / 1000\nw1: [16.25117084] w2: [-8.26713155] bias: [16.5425797] loss: 46.25316940458626\nEpoch: 642 / 1000\nw1: [16.25980096] w2: [-8.28212029] bias: [16.54260034] loss: 46.22321238589598\nEpoch: 643 / 1000\nw1: [16.26842303] w2: [-8.29709465] bias: [16.54262085] loss: 46.19331245197321\nEpoch: 644 / 1000\nw1: [16.27703703] w2: [-8.31205465] bias: [16.54264123] loss: 46.1634694940204\nEpoch: 645 / 1000\nw1: [16.28564299] w2: [-8.32700029] bias: [16.54266147] loss: 46.133683403447414\nEpoch: 646 / 1000\nw1: [16.29424091] w2: [-8.34193159] bias: [16.54268158] loss: 46.10395407187104\nEpoch: 647 / 1000\nw1: [16.3028308] w2: [-8.35684856] bias: [16.54270156] loss: 46.074281391114674\nEpoch: 648 / 1000\nw1: [16.31141266] w2: [-8.37175122] bias: [16.5427214] loss: 46.04466525320786\nEpoch: 649 / 1000\nw1: [16.3199865] w2: [-8.38663958] bias: [16.54274111] loss: 46.01510555038595\nEpoch: 650 / 1000\nw1: [16.32855233] w2: [-8.40151365] bias: [16.54276069] loss: 45.985602175089674\nEpoch: 651 / 1000\nw1: [16.33711016] w2: [-8.41637346] bias: [16.54278013] loss: 45.95615501996476\nEpoch: 652 / 1000\nw1: [16.34565999] w2: [-8.431219] bias: [16.54279944] loss: 45.926763977861576\nEpoch: 653 / 1000\nw1: [16.35420184] w2: [-8.4460503] bias: [16.54281862] loss: 45.89742894183468\nEpoch: 654 / 1000\nw1: [16.36273571] w2: [-8.46086737] bias: [16.54283767] loss: 45.86814980514248\nEpoch: 655 / 1000\nw1: [16.3712616] w2: [-8.47567022] bias: [16.54285658] loss: 45.83892646124681\nEpoch: 656 / 1000\nw1: [16.37977953] w2: [-8.49045887] bias: [16.54287536] loss: 45.80975880381258\nEpoch: 657 / 1000\nw1: [16.38828951] w2: [-8.50523333] bias: [16.54289401] loss: 45.780646726707346\nEpoch: 658 / 1000\nw1: [16.39679153] w2: [-8.51999361] bias: [16.54291253] loss: 45.75159012400097\nEpoch: 659 / 1000\nw1: [16.40528561] w2: [-8.53473972] bias: [16.54293092] loss: 45.722588889965195\nEpoch: 660 / 1000\nw1: [16.41377176] w2: [-8.54947169] bias: [16.54294918] loss: 45.69364291907328\nEpoch: 661 / 1000\nw1: [16.42224998] w2: [-8.56418952] bias: [16.5429673] loss: 45.664752105999604\nEpoch: 662 / 1000\nw1: [16.43072029] w2: [-8.57889323] bias: [16.54298529] loss: 45.63591634561931\nEpoch: 663 / 1000\nw1: [16.43918268] w2: [-8.59358283] bias: [16.54300316] loss: 45.60713553300789\nEpoch: 664 / 1000\nw1: [16.44763716] w2: [-8.60825833] bias: [16.54302089] loss: 45.578409563440815\nEpoch: 665 / 1000\nw1: [16.45608375] w2: [-8.62291975] bias: [16.54303849] loss: 45.54973833239317\nEpoch: 666 / 1000\nw1: [16.46452245] w2: [-8.6375671] bias: [16.54305596] loss: 45.52112173553921\nEpoch: 667 / 1000\nw1: [16.47295327] w2: [-8.6522004] bias: [16.5430733] loss: 45.4925596687521\nEpoch: 668 / 1000\nw1: [16.48137622] w2: [-8.66681965] bias: [16.54309051] loss: 45.4640520281034\nEpoch: 669 / 1000\nw1: [16.48979129] w2: [-8.68142488] bias: [16.54310759] loss: 45.435598709862816\nEpoch: 670 / 1000\nw1: [16.49819851] w2: [-8.69601608] bias: [16.54312454] loss: 45.40719961049769\nEpoch: 671 / 1000\nw1: [16.50659788] w2: [-8.71059329] bias: [16.54314135] loss: 45.37885462667274\nEpoch: 672 / 1000\nw1: [16.5149894] w2: [-8.72515651] bias: [16.54315804] loss: 45.3505636552496\nEpoch: 673 / 1000\nw1: [16.52337309] w2: [-8.73970575] bias: [16.5431746] loss: 45.322326593286526\nEpoch: 674 / 1000\nw1: [16.53174895] w2: [-8.75424103] bias: [16.54319103] loss: 45.29414333803792\nEpoch: 675 / 1000\nw1: [16.54011698] w2: [-8.76876237] bias: [16.54320734] loss: 45.26601378695406\nEpoch: 676 / 1000\nw1: [16.5484772] w2: [-8.78326976] bias: [16.54322351] loss: 45.23793783768064\nEpoch: 677 / 1000\nw1: [16.55682962] w2: [-8.79776324] bias: [16.54323955] loss: 45.209915388058455\nEpoch: 678 / 1000\nw1: [16.56517423] w2: [-8.8122428] bias: [16.54325546] loss: 45.18194633612302\nEpoch: 679 / 1000\nw1: [16.57351105] w2: [-8.82670847] bias: [16.54327125] loss: 45.15403058010415\nEpoch: 680 / 1000\nw1: [16.58184009] w2: [-8.84116026] bias: [16.54328691] loss: 45.126168018425666\nEpoch: 681 / 1000\nw1: [16.59016135] w2: [-8.85559818] bias: [16.54330243] loss: 45.09835854970497\nEpoch: 682 / 1000\nw1: [16.59847484] w2: [-8.87002225] bias: [16.54331783] loss: 45.0706020727527\nEpoch: 683 / 1000\nw1: [16.60678057] w2: [-8.88443247] bias: [16.54333311] loss: 45.04289848657234\nEpoch: 684 / 1000\nw1: [16.61507854] w2: [-8.89882886] bias: [16.54334825] loss: 45.01524769035989\nEpoch: 685 / 1000\nw1: [16.62336876] w2: [-8.91321143] bias: [16.54336326] loss: 44.98764958350346\nEpoch: 686 / 1000\nw1: [16.63165125] w2: [-8.92758021] bias: [16.54337815] loss: 44.96010406558293\nEpoch: 687 / 1000\nw1: [16.639926] w2: [-8.94193519] bias: [16.54339291] loss: 44.93261103636958\nEpoch: 688 / 1000\nw1: [16.64819302] w2: [-8.9562764] bias: [16.54340755] loss: 44.90517039582568\nEpoch: 689 / 1000\nw1: [16.65645233] w2: [-8.97060384] bias: [16.54342205] loss: 44.87778204410422\nEpoch: 690 / 1000\nw1: [16.66470392] w2: [-8.98491753] bias: [16.54343643] loss: 44.850445881548474\nEpoch: 691 / 1000\nw1: [16.67294781] w2: [-8.99921749] bias: [16.54345068] loss: 44.82316180869164\nEpoch: 692 / 1000\nw1: [16.68118401] w2: [-9.01350372] bias: [16.5434648] loss: 44.795929726256496\nEpoch: 693 / 1000\nw1: [16.68941252] w2: [-9.02777624] bias: [16.5434788] loss: 44.768749535155074\nEpoch: 694 / 1000\nw1: [16.69763334] w2: [-9.04203506] bias: [16.54349267] loss: 44.74162113648819\nEpoch: 695 / 1000\nw1: [16.70584649] w2: [-9.0562802] bias: [16.54350642] loss: 44.71454443154524\nEpoch: 696 / 1000\nw1: [16.71405197] w2: [-9.07051167] bias: [16.54352003] loss: 44.687519321803705\nEpoch: 697 / 1000\nw1: [16.72224979] w2: [-9.08472947] bias: [16.54353353] loss: 44.660545708928844\nEpoch: 698 / 1000\nw1: [16.73043996] w2: [-9.09893363] bias: [16.54354689] loss: 44.63362349477335\nEpoch: 699 / 1000\nw1: [16.73862249] w2: [-9.11312416] bias: [16.54356013] loss: 44.60675258137698\nEpoch: 700 / 1000\nw1: [16.74679738] w2: [-9.12730107] bias: [16.54357324] loss: 44.57993287096619\nEpoch: 701 / 1000\nw1: [16.75496463] w2: [-9.14146437] bias: [16.54358623] loss: 44.553164265953804\nEpoch: 702 / 1000\nw1: [16.76312426] w2: [-9.15561408] bias: [16.5435991] loss: 44.52644666893861\nEpoch: 703 / 1000\nw1: [16.77127628] w2: [-9.16975021] bias: [16.54361183] loss: 44.49977998270506\nEpoch: 704 / 1000\nw1: [16.77942069] w2: [-9.18387277] bias: [16.54362444] loss: 44.47316411022289\nEpoch: 705 / 1000\nw1: [16.78755749] w2: [-9.19798178] bias: [16.54363693] loss: 44.44659895464675\nEpoch: 706 / 1000\nw1: [16.7956867] w2: [-9.21207724] bias: [16.54364929] loss: 44.42008441931593\nEpoch: 707 / 1000\nw1: [16.80380833] w2: [-9.22615917] bias: [16.54366153] loss: 44.393620407753865\nEpoch: 708 / 1000\nw1: [16.81192237] w2: [-9.24022759] bias: [16.54367364] loss: 44.36720682366794\nEpoch: 709 / 1000\nw1: [16.82002884] w2: [-9.25428251] bias: [16.54368563] loss: 44.34084357094904\nEpoch: 710 / 1000\nw1: [16.82812774] w2: [-9.26832393] bias: [16.54369749] loss: 44.314530553671204\nEpoch: 711 / 1000\nw1: [16.83621909] w2: [-9.28235188] bias: [16.54370923] loss: 44.28826767609135\nEpoch: 712 / 1000\nw1: [16.84430288] w2: [-9.29636636] bias: [16.54372084] loss: 44.262054842648844\nEpoch: 713 / 1000\nw1: [16.85237913] w2: [-9.31036739] bias: [16.54373233] loss: 44.235891957965194\nEpoch: 714 / 1000\nw1: [16.86044785] w2: [-9.32435499] bias: [16.5437437] loss: 44.209778926843676\nEpoch: 715 / 1000\nw1: [16.86850903] w2: [-9.33832915] bias: [16.54375494] loss: 44.18371565426905\nEpoch: 716 / 1000\nw1: [16.87656269] w2: [-9.35228991] bias: [16.54376606] loss: 44.15770204540712\nEpoch: 717 / 1000\nw1: [16.88460883] w2: [-9.36623726] bias: [16.54377705] loss: 44.13173800560446\nEpoch: 718 / 1000\nw1: [16.89264747] w2: [-9.38017122] bias: [16.54378793] loss: 44.10582344038806\nEpoch: 719 / 1000\nw1: [16.90067861] w2: [-9.39409181] bias: [16.54379867] loss: 44.079958255464945\nEpoch: 720 / 1000\nw1: [16.90870225] w2: [-9.40799904] bias: [16.5438093] loss: 44.05414235672187\nEpoch: 721 / 1000\nw1: [16.9167184] w2: [-9.42189292] bias: [16.5438198] loss: 44.02837565022497\nEpoch: 722 / 1000\nw1: [16.92472707] w2: [-9.43577346] bias: [16.54383018] loss: 44.00265804221941\nEpoch: 723 / 1000\nw1: [16.93272828] w2: [-9.44964068] bias: [16.54384044] loss: 43.97698943912903\nEpoch: 724 / 1000\nw1: [16.94072201] w2: [-9.46349459] bias: [16.54385057] loss: 43.95136974755603\nEpoch: 725 / 1000\nw1: [16.94870829] w2: [-9.4773352] bias: [16.54386058] loss: 43.92579887428063\nEpoch: 726 / 1000\nw1: [16.95668711] w2: [-9.49116252] bias: [16.54387047] loss: 43.900276726260714\nEpoch: 727 / 1000\nw1: [16.96465849] w2: [-9.50497657] bias: [16.54388024] loss: 43.8748032106315\nEpoch: 728 / 1000\nw1: [16.97262244] w2: [-9.51877736] bias: [16.54388988] loss: 43.84937823470519\nEpoch: 729 / 1000\nw1: [16.98057895] w2: [-9.53256491] bias: [16.54389941] loss: 43.82400170597068\nEpoch: 730 / 1000\nw1: [16.98852804] w2: [-9.54633921] bias: [16.54390881] loss: 43.798673532093126\nEpoch: 731 / 1000\nw1: [16.99646971] w2: [-9.5601003] bias: [16.54391809] loss: 43.77339362091374\nEpoch: 732 / 1000\nw1: [17.00440398] w2: [-9.57384817] bias: [16.54392724] loss: 43.7481618804493\nEpoch: 733 / 1000\nw1: [17.01233084] w2: [-9.58758285] bias: [16.54393628] loss: 43.72297821889198\nEpoch: 734 / 1000\nw1: [17.02025031] w2: [-9.60130434] bias: [16.5439452] loss: 43.69784254460888\nEpoch: 735 / 1000\nw1: [17.02816239] w2: [-9.61501266] bias: [16.54395399] loss: 43.672754766141786\nEpoch: 736 / 1000\nw1: [17.03606709] w2: [-9.62870782] bias: [16.54396266] loss: 43.647714792206756\nEpoch: 737 / 1000\nw1: [17.04396441] w2: [-9.64238984] bias: [16.54397121] loss: 43.62272253169386\nEpoch: 738 / 1000\nw1: [17.05185437] w2: [-9.65605872] bias: [16.54397965] loss: 43.59777789366681\nEpoch: 739 / 1000\nw1: [17.05973697] w2: [-9.66971448] bias: [16.54398796] loss: 43.572880787362635\nEpoch: 740 / 1000\nw1: [17.06761221] w2: [-9.68335713] bias: [16.54399615] loss: 43.54803112219137\nEpoch: 741 / 1000\nw1: [17.07548011] w2: [-9.69698668] bias: [16.54400422] loss: 43.523228807735656\nEpoch: 742 / 1000\nw1: [17.08334067] w2: [-9.71060314] bias: [16.54401217] loss: 43.49847375375053\nEpoch: 743 / 1000\nw1: [17.0911939] w2: [-9.72420654] bias: [16.54402] loss: 43.473765870163\nEpoch: 744 / 1000\nw1: [17.0990398] w2: [-9.73779688] bias: [16.5440277] loss: 43.449105067071734\nEpoch: 745 / 1000\nw1: [17.10687838] w2: [-9.75137417] bias: [16.54403529] loss: 43.42449125474679\nEpoch: 746 / 1000\nw1: [17.11470966] w2: [-9.76493842] bias: [16.54404276] loss: 43.39992434362919\nEpoch: 747 / 1000\nw1: [17.12253363] w2: [-9.77848965] bias: [16.54405011] loss: 43.375404244330696\nEpoch: 748 / 1000\nw1: [17.1303503] w2: [-9.79202788] bias: [16.54405735] loss: 43.350930867633416\nEpoch: 749 / 1000\nw1: [17.13815968] w2: [-9.8055531] bias: [16.54406446] loss: 43.32650412448951\nEpoch: 750 / 1000\nw1: [17.14596178] w2: [-9.81906535] bias: [16.54407145] loss: 43.30212392602084\nEpoch: 751 / 1000\nw1: [17.1537566] w2: [-9.83256461] bias: [16.54407832] loss: 43.2777901835187\nEpoch: 752 / 1000\nw1: [17.16154415] w2: [-9.84605092] bias: [16.54408508] loss: 43.253502808443415\nEpoch: 753 / 1000\nw1: [17.16932444] w2: [-9.85952429] bias: [16.54409171] loss: 43.2292617124241\nEpoch: 754 / 1000\nw1: [17.17709747] w2: [-9.87298471] bias: [16.54409823] loss: 43.205066807258284\nEpoch: 755 / 1000\nw1: [17.18486326] w2: [-9.88643221] bias: [16.54410463] loss: 43.18091800491159\nEpoch: 756 / 1000\nw1: [17.1926218] w2: [-9.89986681] bias: [16.54411091] loss: 43.156815217517455\nEpoch: 757 / 1000\nw1: [17.20037311] w2: [-9.9132885] bias: [16.54411707] loss: 43.13275835737677\nEpoch: 758 / 1000\nw1: [17.20811719] w2: [-9.92669731] bias: [16.54412311] loss: 43.10874733695756\nEpoch: 759 / 1000\nw1: [17.21585405] w2: [-9.94009325] bias: [16.54412904] loss: 43.08478206889473\nEpoch: 760 / 1000\nw1: [17.22358369] w2: [-9.95347632] bias: [16.54413484] loss: 43.06086246598963\nEpoch: 761 / 1000\nw1: [17.23130613] w2: [-9.96684654] bias: [16.54414053] loss: 43.03698844120987\nEpoch: 762 / 1000\nw1: [17.23902136] w2: [-9.98020393] bias: [16.5441461] loss: 43.013159907688895\nEpoch: 763 / 1000\nw1: [17.24672941] w2: [-9.99354849] bias: [16.54415156] loss: 42.989376778725735\nEpoch: 764 / 1000\nw1: [17.25443026] w2: [-10.00688024] bias: [16.5441569] loss: 42.96563896778464\nEpoch: 765 / 1000\nw1: [17.26212394] w2: [-10.02019919] bias: [16.54416211] loss: 42.941946388494834\nEpoch: 766 / 1000\nw1: [17.26981044] w2: [-10.03350536] bias: [16.54416722] loss: 42.918298954650126\nEpoch: 767 / 1000\nw1: [17.27748977] w2: [-10.04679874] bias: [16.5441722] loss: 42.89469658020864\nEpoch: 768 / 1000\nw1: [17.28516195] w2: [-10.06007937] bias: [16.54417707] loss: 42.871139179292484\nEpoch: 769 / 1000\nw1: [17.29282697] w2: [-10.07334724] bias: [16.54418182] loss: 42.847626666187445\nEpoch: 770 / 1000\nw1: [17.30048484] w2: [-10.08660237] bias: [16.54418646] loss: 42.824158955342696\nEpoch: 771 / 1000\nw1: [17.30813558] w2: [-10.09984478] bias: [16.54419098] loss: 42.800735961370435\nEpoch: 772 / 1000\nw1: [17.31577918] w2: [-10.11307447] bias: [16.54419538] loss: 42.77735759904562\nEpoch: 773 / 1000\nw1: [17.32341566] w2: [-10.12629146] bias: [16.54419966] loss: 42.75402378330564\nEpoch: 774 / 1000\nw1: [17.33104502] w2: [-10.13949575] bias: [16.54420383] loss: 42.73073442925003\nEpoch: 775 / 1000\nw1: [17.33866727] w2: [-10.15268737] bias: [16.54420789] loss: 42.70748945214008\nEpoch: 776 / 1000\nw1: [17.34628241] w2: [-10.16586633] bias: [16.54421183] loss: 42.684288767398655\nEpoch: 777 / 1000\nw1: [17.35389045] w2: [-10.17903263] bias: [16.54421565] loss: 42.66113229060978\nEpoch: 778 / 1000\nw1: [17.3614914] w2: [-10.19218628] bias: [16.54421935] loss: 42.63801993751839\nEpoch: 779 / 1000\nw1: [17.36908526] w2: [-10.20532731] bias: [16.54422294] loss: 42.61495162402998\nEpoch: 780 / 1000\nw1: [17.37667205] w2: [-10.21845572] bias: [16.54422642] loss: 42.591927266210355\nEpoch: 781 / 1000\nw1: [17.38425176] w2: [-10.23157152] bias: [16.54422978] loss: 42.56894678028527\nEpoch: 782 / 1000\nw1: [17.39182441] w2: [-10.24467473] bias: [16.54423302] loss: 42.54601008264017\nEpoch: 783 / 1000\nw1: [17.39939] w2: [-10.25776535] bias: [16.54423615] loss: 42.52311708981985\nEpoch: 784 / 1000\nw1: [17.40694854] w2: [-10.27084341] bias: [16.54423917] loss: 42.50026771852815\nEpoch: 785 / 1000\nw1: [17.41450004] w2: [-10.28390891] bias: [16.54424207] loss: 42.4774618856277\nEpoch: 786 / 1000\nw1: [17.42204449] w2: [-10.29696186] bias: [16.54424486] loss: 42.45469950813957\nEpoch: 787 / 1000\nw1: [17.42958192] w2: [-10.31000228] bias: [16.54424753] loss: 42.43198050324298\nEpoch: 788 / 1000\nw1: [17.43711232] w2: [-10.32303017] bias: [16.54425008] loss: 42.409304788275016\nEpoch: 789 / 1000\nw1: [17.4446357] w2: [-10.33604556] bias: [16.54425253] loss: 42.38667228073029\nEpoch: 790 / 1000\nw1: [17.45215207] w2: [-10.34904844] bias: [16.54425485] loss: 42.364082898260676\nEpoch: 791 / 1000\nw1: [17.45966143] w2: [-10.36203885] bias: [16.54425707] loss: 42.341536558675\nEpoch: 792 / 1000\nw1: [17.4671638] w2: [-10.37501677] bias: [16.54425917] loss: 42.31903317993875\nEpoch: 793 / 1000\nw1: [17.47465917] w2: [-10.38798224] bias: [16.54426116] loss: 42.29657268017374\nEpoch: 794 / 1000\nw1: [17.48214756] w2: [-10.40093525] bias: [16.54426303] loss: 42.274154977657865\nEpoch: 795 / 1000\nw1: [17.48962898] w2: [-10.41387583] bias: [16.54426479] loss: 42.25177999082477\nEpoch: 796 / 1000\nw1: [17.49710342] w2: [-10.42680397] bias: [16.54426643] loss: 42.22944763826356\nEpoch: 797 / 1000\nw1: [17.50457089] w2: [-10.43971971] bias: [16.54426797] loss: 42.20715783871849\nEpoch: 798 / 1000\nw1: [17.51203141] w2: [-10.45262304] bias: [16.54426939] loss: 42.1849105110887\nEpoch: 799 / 1000\nw1: [17.51948497] w2: [-10.46551398] bias: [16.54427069] loss: 42.162705574427896\nEpoch: 800 / 1000\nw1: [17.52693159] w2: [-10.47839254] bias: [16.54427189] loss: 42.14054294794409\nEpoch: 801 / 1000\nw1: [17.53437127] w2: [-10.49125873] bias: [16.54427297] loss: 42.11842255099922\nEpoch: 802 / 1000\nw1: [17.54180402] w2: [-10.50411257] bias: [16.54427394] loss: 42.09634430310896\nEpoch: 803 / 1000\nw1: [17.54922984] w2: [-10.51695406] bias: [16.54427479] loss: 42.07430812394239\nEpoch: 804 / 1000\nw1: [17.55664875] w2: [-10.52978322] bias: [16.54427554] loss: 42.05231393332165\nEpoch: 805 / 1000\nw1: [17.56406074] w2: [-10.54260006] bias: [16.54427617] loss: 42.03036165122174\nEpoch: 806 / 1000\nw1: [17.57146583] w2: [-10.55540459] bias: [16.54427669] loss: 42.008451197770164\nEpoch: 807 / 1000\nw1: [17.57886401] w2: [-10.56819682] bias: [16.54427709] loss: 41.98658249324665\nEpoch: 808 / 1000\nw1: [17.58625531] w2: [-10.58097677] bias: [16.54427739] loss: 41.9647554580829\nEpoch: 809 / 1000\nw1: [17.59363972] w2: [-10.59374445] bias: [16.54427757] loss: 41.942970012862204\nEpoch: 810 / 1000\nw1: [17.60101725] w2: [-10.60649986] bias: [16.54427764] loss: 41.921226078319286\nEpoch: 811 / 1000\nw1: [17.6083879] w2: [-10.61924302] bias: [16.5442776] loss: 41.899523575339906\nEpoch: 812 / 1000\nw1: [17.61575169] w2: [-10.63197395] bias: [16.54427745] loss: 41.87786242496064\nEpoch: 813 / 1000\nw1: [17.62310862] w2: [-10.64469265] bias: [16.54427719] loss: 41.85624254836851\nEpoch: 814 / 1000\nw1: [17.6304587] w2: [-10.65739913] bias: [16.54427681] loss: 41.834663866900804\nEpoch: 815 / 1000\nw1: [17.63780193] w2: [-10.67009341] bias: [16.54427633] loss: 41.813126302044715\nEpoch: 816 / 1000\nw1: [17.64513832] w2: [-10.6827755] bias: [16.54427573] loss: 41.79162977543709\nEpoch: 817 / 1000\nw1: [17.65246787] w2: [-10.69544541] bias: [16.54427502] loss: 41.7701742088641\nEpoch: 818 / 1000\nw1: [17.6597906] w2: [-10.70810314] bias: [16.54427421] loss: 41.74875952426101\nEpoch: 819 / 1000\nw1: [17.66710651] w2: [-10.72074873] bias: [16.54427328] loss: 41.72738564371187\nEpoch: 820 / 1000\nw1: [17.6744156] w2: [-10.73338217] bias: [16.54427224] loss: 41.70605248944924\nEpoch: 821 / 1000\nw1: [17.68171789] w2: [-10.74600347] bias: [16.54427109] loss: 41.6847599838539\nEpoch: 822 / 1000\nw1: [17.68901337] w2: [-10.75861265] bias: [16.54426983] loss: 41.66350804945453\nEpoch: 823 / 1000\nw1: [17.69630206] w2: [-10.77120972] bias: [16.54426846] loss: 41.64229660892751\nEpoch: 824 / 1000\nw1: [17.70358396] w2: [-10.78379469] bias: [16.54426698] loss: 41.62112558509658\nEpoch: 825 / 1000\nw1: [17.71085908] w2: [-10.79636758] bias: [16.54426539] loss: 41.59999490093258\nEpoch: 826 / 1000\nw1: [17.71812742] w2: [-10.80892839] bias: [16.54426369] loss: 41.57890447955313\nEpoch: 827 / 1000\nw1: [17.72538899] w2: [-10.82147713] bias: [16.54426188] loss: 41.55785424422244\nEpoch: 828 / 1000\nw1: [17.7326438] w2: [-10.83401382] bias: [16.54425996] loss: 41.53684411835091\nEpoch: 829 / 1000\nw1: [17.73989186] w2: [-10.84653847] bias: [16.54425794] loss: 41.51587402549497\nEpoch: 830 / 1000\nw1: [17.74713316] w2: [-10.8590511] bias: [16.5442558] loss: 41.49494388935672\nEpoch: 831 / 1000\nw1: [17.75436772] w2: [-10.8715517] bias: [16.54425355] loss: 41.47405363378367\nEpoch: 832 / 1000\nw1: [17.76159554] w2: [-10.88404029] bias: [16.5442512] loss: 41.4532031827685\nEpoch: 833 / 1000\nw1: [17.76881664] w2: [-10.89651689] bias: [16.54424873] loss: 41.432392460448725\nEpoch: 834 / 1000\nw1: [17.776031] w2: [-10.90898151] bias: [16.54424616] loss: 41.41162139110647\nEpoch: 835 / 1000\nw1: [17.78323865] w2: [-10.92143415] bias: [16.54424348] loss: 41.39088989916818\nEpoch: 836 / 1000\nw1: [17.79043959] w2: [-10.93387484] bias: [16.54424069] loss: 41.370197909204315\nEpoch: 837 / 1000\nw1: [17.79763383] w2: [-10.94630357] bias: [16.54423779] loss: 41.3495453459291\nEpoch: 838 / 1000\nw1: [17.80482136] w2: [-10.95872036] bias: [16.54423478] loss: 41.3289321342003\nEpoch: 839 / 1000\nw1: [17.8120022] w2: [-10.97112523] bias: [16.54423167] loss: 41.30835819901881\nEpoch: 840 / 1000\nw1: [17.81917636] w2: [-10.98351818] bias: [16.54422844] loss: 41.28782346552855\nEpoch: 841 / 1000\nw1: [17.82634384] w2: [-10.99589923] bias: [16.54422511] loss: 41.26732785901605\nEpoch: 842 / 1000\nw1: [17.83350464] w2: [-11.00826838] bias: [16.54422167] loss: 41.246871304910286\nEpoch: 843 / 1000\nw1: [17.84065878] w2: [-11.02062566] bias: [16.54421813] loss: 41.22645372878233\nEpoch: 844 / 1000\nw1: [17.84780625] w2: [-11.03297106] bias: [16.54421447] loss: 41.206075056345135\nEpoch: 845 / 1000\nw1: [17.85494707] w2: [-11.04530461] bias: [16.54421071] loss: 41.18573521345323\nEpoch: 846 / 1000\nw1: [17.86208125] w2: [-11.0576263] bias: [16.54420684] loss: 41.16543412610245\nEpoch: 847 / 1000\nw1: [17.86920878] w2: [-11.06993617] bias: [16.54420286] loss: 41.14517172042967\nEpoch: 848 / 1000\nw1: [17.87632967] w2: [-11.0822342] bias: [16.54419878] loss: 41.1249479227126\nEpoch: 849 / 1000\nw1: [17.88344394] w2: [-11.09452042] bias: [16.54419459] loss: 41.104762659369385\nEpoch: 850 / 1000\nw1: [17.89055159] w2: [-11.10679484] bias: [16.54419029] loss: 41.08461585695848\nEpoch: 851 / 1000\nw1: [17.89765261] w2: [-11.11905747] bias: [16.54418589] loss: 41.064507442178254\nEpoch: 852 / 1000\nw1: [17.90474703] w2: [-11.13130832] bias: [16.54418137] loss: 41.044437341866825\nEpoch: 853 / 1000\nw1: [17.91183484] w2: [-11.14354741] bias: [16.54417676] loss: 41.02440548300175\nEpoch: 854 / 1000\nw1: [17.91891606] w2: [-11.15577473] bias: [16.54417203] loss: 41.00441179269974\nEpoch: 855 / 1000\nw1: [17.92599069] w2: [-11.16799031] bias: [16.5441672] loss: 40.98445619821644\nEpoch: 856 / 1000\nw1: [17.93305872] w2: [-11.18019415] bias: [16.54416226] loss: 40.96453862694614\nEpoch: 857 / 1000\nw1: [17.94012019] w2: [-11.19238627] bias: [16.54415722] loss: 40.94465900642148\nEpoch: 858 / 1000\nw1: [17.94717507] w2: [-11.20456668] bias: [16.54415207] loss: 40.924817264313255\nEpoch: 859 / 1000\nw1: [17.95422339] w2: [-11.21673538] bias: [16.54414682] loss: 40.90501332843011\nEpoch: 860 / 1000\nw1: [17.96126516] w2: [-11.2288924] bias: [16.54414146] loss: 40.88524712671826\nEpoch: 861 / 1000\nw1: [17.96830036] w2: [-11.24103773] bias: [16.54413599] loss: 40.865518587261285\nEpoch: 862 / 1000\nw1: [17.97532902] w2: [-11.2531714] bias: [16.54413042] loss: 40.8458276382798\nEpoch: 863 / 1000\nw1: [17.98235114] w2: [-11.26529342] bias: [16.54412474] loss: 40.82617420813124\nEpoch: 864 / 1000\nw1: [17.98936672] w2: [-11.27740378] bias: [16.54411896] loss: 40.80655822530959\nEpoch: 865 / 1000\nw1: [17.99637578] w2: [-11.28950251] bias: [16.54411307] loss: 40.786979618445116\nEpoch: 866 / 1000\nw1: [18.00337831] w2: [-11.30158962] bias: [16.54410708] loss: 40.767438316304116\nEpoch: 867 / 1000\nw1: [18.01037432] w2: [-11.31366512] bias: [16.54410098] loss: 40.74793424778864\nEpoch: 868 / 1000\nw1: [18.01736383] w2: [-11.32572902] bias: [16.54409478] loss: 40.72846734193626\nEpoch: 869 / 1000\nw1: [18.02434683] w2: [-11.33778132] bias: [16.54408847] loss: 40.709037527919804\nEpoch: 870 / 1000\nw1: [18.03132333] w2: [-11.34982205] bias: [16.54408206] loss: 40.68964473504709\nEpoch: 871 / 1000\nw1: [18.03829334] w2: [-11.36185121] bias: [16.54407554] loss: 40.670288892760645\nEpoch: 872 / 1000\nw1: [18.04525686] w2: [-11.37386881] bias: [16.54406892] loss: 40.65096993063751\nEpoch: 873 / 1000\nw1: [18.05221391] w2: [-11.38587486] bias: [16.5440622] loss: 40.63168777838892\nEpoch: 874 / 1000\nw1: [18.05916448] w2: [-11.39786938] bias: [16.54405537] loss: 40.61244236586009\nEpoch: 875 / 1000\nw1: [18.06610858] w2: [-11.40985238] bias: [16.54404843] loss: 40.59323362302995\nEpoch: 876 / 1000\nw1: [18.07304623] w2: [-11.42182386] bias: [16.5440414] loss: 40.57406148001086\nEpoch: 877 / 1000\nw1: [18.07997742] w2: [-11.43378385] bias: [16.54403426] loss: 40.55492586704844\nEpoch: 878 / 1000\nw1: [18.08690215] w2: [-11.44573234] bias: [16.54402701] loss: 40.53582671452118\nEpoch: 879 / 1000\nw1: [18.09382045] w2: [-11.45766935] bias: [16.54401966] loss: 40.516763952940316\nEpoch: 880 / 1000\nw1: [18.10073231] w2: [-11.46959489] bias: [16.54401221] loss: 40.49773751294951\nEpoch: 881 / 1000\nw1: [18.10763774] w2: [-11.48150898] bias: [16.54400466] loss: 40.47874732532461\nEpoch: 882 / 1000\nw1: [18.11453674] w2: [-11.49341161] bias: [16.543997] loss: 40.459793320973425\nEpoch: 883 / 1000\nw1: [18.12142933] w2: [-11.50530281] bias: [16.54398924] loss: 40.44087543093541\nEpoch: 884 / 1000\nw1: [18.12831551] w2: [-11.51718259] bias: [16.54398137] loss: 40.421993586381475\nEpoch: 885 / 1000\nw1: [18.13519527] w2: [-11.52905095] bias: [16.5439734] loss: 40.40314771861373\nEpoch: 886 / 1000\nw1: [18.14206864] w2: [-11.54090791] bias: [16.54396533] loss: 40.38433775906518\nEpoch: 887 / 1000\nw1: [18.14893562] w2: [-11.55275348] bias: [16.54395716] loss: 40.36556363929955\nEpoch: 888 / 1000\nw1: [18.1557962] w2: [-11.56458766] bias: [16.54394889] loss: 40.346825291010994\nEpoch: 889 / 1000\nw1: [18.16265041] w2: [-11.57641047] bias: [16.54394051] loss: 40.328122646023836\nEpoch: 890 / 1000\nw1: [18.16949824] w2: [-11.58822192] bias: [16.54393203] loss: 40.30945563629234\nEpoch: 891 / 1000\nw1: [18.1763397] w2: [-11.60002203] bias: [16.54392345] loss: 40.290824193900484\nEpoch: 892 / 1000\nw1: [18.18317479] w2: [-11.61181079] bias: [16.54391476] loss: 40.27222825106168\nEpoch: 893 / 1000\nw1: [18.19000353] w2: [-11.62358823] bias: [16.54390597] loss: 40.25366774011851\nEpoch: 894 / 1000\nw1: [18.19682592] w2: [-11.63535435] bias: [16.54389709] loss: 40.23514259354255\nEpoch: 895 / 1000\nw1: [18.20364196] w2: [-11.64710917] bias: [16.5438881] loss: 40.216652743934034\nEpoch: 896 / 1000\nw1: [18.21045166] w2: [-11.65885269] bias: [16.543879] loss: 40.19819812402171\nEpoch: 897 / 1000\nw1: [18.21725503] w2: [-11.67058493] bias: [16.54386981] loss: 40.1797786666625\nEpoch: 898 / 1000\nw1: [18.22405207] w2: [-11.68230589] bias: [16.54386052] loss: 40.161394304841316\nEpoch: 899 / 1000\nw1: [18.23084279] w2: [-11.69401559] bias: [16.54385112] loss: 40.143044971670776\nEpoch: 900 / 1000\nw1: [18.23762719] w2: [-11.70571404] bias: [16.54384162] loss: 40.124730600391004\nEpoch: 901 / 1000\nw1: [18.24440529] w2: [-11.71740124] bias: [16.54383202] loss: 40.10645112436936\nEpoch: 902 / 1000\nw1: [18.25117708] w2: [-11.72907722] bias: [16.54382233] loss: 40.08820647710018\nEpoch: 903 / 1000\nw1: [18.25794258] w2: [-11.74074198] bias: [16.54381252] loss: 40.06999659220458\nEpoch: 904 / 1000\nw1: [18.26470178] w2: [-11.75239552] bias: [16.54380262] loss: 40.051821403430196\nEpoch: 905 / 1000\nw1: [18.2714547] w2: [-11.76403787] bias: [16.54379262] loss: 40.03368084465091\nEpoch: 906 / 1000\nw1: [18.27820133] w2: [-11.77566903] bias: [16.54378252] loss: 40.015574849866645\nEpoch: 907 / 1000\nw1: [18.2849417] w2: [-11.78728902] bias: [16.54377232] loss: 39.997503353203136\nEpoch: 908 / 1000\nw1: [18.29167579] w2: [-11.79889783] bias: [16.54376201] loss: 39.97946628891163\nEpoch: 909 / 1000\nw1: [18.29840363] w2: [-11.8104955] bias: [16.54375161] loss: 39.961463591368734\nEpoch: 910 / 1000\nw1: [18.30512521] w2: [-11.82208201] bias: [16.54374111] loss: 39.943495195076096\nEpoch: 911 / 1000\nw1: [18.31184054] w2: [-11.8336574] bias: [16.5437305] loss: 39.92556103466022\nEpoch: 912 / 1000\nw1: [18.31854962] w2: [-11.84522165] bias: [16.5437198] loss: 39.907661044872185\nEpoch: 913 / 1000\nw1: [18.32525247] w2: [-11.8567748] bias: [16.543709] loss: 39.88979516058747\nEpoch: 914 / 1000\nw1: [18.33194909] w2: [-11.86831684] bias: [16.5436981] loss: 39.87196331680561\nEpoch: 915 / 1000\nw1: [18.33863948] w2: [-11.87984779] bias: [16.54368709] loss: 39.8541654486501\nEpoch: 916 / 1000\nw1: [18.34532365] w2: [-11.89136766] bias: [16.54367599] loss: 39.83640149136805\nEpoch: 917 / 1000\nw1: [18.35200161] w2: [-11.90287646] bias: [16.54366479] loss: 39.81867138032997\nEpoch: 918 / 1000\nw1: [18.35867335] w2: [-11.9143742] bias: [16.54365349] loss: 39.80097505102958\nEpoch: 919 / 1000\nw1: [18.3653389] w2: [-11.92586089] bias: [16.54364209] loss: 39.78331243908354\nEpoch: 920 / 1000\nw1: [18.37199825] w2: [-11.93733654] bias: [16.54363059] loss: 39.765683480231196\nEpoch: 921 / 1000\nw1: [18.37865141] w2: [-11.94880116] bias: [16.54361899] loss: 39.7480881103344\nEpoch: 922 / 1000\nw1: [18.38529838] w2: [-11.96025476] bias: [16.5436073] loss: 39.73052626537724\nEpoch: 923 / 1000\nw1: [18.39193917] w2: [-11.97169736] bias: [16.5435955] loss: 39.7129978814658\nEpoch: 924 / 1000\nw1: [18.3985738] w2: [-11.98312895] bias: [16.54358361] loss: 39.69550289482796\nEpoch: 925 / 1000\nw1: [18.40520225] w2: [-11.99454957] bias: [16.54357162] loss: 39.678041241813126\nEpoch: 926 / 1000\nw1: [18.41182454] w2: [-12.0059592] bias: [16.54355953] loss: 39.660612858892044\nEpoch: 927 / 1000\nw1: [18.41844068] w2: [-12.01735787] bias: [16.54354734] loss: 39.64321768265652\nEpoch: 928 / 1000\nw1: [18.42505067] w2: [-12.02874559] bias: [16.54353505] loss: 39.625855649819215\nEpoch: 929 / 1000\nw1: [18.43165452] w2: [-12.04012236] bias: [16.54352267] loss: 39.60852669721343\nEpoch: 930 / 1000\nw1: [18.43825222] w2: [-12.05148819] bias: [16.54351019] loss: 39.59123076179283\nEpoch: 931 / 1000\nw1: [18.4448438] w2: [-12.0628431] bias: [16.54349761] loss: 39.57396778063127\nEpoch: 932 / 1000\nw1: [18.45142925] w2: [-12.0741871] bias: [16.54348493] loss: 39.55673769092252\nEpoch: 933 / 1000\nw1: [18.45800858] w2: [-12.0855202] bias: [16.54347215] loss: 39.53954042998005\nEpoch: 934 / 1000\nw1: [18.46458179] w2: [-12.0968424] bias: [16.54345928] loss: 39.522375935236845\nEpoch: 935 / 1000\nw1: [18.47114889] w2: [-12.10815372] bias: [16.54344631] loss: 39.505244144245076\nEpoch: 936 / 1000\nw1: [18.4777099] w2: [-12.11945417] bias: [16.54343324] loss: 39.488144994675984\nEpoch: 937 / 1000\nw1: [18.4842648] w2: [-12.13074376] bias: [16.54342008] loss: 39.47107842431961\nEpoch: 938 / 1000\nw1: [18.49081361] w2: [-12.1420225] bias: [16.54340681] loss: 39.4540443710845\nEpoch: 939 / 1000\nw1: [18.49735634] w2: [-12.15329039] bias: [16.54339345] loss: 39.43704277299764\nEpoch: 940 / 1000\nw1: [18.50389299] w2: [-12.16454746] bias: [16.54338] loss: 39.42007356820404\nEpoch: 941 / 1000\nw1: [18.51042357] w2: [-12.1757937] bias: [16.54336645] loss: 39.40313669496665\nEpoch: 942 / 1000\nw1: [18.51694807] w2: [-12.18702914] bias: [16.5433528] loss: 39.38623209166608\nEpoch: 943 / 1000\nw1: [18.52346651] w2: [-12.19825378] bias: [16.54333905] loss: 39.36935969680039\nEpoch: 944 / 1000\nw1: [18.5299789] w2: [-12.20946763] bias: [16.54332521] loss: 39.35251944898482\nEpoch: 945 / 1000\nw1: [18.53648524] w2: [-12.22067069] bias: [16.54331127] loss: 39.33571128695167\nEpoch: 946 / 1000\nw1: [18.54298553] w2: [-12.231863] bias: [16.54329724] loss: 39.31893514954995\nEpoch: 947 / 1000\nw1: [18.54947978] w2: [-12.24304454] bias: [16.54328311] loss: 39.30219097574526\nEpoch: 948 / 1000\nw1: [18.555968] w2: [-12.25421533] bias: [16.54326888] loss: 39.285478704619514\nEpoch: 949 / 1000\nw1: [18.56245019] w2: [-12.26537539] bias: [16.54325456] loss: 39.26879827537074\nEpoch: 950 / 1000\nw1: [18.56892635] w2: [-12.27652472] bias: [16.54324014] loss: 39.25214962731283\nEpoch: 951 / 1000\nw1: [18.5753965] w2: [-12.28766333] bias: [16.54322562] loss: 39.235532699875364\nEpoch: 952 / 1000\nw1: [18.58186064] w2: [-12.29879124] bias: [16.54321101] loss: 39.218947432603336\nEpoch: 953 / 1000\nw1: [18.58831878] w2: [-12.30990845] bias: [16.54319631] loss: 39.202393765157\nEpoch: 954 / 1000\nw1: [18.59477091] w2: [-12.32101497] bias: [16.54318151] loss: 39.185871637311585\nEpoch: 955 / 1000\nw1: [18.60121705] w2: [-12.33211082] bias: [16.54316661] loss: 39.169380988957116\nEpoch: 956 / 1000\nw1: [18.60765721] w2: [-12.343196] bias: [16.54315162] loss: 39.152921760098195\nEpoch: 957 / 1000\nw1: [18.61409138] w2: [-12.35427052] bias: [16.54313654] loss: 39.13649389085375\nEpoch: 958 / 1000\nw1: [18.62051957] w2: [-12.3653344] bias: [16.54312135] loss: 39.12009732145685\nEpoch: 959 / 1000\nw1: [18.62694179] w2: [-12.37638764] bias: [16.54310608] loss: 39.10373199225448\nEpoch: 960 / 1000\nw1: [18.63335805] w2: [-12.38743026] bias: [16.54309071] loss: 39.08739784370729\nEpoch: 961 / 1000\nw1: [18.63976835] w2: [-12.39846226] bias: [16.54307524] loss: 39.07109481638947\nEpoch: 962 / 1000\nw1: [18.64617269] w2: [-12.40948366] bias: [16.54305968] loss: 39.0548228509884\nEpoch: 963 / 1000\nw1: [18.65257109] w2: [-12.42049446] bias: [16.54304403] loss: 39.03858188830457\nEpoch: 964 / 1000\nw1: [18.65896354] w2: [-12.43149468] bias: [16.54302828] loss: 39.02237186925123\nEpoch: 965 / 1000\nw1: [18.66535006] w2: [-12.44248432] bias: [16.54301244] loss: 39.00619273485433\nEpoch: 966 / 1000\nw1: [18.67173064] w2: [-12.4534634] bias: [16.5429965] loss: 38.990044426252155\nEpoch: 967 / 1000\nw1: [18.6781053] w2: [-12.46443192] bias: [16.54298047] loss: 38.97392688469518\nEpoch: 968 / 1000\nw1: [18.68447404] w2: [-12.4753899] bias: [16.54296435] loss: 38.957840051545915\nEpoch: 969 / 1000\nw1: [18.69083687] w2: [-12.48633734] bias: [16.54294813] loss: 38.94178386827855\nEpoch: 970 / 1000\nw1: [18.69719378] w2: [-12.49727426] bias: [16.54293182] loss: 38.925758276478874\nEpoch: 971 / 1000\nw1: [18.70354479] w2: [-12.50820067] bias: [16.54291541] loss: 38.90976321784399\nEpoch: 972 / 1000\nw1: [18.70988991] w2: [-12.51911657] bias: [16.54289891] loss: 38.89379863418212\nEpoch: 973 / 1000\nw1: [18.71622913] w2: [-12.53002197] bias: [16.54288232] loss: 38.87786446741241\nEpoch: 974 / 1000\nw1: [18.72256247] w2: [-12.5409169] bias: [16.54286563] loss: 38.861960659564694\nEpoch: 975 / 1000\nw1: [18.72888992] w2: [-12.55180134] bias: [16.54284886] loss: 38.84608715277928\nEpoch: 976 / 1000\nw1: [18.7352115] w2: [-12.56267533] bias: [16.54283198] loss: 38.8302438893068\nEpoch: 977 / 1000\nw1: [18.74152721] w2: [-12.57353886] bias: [16.54281502] loss: 38.81443081150789\nEpoch: 978 / 1000\nw1: [18.74783706] w2: [-12.58439194] bias: [16.54279796] loss: 38.798647861853084\nEpoch: 979 / 1000\nw1: [18.75414104] w2: [-12.59523459] bias: [16.54278081] loss: 38.782894982922556\nEpoch: 980 / 1000\nw1: [18.76043918] w2: [-12.60606682] bias: [16.54276357] loss: 38.76717211740592\nEpoch: 981 / 1000\nw1: [18.76673146] w2: [-12.61688863] bias: [16.54274623] loss: 38.751479208102\nEpoch: 982 / 1000\nw1: [18.77301791] w2: [-12.62770004] bias: [16.5427288] loss: 38.735816197918666\nEpoch: 983 / 1000\nw1: [18.77929852] w2: [-12.63850105] bias: [16.54271128] loss: 38.7201830298726\nEpoch: 984 / 1000\nw1: [18.78557329] w2: [-12.64929168] bias: [16.54269367] loss: 38.70457964708908\nEpoch: 985 / 1000\nw1: [18.79184225] w2: [-12.66007193] bias: [16.54267597] loss: 38.689005992801775\nEpoch: 986 / 1000\nw1: [18.79810538] w2: [-12.67084182] bias: [16.54265817] loss: 38.67346201035258\nEpoch: 987 / 1000\nw1: [18.8043627] w2: [-12.68160135] bias: [16.54264028] loss: 38.65794764319134\nEpoch: 988 / 1000\nw1: [18.81061421] w2: [-12.69235054] bias: [16.5426223] loss: 38.642462834875715\nEpoch: 989 / 1000\nw1: [18.81685991] w2: [-12.70308939] bias: [16.54260423] loss: 38.627007529070895\nEpoch: 990 / 1000\nw1: [18.82309982] w2: [-12.71381792] bias: [16.54258606] loss: 38.61158166954948\nEpoch: 991 / 1000\nw1: [18.82933394] w2: [-12.72453613] bias: [16.54256781] loss: 38.596185200191236\nEpoch: 992 / 1000\nw1: [18.83556227] w2: [-12.73524404] bias: [16.54254946] loss: 38.58081806498287\nEpoch: 993 / 1000\nw1: [18.84178482] w2: [-12.74594165] bias: [16.54253102] loss: 38.56548020801785\nEpoch: 994 / 1000\nw1: [18.84800159] w2: [-12.75662898] bias: [16.54251249] loss: 38.550171573496215\nEpoch: 995 / 1000\nw1: [18.85421259] w2: [-12.76730603] bias: [16.54249387] loss: 38.53489210572434\nEpoch: 996 / 1000\nw1: [18.86041783] w2: [-12.77797281] bias: [16.54247516] loss: 38.51964174911475\nEpoch: 997 / 1000\nw1: [18.86661731] w2: [-12.78862933] bias: [16.54245636] loss: 38.504420448185904\nEpoch: 998 / 1000\nw1: [18.87281103] w2: [-12.79927561] bias: [16.54243747] loss: 38.489228147562045\nEpoch: 999 / 1000\nw1: [18.87899901] w2: [-12.80991166] bias: [16.54241848] loss: 38.47406479197291\nEpoch: 1000 / 1000\nw1: [18.88518124] w2: [-12.82053747] bias: [16.54239941] loss: 38.45893032625361\n##### 최종 w1, w2, bias #######\n[18.88518124] [-12.82053747] [16.54239941]\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### 계산된 Weight와 Bias를 이용하여 Price 예측\n* 예측 feature 역시 0~1사이의 scaled값을 이용하고 Weight와 bias를 적용하여 예측값 계산. ","metadata":{}},{"cell_type":"code","source":"#scaled_features[:, 0] = rm\n#scaled_features[:, 1] = lstat\n##### 위의 연산으로 구해진 최종 w1, w2, bias #######\n#[w1 = 18.88518124] [w2 = -12.82053747] [bias = 16.54239941]\npredicted = scaled_features[:, 0]*w1 + scaled_features[:, 1]*w2 + bias\nbostonDF['PREDICTED_PRICE'] = predicted\nbostonDF.head(10)","metadata":{"execution":{"iopub.status.busy":"2024-05-18T06:10:10.532606Z","iopub.execute_input":"2024-05-18T06:10:10.533110Z","iopub.status.idle":"2024-05-18T06:10:10.570799Z","shell.execute_reply.started":"2024-05-18T06:10:10.533067Z","shell.execute_reply":"2024-05-18T06:10:10.569146Z"},"trusted":true},"execution_count":12,"outputs":[{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"      CRIM    ZN  INDUS  CHAS    NOX     RM    AGE     DIS  RAD    TAX  \\\n0  0.00632  18.0   2.31   0.0  0.538  6.575   65.2  4.0900  1.0  296.0   \n1  0.02731   0.0   7.07   0.0  0.469  6.421   78.9  4.9671  2.0  242.0   \n2  0.02729   0.0   7.07   0.0  0.469  7.185   61.1  4.9671  2.0  242.0   \n3  0.03237   0.0   2.18   0.0  0.458  6.998   45.8  6.0622  3.0  222.0   \n4  0.06905   0.0   2.18   0.0  0.458  7.147   54.2  6.0622  3.0  222.0   \n5  0.02985   0.0   2.18   0.0  0.458  6.430   58.7  6.0622  3.0  222.0   \n6  0.08829  12.5   7.87   0.0  0.524  6.012   66.6  5.5605  5.0  311.0   \n7  0.14455  12.5   7.87   0.0  0.524  6.172   96.1  5.9505  5.0  311.0   \n8  0.21124  12.5   7.87   0.0  0.524  5.631  100.0  6.0821  5.0  311.0   \n9  0.17004  12.5   7.87   0.0  0.524  6.004   85.9  6.5921  5.0  311.0   \n\n   PTRATIO       B  LSTAT  PRICE  PREDICTED_PRICE  \n0     15.3  396.90   4.98   24.0        26.298946  \n1     17.8  396.90   9.14   21.6        24.270017  \n2     17.8  392.83   4.03   34.7        28.842337  \n3     18.7  394.63   2.94   33.4        28.551276  \n4     18.7  396.90   5.33   36.2        28.244935  \n5     18.7  394.12   5.21   28.7        25.692891  \n6     15.2  395.60  12.43   22.9        21.626138  \n7     15.2  396.90  19.15   27.1        19.827786  \n8     15.2  386.63  29.93   16.5        14.056539  \n9     15.2  386.71  17.10   18.9        19.945095  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>CRIM</th>\n      <th>ZN</th>\n      <th>INDUS</th>\n      <th>CHAS</th>\n      <th>NOX</th>\n      <th>RM</th>\n      <th>AGE</th>\n      <th>DIS</th>\n      <th>RAD</th>\n      <th>TAX</th>\n      <th>PTRATIO</th>\n      <th>B</th>\n      <th>LSTAT</th>\n      <th>PRICE</th>\n      <th>PREDICTED_PRICE</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.00632</td>\n      <td>18.0</td>\n      <td>2.31</td>\n      <td>0.0</td>\n      <td>0.538</td>\n      <td>6.575</td>\n      <td>65.2</td>\n      <td>4.0900</td>\n      <td>1.0</td>\n      <td>296.0</td>\n      <td>15.3</td>\n      <td>396.90</td>\n      <td>4.98</td>\n      <td>24.0</td>\n      <td>26.298946</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.02731</td>\n      <td>0.0</td>\n      <td>7.07</td>\n      <td>0.0</td>\n      <td>0.469</td>\n      <td>6.421</td>\n      <td>78.9</td>\n      <td>4.9671</td>\n      <td>2.0</td>\n      <td>242.0</td>\n      <td>17.8</td>\n      <td>396.90</td>\n      <td>9.14</td>\n      <td>21.6</td>\n      <td>24.270017</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.02729</td>\n      <td>0.0</td>\n      <td>7.07</td>\n      <td>0.0</td>\n      <td>0.469</td>\n      <td>7.185</td>\n      <td>61.1</td>\n      <td>4.9671</td>\n      <td>2.0</td>\n      <td>242.0</td>\n      <td>17.8</td>\n      <td>392.83</td>\n      <td>4.03</td>\n      <td>34.7</td>\n      <td>28.842337</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.03237</td>\n      <td>0.0</td>\n      <td>2.18</td>\n      <td>0.0</td>\n      <td>0.458</td>\n      <td>6.998</td>\n      <td>45.8</td>\n      <td>6.0622</td>\n      <td>3.0</td>\n      <td>222.0</td>\n      <td>18.7</td>\n      <td>394.63</td>\n      <td>2.94</td>\n      <td>33.4</td>\n      <td>28.551276</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.06905</td>\n      <td>0.0</td>\n      <td>2.18</td>\n      <td>0.0</td>\n      <td>0.458</td>\n      <td>7.147</td>\n      <td>54.2</td>\n      <td>6.0622</td>\n      <td>3.0</td>\n      <td>222.0</td>\n      <td>18.7</td>\n      <td>396.90</td>\n      <td>5.33</td>\n      <td>36.2</td>\n      <td>28.244935</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>0.02985</td>\n      <td>0.0</td>\n      <td>2.18</td>\n      <td>0.0</td>\n      <td>0.458</td>\n      <td>6.430</td>\n      <td>58.7</td>\n      <td>6.0622</td>\n      <td>3.0</td>\n      <td>222.0</td>\n      <td>18.7</td>\n      <td>394.12</td>\n      <td>5.21</td>\n      <td>28.7</td>\n      <td>25.692891</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>0.08829</td>\n      <td>12.5</td>\n      <td>7.87</td>\n      <td>0.0</td>\n      <td>0.524</td>\n      <td>6.012</td>\n      <td>66.6</td>\n      <td>5.5605</td>\n      <td>5.0</td>\n      <td>311.0</td>\n      <td>15.2</td>\n      <td>395.60</td>\n      <td>12.43</td>\n      <td>22.9</td>\n      <td>21.626138</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>0.14455</td>\n      <td>12.5</td>\n      <td>7.87</td>\n      <td>0.0</td>\n      <td>0.524</td>\n      <td>6.172</td>\n      <td>96.1</td>\n      <td>5.9505</td>\n      <td>5.0</td>\n      <td>311.0</td>\n      <td>15.2</td>\n      <td>396.90</td>\n      <td>19.15</td>\n      <td>27.1</td>\n      <td>19.827786</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>0.21124</td>\n      <td>12.5</td>\n      <td>7.87</td>\n      <td>0.0</td>\n      <td>0.524</td>\n      <td>5.631</td>\n      <td>100.0</td>\n      <td>6.0821</td>\n      <td>5.0</td>\n      <td>311.0</td>\n      <td>15.2</td>\n      <td>386.63</td>\n      <td>29.93</td>\n      <td>16.5</td>\n      <td>14.056539</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>0.17004</td>\n      <td>12.5</td>\n      <td>7.87</td>\n      <td>0.0</td>\n      <td>0.524</td>\n      <td>6.004</td>\n      <td>85.9</td>\n      <td>6.5921</td>\n      <td>5.0</td>\n      <td>311.0</td>\n      <td>15.2</td>\n      <td>386.71</td>\n      <td>17.10</td>\n      <td>18.9</td>\n      <td>19.945095</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}