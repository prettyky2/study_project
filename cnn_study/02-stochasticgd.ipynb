{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30698,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-05-18T06:45:43.844270Z","iopub.execute_input":"2024-05-18T06:45:43.844814Z","iopub.status.idle":"2024-05-18T06:45:44.961653Z","shell.execute_reply.started":"2024-05-18T06:45:43.844786Z","shell.execute_reply":"2024-05-18T06:45:44.960724Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"### Stochastic Gradient Descent와 Mini Batch Gradient Descent 구현\n* SGD 는 전체 데이터에서 한건만 임의로 선택하여 Gradient Descent 로 Weight/Bias Update 계산한 뒤 Weight/Bias 적용\n* Mini Batch GD는 전체 데이터에서 Batch 건수만큼 데이터를 선택하여 Gradient Descent로 Weight/Bias Update 계산한 뒤 Weight/Bias 적용","metadata":{}},{"cell_type":"code","source":"!pip install scikit-learn==1.0.2","metadata":{"execution":{"iopub.status.busy":"2024-05-18T06:45:47.555908Z","iopub.execute_input":"2024-05-18T06:45:47.556423Z","iopub.status.idle":"2024-05-18T06:46:06.022390Z","shell.execute_reply.started":"2024-05-18T06:45:47.556389Z","shell.execute_reply":"2024-05-18T06:46:06.021096Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Collecting scikit-learn==1.0.2\n  Downloading scikit_learn-1.0.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (10 kB)\nRequirement already satisfied: numpy>=1.14.6 in /opt/conda/lib/python3.10/site-packages (from scikit-learn==1.0.2) (1.26.4)\nRequirement already satisfied: scipy>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn==1.0.2) (1.11.4)\nRequirement already satisfied: joblib>=0.11 in /opt/conda/lib/python3.10/site-packages (from scikit-learn==1.0.2) (1.4.0)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn==1.0.2) (3.2.0)\nDownloading scikit_learn-1.0.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (26.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m26.5/26.5 MB\u001b[0m \u001b[31m57.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: scikit-learn\n  Attempting uninstall: scikit-learn\n    Found existing installation: scikit-learn 1.2.2\n    Uninstalling scikit-learn-1.2.2:\n      Successfully uninstalled scikit-learn-1.2.2\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nspopt 0.6.0 requires scikit-learn>=1.1, but you have scikit-learn 1.0.2 which is incompatible.\nspopt 0.6.0 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\nwoodwork 0.30.0 requires scikit-learn>=1.1.0, but you have scikit-learn 1.0.2 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed scikit-learn-1.0.2\n","output_type":"stream"}]},{"cell_type":"code","source":"import sklearn\n\nprint(sklearn.__version__)","metadata":{"execution":{"iopub.status.busy":"2024-05-18T06:46:24.836904Z","iopub.execute_input":"2024-05-18T06:46:24.837327Z","iopub.status.idle":"2024-05-18T06:46:25.282229Z","shell.execute_reply.started":"2024-05-18T06:46:24.837288Z","shell.execute_reply":"2024-05-18T06:46:25.281119Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"1.0.2\n","output_type":"stream"}]},{"cell_type":"code","source":"import numpy as np \nimport pandas as pd \nfrom sklearn.datasets import load_boston\n\nboston = load_boston()  #Boston Housing Dataset을 로드하고, 이를 boston 변수에 저장합니다.\nbostonDF = pd.DataFrame(boston.data, columns=boston.feature_names)\n#boston.data는 데이터셋의 특성(features) 값들을 포함하는 NumPy 배열입니다.\n#boston.feature_names는 각 특성의 이름을 포함하는 리스트입니다.\n#pd.DataFrame은 pandas DataFrame 객체를 생성하는 함수입니다.\n#여기서는 boston.data를 사용해 DataFrame을 만들고, 각 열(column)에 boston.feature_names에 있는 이름을 할당합니다.\nbostonDF['PRICE'] = boston.target\n#boston.target은 데이터셋의 타겟 값(즉, 주택 가격)을 포함하는 NumPy 배열입니다.\n#bostonDF['PRICE']는 DataFrame에 'PRICE'라는 이름의 새로운 열(column)을 추가하고, 여기에 boston.target의 값을 할당합니다.\nprint(bostonDF.shape)\nbostonDF.head()","metadata":{"execution":{"iopub.status.busy":"2024-05-18T06:46:26.073925Z","iopub.execute_input":"2024-05-18T06:46:26.074588Z","iopub.status.idle":"2024-05-18T06:46:26.203048Z","shell.execute_reply.started":"2024-05-18T06:46:26.074555Z","shell.execute_reply":"2024-05-18T06:46:26.201944Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"(506, 14)\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function load_boston is deprecated; `load_boston` is deprecated in 1.0 and will be removed in 1.2.\n\n    The Boston housing prices dataset has an ethical problem. You can refer to\n    the documentation of this function for further details.\n\n    The scikit-learn maintainers therefore strongly discourage the use of this\n    dataset unless the purpose of the code is to study and educate about\n    ethical issues in data science and machine learning.\n\n    In this special case, you can fetch the dataset from the original\n    source::\n\n        import pandas as pd\n        import numpy as np\n\n\n        data_url = \"http://lib.stat.cmu.edu/datasets/boston\"\n        raw_df = pd.read_csv(data_url, sep=\"\\s+\", skiprows=22, header=None)\n        data = np.hstack([raw_df.values[::2, :], raw_df.values[1::2, :2]])\n        target = raw_df.values[1::2, 2]\n\n    Alternative datasets include the California housing dataset (i.e.\n    :func:`~sklearn.datasets.fetch_california_housing`) and the Ames housing\n    dataset. You can load the datasets as follows::\n\n        from sklearn.datasets import fetch_california_housing\n        housing = fetch_california_housing()\n\n    for the California housing dataset and::\n\n        from sklearn.datasets import fetch_openml\n        housing = fetch_openml(name=\"house_prices\", as_frame=True)\n\n    for the Ames housing dataset.\n    \n  warnings.warn(msg, category=FutureWarning)\n","output_type":"stream"},{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"      CRIM    ZN  INDUS  CHAS    NOX     RM   AGE     DIS  RAD    TAX  \\\n0  0.00632  18.0   2.31   0.0  0.538  6.575  65.2  4.0900  1.0  296.0   \n1  0.02731   0.0   7.07   0.0  0.469  6.421  78.9  4.9671  2.0  242.0   \n2  0.02729   0.0   7.07   0.0  0.469  7.185  61.1  4.9671  2.0  242.0   \n3  0.03237   0.0   2.18   0.0  0.458  6.998  45.8  6.0622  3.0  222.0   \n4  0.06905   0.0   2.18   0.0  0.458  7.147  54.2  6.0622  3.0  222.0   \n\n   PTRATIO       B  LSTAT  PRICE  \n0     15.3  396.90   4.98   24.0  \n1     17.8  396.90   9.14   21.6  \n2     17.8  392.83   4.03   34.7  \n3     18.7  394.63   2.94   33.4  \n4     18.7  396.90   5.33   36.2  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>CRIM</th>\n      <th>ZN</th>\n      <th>INDUS</th>\n      <th>CHAS</th>\n      <th>NOX</th>\n      <th>RM</th>\n      <th>AGE</th>\n      <th>DIS</th>\n      <th>RAD</th>\n      <th>TAX</th>\n      <th>PTRATIO</th>\n      <th>B</th>\n      <th>LSTAT</th>\n      <th>PRICE</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.00632</td>\n      <td>18.0</td>\n      <td>2.31</td>\n      <td>0.0</td>\n      <td>0.538</td>\n      <td>6.575</td>\n      <td>65.2</td>\n      <td>4.0900</td>\n      <td>1.0</td>\n      <td>296.0</td>\n      <td>15.3</td>\n      <td>396.90</td>\n      <td>4.98</td>\n      <td>24.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.02731</td>\n      <td>0.0</td>\n      <td>7.07</td>\n      <td>0.0</td>\n      <td>0.469</td>\n      <td>6.421</td>\n      <td>78.9</td>\n      <td>4.9671</td>\n      <td>2.0</td>\n      <td>242.0</td>\n      <td>17.8</td>\n      <td>396.90</td>\n      <td>9.14</td>\n      <td>21.6</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.02729</td>\n      <td>0.0</td>\n      <td>7.07</td>\n      <td>0.0</td>\n      <td>0.469</td>\n      <td>7.185</td>\n      <td>61.1</td>\n      <td>4.9671</td>\n      <td>2.0</td>\n      <td>242.0</td>\n      <td>17.8</td>\n      <td>392.83</td>\n      <td>4.03</td>\n      <td>34.7</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.03237</td>\n      <td>0.0</td>\n      <td>2.18</td>\n      <td>0.0</td>\n      <td>0.458</td>\n      <td>6.998</td>\n      <td>45.8</td>\n      <td>6.0622</td>\n      <td>3.0</td>\n      <td>222.0</td>\n      <td>18.7</td>\n      <td>394.63</td>\n      <td>2.94</td>\n      <td>33.4</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.06905</td>\n      <td>0.0</td>\n      <td>2.18</td>\n      <td>0.0</td>\n      <td>0.458</td>\n      <td>7.147</td>\n      <td>54.2</td>\n      <td>6.0622</td>\n      <td>3.0</td>\n      <td>222.0</td>\n      <td>18.7</td>\n      <td>396.90</td>\n      <td>5.33</td>\n      <td>36.2</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"### SGD 기반으로 Weight/Bias update 값 구하기","metadata":{}},{"cell_type":"code","source":"def get_update_weights_value_sgd(bias, w1, w2, rm_sgd, lstat_sgd, target_sgd, learning_rate=0.01):\n    \n    # 데이터 건수 (1건만 가지고 옴)\n    N = target_sgd.shape[0] #target_sgd 행렬의 행의 갯수를 반환합니다 (열의 갯수는 .shape[1])\n    # 예측 값. (1건만 계산함)\n    predicted_sgd = w1 * rm_sgd + w2*lstat_sgd + bias\n    # 실제값과 예측값의 차이 \n    diff_sgd = target_sgd - predicted_sgd\n    # bias 를 array 기반으로 구하기 위해서 설정. N = 1\n    bias_factors = np.ones((N,))\n    \n    # weight와 bias를 얼마나 update할 것인지를 계산.  로직은 똑같음\n    w1_update = -(2/N)*learning_rate*(np.dot(rm_sgd.T, diff_sgd))\n    w2_update = -(2/N)*learning_rate*(np.dot(lstat_sgd.T, diff_sgd))\n    bias_update = -(2/N)*learning_rate*(np.dot(bias_factors.T, diff_sgd))\n    \n    # Mean Squared Error값을 계산. 여기서 계산하지 않고 st_gradient_descent 함수로 계산함\n    #mse_loss = np.mean(np.square(diff))\n    \n    # weight와 bias가 update되어야 할 값 반환 \n    return bias_update, w1_update, w2_update","metadata":{"execution":{"iopub.status.busy":"2024-05-18T06:46:30.668544Z","iopub.execute_input":"2024-05-18T06:46:30.668939Z","iopub.status.idle":"2024-05-18T06:46:30.676495Z","shell.execute_reply.started":"2024-05-18T06:46:30.668905Z","shell.execute_reply":"2024-05-18T06:46:30.675409Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"### SGD 수행하기","metadata":{}},{"cell_type":"code","source":"print(bostonDF['PRICE'].values.shape)\nprint(np.random.choice(bostonDF['PRICE'].values.shape[0], 1))\nprint(np.random.choice(506, 1))","metadata":{"execution":{"iopub.status.busy":"2024-05-18T06:46:38.015627Z","iopub.execute_input":"2024-05-18T06:46:38.016036Z","iopub.status.idle":"2024-05-18T06:46:38.032824Z","shell.execute_reply.started":"2024-05-18T06:46:38.016005Z","shell.execute_reply":"2024-05-18T06:46:38.031679Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"(506,)\n[310]\n[378]\n","output_type":"stream"}]},{"cell_type":"code","source":"# RM, LSTAT feature array와 PRICE target array를 입력 받아서 iter_epochs수만큼 반복적으로 Weight와 Bias를 update적용. \ndef st_gradient_descent(features, target, iter_epochs=1000, verbose=True):\n    # w1, w2는 numpy array 연산을 위해 1차원 array로 변환하되 초기 값은 0으로 설정\n    # bias도 1차원 array로 변환하되 초기 값은 1로 설정. \n    np.random.seed = 2021\n    w1 = np.zeros((1,))\n    w2 = np.zeros((1,))\n    bias = np.zeros((1, ))\n    print('최초 w1, w2, bias:', w1, w2, bias)\n    \n    # learning_rate와 RM, LSTAT 피처 지정. 호출 시 numpy array형태로 RM과 LSTAT으로 된 2차원 feature가 입력됨.\n    learning_rate = 0.01\n    rm = features[:, 0]\n    lstat = features[:, 1]\n    \n    \n    # iter_epochs 수만큼 반복하면서 weight와 bias update 수행. \n    for i in range(iter_epochs):\n        # iteration 시마다 stochastic gradient descent 를 수행할 데이터를 한개만 추출. 추출할 데이터의 인덱스를 random.choice() 로 선택. \n        stochastic_index = np.random.choice(target.shape[0], 1) #target 행렬의 행(0~505) 중에 1건만 랜덤으로 가지고 온다.\n        rm_sgd = rm[stochastic_index] #rm array에서 1건만 랜덤으로 가지고 와라.\n        lstat_sgd = lstat[stochastic_index] #lstat array에서 1건만 랜덤으로 가지고 와라.\n        target_sgd = target[stochastic_index] #target array에서 1건만 랜덤으로 가지고 와라.\n        # SGD 기반으로 Weight/Bias의 Update를 구함.  \n        bias_update, w1_update, w2_update = get_update_weights_value_sgd(bias, w1, w2, rm_sgd, lstat_sgd, target_sgd, learning_rate)\n        \n        # SGD로 구한 weight/bias의 update 적용. \n        w1 = w1 - w1_update\n        w2 = w2 - w2_update\n        bias = bias - bias_update\n        if verbose:\n            print('Epoch:', i+1,'/', iter_epochs)\n            # Loss는 1 건이 아니라 전체 학습 데이터 기반으로 구해야 함.\n            predicted = w1 * rm + w2*lstat + bias # 506건\n            diff = target - predicted  # 506건\n            mse_loss = np.mean(np.square(diff))\n            print('w1:', w1, 'w2:', w2, 'bias:', bias, 'loss:', mse_loss)\n        \n    return w1, w2, bias","metadata":{"execution":{"iopub.status.busy":"2024-05-18T06:46:39.046069Z","iopub.execute_input":"2024-05-18T06:46:39.046490Z","iopub.status.idle":"2024-05-18T06:46:39.059306Z","shell.execute_reply.started":"2024-05-18T06:46:39.046449Z","shell.execute_reply":"2024-05-18T06:46:39.058264Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"from sklearn.preprocessing import MinMaxScaler\n\nscaler = MinMaxScaler()\nscaled_features = scaler.fit_transform(bostonDF[['RM', 'LSTAT']])\n\nw1, w2, bias = st_gradient_descent(scaled_features, bostonDF['PRICE'].values, iter_epochs=1000, verbose=True)\nprint('##### 최종 w1, w2, bias #######')\nprint(w1, w2, bias)","metadata":{"execution":{"iopub.status.busy":"2024-05-18T06:46:41.023898Z","iopub.execute_input":"2024-05-18T06:46:41.024809Z","iopub.status.idle":"2024-05-18T06:46:41.433666Z","shell.execute_reply.started":"2024-05-18T06:46:41.024760Z","shell.execute_reply":"2024-05-18T06:46:41.432820Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"최초 w1, w2, bias: [0.] [0.] [0.]\nEpoch: 1 / 1000\nw1: [0.17000383] w2: [0.18219371] bias: [0.39] loss: 568.5779256883917\nEpoch: 2 / 1000\nw1: [0.30617707] w2: [0.30475996] bias: [0.66084152] loss: 552.1472341928779\nEpoch: 3 / 1000\nw1: [0.38242538] w2: [0.40952355] bias: [0.81442697] loss: 542.6210006390965\nEpoch: 4 / 1000\nw1: [0.55901273] w2: [0.52256852] bias: [1.13299227] loss: 523.7866608563384\nEpoch: 5 / 1000\nw1: [0.68710416] w2: [0.61458712] bias: [1.44465155] loss: 506.95783174331166\nEpoch: 6 / 1000\nw1: [0.89454544] w2: [0.73704609] bias: [1.89202182] loss: 482.93726599913174\nEpoch: 7 / 1000\nw1: [1.09937751] w2: [0.84007552] bias: [2.33652019] loss: 459.96275534663965\nEpoch: 8 / 1000\nw1: [1.39161348] w2: [0.88080345] bias: [2.8334832] loss: 434.37794100692\nEpoch: 9 / 1000\nw1: [2.16001853] w2: [0.91708114] bias: [3.75285643] loss: 385.14254736099736\nEpoch: 10 / 1000\nw1: [2.43713212] w2: [1.0201858] bias: [4.22226758] loss: 362.95290400737775\nEpoch: 11 / 1000\nw1: [2.58098003] w2: [1.16051616] bias: [4.53638569] loss: 348.8362092842256\nEpoch: 12 / 1000\nw1: [2.65083433] w2: [1.23888248] bias: [4.66571153] loss: 342.7913020767893\nEpoch: 13 / 1000\nw1: [2.67169131] w2: [1.26256325] bias: [4.7111425] loss: 340.77958341909755\nEpoch: 14 / 1000\nw1: [2.75191174] w2: [1.33432745] bias: [4.87069678] loss: 333.7298778680473\nEpoch: 15 / 1000\nw1: [2.81601935] w2: [1.40624683] bias: [4.98938341] loss: 328.3455924406942\nEpoch: 16 / 1000\nw1: [2.89671538] w2: [1.48274599] bias: [5.1525576] loss: 321.32202706958356\nEpoch: 17 / 1000\nw1: [3.10093575] w2: [1.51813794] bias: [5.51385458] loss: 306.5476122131013\nEpoch: 18 / 1000\nw1: [3.28458852] w2: [1.56434826] bias: [5.84945817] loss: 293.25585860346814\nEpoch: 19 / 1000\nw1: [3.38652722] w2: [1.61355115] bias: [6.07374909] loss: 284.8292408171925\nEpoch: 20 / 1000\nw1: [3.38657182] w2: [1.61363477] bias: [6.07386724] loss: 284.82465786352526\nEpoch: 21 / 1000\nw1: [3.59440675] w2: [1.65287932] bias: [6.4015683] loss: 272.0609611738508\nEpoch: 22 / 1000\nw1: [3.69564297] w2: [1.71477717] bias: [6.63330155] loss: 263.8096589398499\nEpoch: 23 / 1000\nw1: [3.78747909] w2: [1.79798478] bias: [6.82949178] loss: 256.7111909049668\nEpoch: 24 / 1000\nw1: [3.89882376] w2: [1.8583962] bias: [7.04329157] loss: 249.12484339101377\nEpoch: 25 / 1000\nw1: [3.86149484] w2: [1.80727635] bias: [6.97548105] loss: 251.70785599167218\nEpoch: 26 / 1000\nw1: [3.96519607] w2: [1.84811411] bias: [7.20212122] loss: 244.11538050855245\nEpoch: 27 / 1000\nw1: [4.04780158] w2: [1.91493115] bias: [7.387389] loss: 237.90697129864708\nEpoch: 28 / 1000\nw1: [4.1950143] w2: [1.97625731] bias: [7.61769572] loss: 229.82567623877583\nEpoch: 29 / 1000\nw1: [4.48128597] w2: [2.08047324] bias: [8.01441686] loss: 215.9619917363839\nEpoch: 30 / 1000\nw1: [4.56260237] w2: [2.13941568] bias: [8.19095192] loss: 210.57437698909246\nEpoch: 31 / 1000\nw1: [4.90884278] w2: [2.15513137] bias: [8.69054515] loss: 195.02255446201045\nEpoch: 32 / 1000\nw1: [4.97476253] w2: [2.19308644] bias: [8.85274938] loss: 190.6378259868575\nEpoch: 33 / 1000\nw1: [5.10867648] w2: [2.24152407] bias: [9.08525001] loss: 184.04478122110382\nEpoch: 34 / 1000\nw1: [5.18665379] w2: [2.2872012] bias: [9.2414141] loss: 179.82906556379518\nEpoch: 35 / 1000\nw1: [5.11569394] w2: [2.18172479] bias: [9.08875974] loss: 184.09308934839737\nEpoch: 36 / 1000\nw1: [5.23658006] w2: [2.20047989] bias: [9.31994503] loss: 177.97087766251042\nEpoch: 37 / 1000\nw1: [5.30154594] w2: [2.23307332] bias: [9.45777305] loss: 174.4222926950128\nEpoch: 38 / 1000\nw1: [5.39430416] w2: [2.29365931] bias: [9.65504492] loss: 169.4191656990335\nEpoch: 39 / 1000\nw1: [5.52030996] w2: [2.32669483] bias: [9.89496622] loss: 163.49000298776403\nEpoch: 40 / 1000\nw1: [5.80385955] w2: [2.36097334] bias: [10.32932057] loss: 152.74791777706602\nEpoch: 41 / 1000\nw1: [5.93148115] w2: [2.40772919] bias: [10.5462773] loss: 147.72082740525752\nEpoch: 42 / 1000\nw1: [6.2269087] w2: [2.44891798] bias: [10.99723851] loss: 137.66062238633123\nEpoch: 43 / 1000\nw1: [6.29903954] w2: [2.47261196] bias: [11.13640703] loss: 134.8671114606041\nEpoch: 44 / 1000\nw1: [6.54467528] w2: [2.51384724] bias: [11.48232525] loss: 127.66267477673983\nEpoch: 45 / 1000\nw1: [6.59031421] w2: [2.54152254] bias: [11.57632271] loss: 125.94537619547519\nEpoch: 46 / 1000\nw1: [6.71746876] w2: [2.5701627] bias: [11.79255589] loss: 121.9754173128007\nEpoch: 47 / 1000\nw1: [6.99835754] w2: [2.61766615] bias: [12.14750953] loss: 115.26569870716821\nEpoch: 48 / 1000\nw1: [7.05476684] w2: [2.67001929] bias: [12.29390441] loss: 113.07550995435452\nEpoch: 49 / 1000\nw1: [7.07870724] w2: [2.68084799] bias: [12.34581342] loss: 112.29313785222169\nEpoch: 50 / 1000\nw1: [7.1215333] w2: [2.70743192] bias: [12.4333954] loss: 110.97007413997379\nEpoch: 51 / 1000\nw1: [7.14298895] w2: [2.72346829] bias: [12.47725636] loss: 110.31606501414082\nEpoch: 52 / 1000\nw1: [7.18592982] w2: [2.75682367] bias: [12.56967219] loss: 108.97808321791575\nEpoch: 53 / 1000\nw1: [7.4335127] w2: [2.78660057] bias: [12.92003419] loss: 103.65445885049051\nEpoch: 54 / 1000\nw1: [7.45984212] w2: [2.80479224] bias: [12.97676965] loss: 102.92434388820928\nEpoch: 55 / 1000\nw1: [7.51183622] w2: [2.82440074] bias: [13.0662674] loss: 101.7304466864172\nEpoch: 56 / 1000\nw1: [7.53379585] w2: [2.83545837] bias: [13.11592395] loss: 101.12485998228969\nEpoch: 57 / 1000\nw1: [7.56645528] w2: [2.85573145] bias: [13.18271453] loss: 100.30331865833409\nEpoch: 58 / 1000\nw1: [7.60507396] w2: [2.88237302] bias: [13.27663385] loss: 99.22044702161924\nEpoch: 59 / 1000\nw1: [7.63234453] w2: [2.89478121] bias: [13.33003915] loss: 98.58568310547992\nEpoch: 60 / 1000\nw1: [7.63052286] w2: [2.89347255] bias: [13.32560685] loss: 98.63528532254303\nEpoch: 61 / 1000\nw1: [7.66038183] w2: [2.92606574] bias: [13.38547391] loss: 97.94051958312212\nEpoch: 62 / 1000\nw1: [7.57151329] w2: [2.82967097] bias: [13.21124217] loss: 100.0155096116561\nEpoch: 63 / 1000\nw1: [7.67718694] w2: [2.84841226] bias: [13.37450764] loss: 97.9286653700978\nEpoch: 64 / 1000\nw1: [7.68738813] w2: [2.85610816] bias: [13.39892966] loss: 97.6590368392863\nEpoch: 65 / 1000\nw1: [7.71633832] w2: [2.86991382] bias: [13.45751479] loss: 96.99365636418536\nEpoch: 66 / 1000\nw1: [7.76993882] w2: [2.88111432] bias: [13.55302212] loss: 95.89583861704783\nEpoch: 67 / 1000\nw1: [7.69430521] w2: [2.804642] bias: [13.42436118] loss: 97.39756289182326\nEpoch: 68 / 1000\nw1: [7.61706514] w2: [2.67113589] bias: [13.20681345] loss: 99.78572749923369\nEpoch: 69 / 1000\nw1: [7.77746576] w2: [2.69467056] bias: [13.46526693] loss: 96.54926201551223\nEpoch: 70 / 1000\nw1: [7.71034993] w2: [2.6353866] bias: [13.31136645] loss: 98.27449974243015\nEpoch: 71 / 1000\nw1: [7.671399] w2: [2.60254356] bias: [13.24184356] loss: 99.13622761537164\nEpoch: 72 / 1000\nw1: [7.81131141] w2: [2.64123349] bias: [13.48070613] loss: 96.21524313659509\nEpoch: 73 / 1000\nw1: [7.80448076] w2: [2.63583274] bias: [13.46785496] loss: 96.36264691314445\nEpoch: 74 / 1000\nw1: [7.76756282] w2: [2.6054551] bias: [13.38905139] loss: 97.25096985886097\nEpoch: 75 / 1000\nw1: [8.20452865] w2: [2.63768834] bias: [13.87375377] loss: 90.97511266870826\nEpoch: 76 / 1000\nw1: [8.24193946] w2: [2.65180977] bias: [13.95002214] loss: 90.24873204819346\nEpoch: 77 / 1000\nw1: [8.22788948] w2: [2.64015871] bias: [13.91731614] loss: 90.54474354689522\nEpoch: 78 / 1000\nw1: [8.2361306] w2: [2.64497445] bias: [13.93478583] loss: 90.38166063579088\nEpoch: 79 / 1000\nw1: [8.29021333] w2: [2.68403256] bias: [14.05413371] loss: 89.31114433558872\nEpoch: 80 / 1000\nw1: [8.46467975] w2: [2.72885101] bias: [14.31113069] loss: 86.87880597376339\nEpoch: 81 / 1000\nw1: [8.75839459] w2: [2.7875095] bias: [14.67575907] loss: 83.68476187358934\nEpoch: 82 / 1000\nw1: [8.80473432] w2: [2.80981076] bias: [14.76322637] loss: 83.10851552921311\nEpoch: 83 / 1000\nw1: [8.72088734] w2: [2.69364237] bias: [14.57238581] loss: 84.25972799646671\nEpoch: 84 / 1000\nw1: [8.65986692] w2: [2.63840767] bias: [14.43547025] loss: 85.20502252685036\nEpoch: 85 / 1000\nw1: [8.65563111] w2: [2.63569406] bias: [14.42744896] loss: 85.26592631202257\nEpoch: 86 / 1000\nw1: [8.63227311] w2: [2.60098573] bias: [14.37433115] loss: 85.6399555506481\nEpoch: 87 / 1000\nw1: [8.65192995] w2: [2.6125128] bias: [14.41053052] loss: 85.35642803107714\nEpoch: 88 / 1000\nw1: [8.60569914] w2: [2.57086808] bias: [14.31347557] loss: 86.08958181926165\nEpoch: 89 / 1000\nw1: [8.63663184] w2: [2.63782072] bias: [14.40069209] loss: 85.50240202602959\nEpoch: 90 / 1000\nw1: [8.73520302] w2: [2.67127643] bias: [14.56908586] loss: 84.19376986893394\nEpoch: 91 / 1000\nw1: [8.70725394] w2: [2.64222318] bias: [14.49851681] loss: 84.65727320540006\nEpoch: 92 / 1000\nw1: [8.71725516] w2: [2.64697058] bias: [14.51793511] loss: 84.5149049255868\nEpoch: 93 / 1000\nw1: [8.73558575] w2: [2.65937088] bias: [14.57105411] loss: 84.16973994913832\nEpoch: 94 / 1000\nw1: [8.73692013] w2: [2.66009487] bias: [14.57361634] loss: 84.15155184740364\nEpoch: 95 / 1000\nw1: [8.7788506] w2: [2.67526142] bias: [14.64641578] loss: 83.62258622683228\nEpoch: 96 / 1000\nw1: [8.79443123] w2: [2.68439813] bias: [14.67510855] loss: 83.42686083849155\nEpoch: 97 / 1000\nw1: [9.19142317] w2: [2.71368266] bias: [15.11547006] loss: 80.11738621865507\nEpoch: 98 / 1000\nw1: [9.0600395] w2: [2.48045498] bias: [14.81617222] loss: 81.48248883855817\nEpoch: 99 / 1000\nw1: [9.10368114] w2: [2.49422817] bias: [14.90374074] loss: 80.95361790177323\nEpoch: 100 / 1000\nw1: [9.09152526] w2: [2.42866429] bias: [14.79379002] loss: 81.40052480102199\nEpoch: 101 / 1000\nw1: [9.12170415] w2: [2.44762288] bias: [14.87258124] loss: 80.96118786595406\nEpoch: 102 / 1000\nw1: [9.39432447] w2: [2.46922146] bias: [15.18819912] loss: 78.8249085502941\nEpoch: 103 / 1000\nw1: [9.22894533] w2: [2.24274405] bias: [14.88777667] loss: 80.2408689699732\nEpoch: 104 / 1000\nw1: [9.22736485] w2: [2.23999619] bias: [14.88330106] loss: 80.26261139611995\nEpoch: 105 / 1000\nw1: [9.39064967] w2: [2.26740709] bias: [15.11324804] loss: 78.80281147008968\nEpoch: 106 / 1000\nw1: [9.58850264] w2: [2.30772404] bias: [15.39422617] loss: 77.3203340171015\nEpoch: 107 / 1000\nw1: [9.45350391] w2: [2.1535855] bias: [15.14429415] loss: 78.31576506002568\nEpoch: 108 / 1000\nw1: [9.46016986] w2: [2.16086186] bias: [15.15765934] loss: 78.25612264083875\nEpoch: 109 / 1000\nw1: [9.47677342] w2: [2.16499159] bias: [15.18759164] loss: 78.10175235687964\nEpoch: 110 / 1000\nw1: [9.40750073] w2: [2.09867314] bias: [15.05920593] loss: 78.71316303263595\nEpoch: 111 / 1000\nw1: [9.49416601] w2: [2.12068925] bias: [15.19676861] loss: 77.94760398772624\nEpoch: 112 / 1000\nw1: [9.30945743] w2: [1.92199235] bias: [14.8974845] loss: 79.50858386264159\nEpoch: 113 / 1000\nw1: [9.33025837] w2: [1.93634214] bias: [14.9480717] loss: 79.22988563351038\nEpoch: 114 / 1000\nw1: [9.31945957] w2: [1.92828656] bias: [14.92694766] loss: 79.3505224524298\nEpoch: 115 / 1000\nw1: [9.3675666] w2: [1.94321443] bias: [15.02169127] loss: 78.79662114627212\nEpoch: 116 / 1000\nw1: [9.45533507] w2: [1.96551078] bias: [15.16100503] loss: 77.98149068024591\nEpoch: 117 / 1000\nw1: [9.59363126] w2: [1.98371527] bias: [15.34842851] loss: 76.92047457237622\nEpoch: 118 / 1000\nw1: [9.59713562] w2: [1.98589057] bias: [15.35559515] loss: 76.89050192840746\nEpoch: 119 / 1000\nw1: [9.64133506] w2: [2.00468763] bias: [15.44360617] loss: 76.5212559813981\nEpoch: 120 / 1000\nw1: [9.67691917] w2: [2.02569402] bias: [15.52034727] loss: 76.23781117407249\nEpoch: 121 / 1000\nw1: [9.65520935] w2: [2.00716234] bias: [15.47247927] loss: 76.4017739480994\nEpoch: 122 / 1000\nw1: [9.78891293] w2: [2.01800341] bias: [15.69074609] loss: 75.49699502999893\nEpoch: 123 / 1000\nw1: [9.86380467] w2: [2.03013093] bias: [15.81420154] loss: 75.07396464680122\nEpoch: 124 / 1000\nw1: [9.86210859] w2: [2.02911806] bias: [15.81042515] loss: 75.08340067171075\nEpoch: 125 / 1000\nw1: [9.75465566] w2: [1.95417706] bias: [15.59808255] loss: 75.69501988712932\nEpoch: 126 / 1000\nw1: [9.77701076] w2: [1.95786311] bias: [15.6322469] loss: 75.55667663396751\nEpoch: 127 / 1000\nw1: [10.1169206] w2: [1.98922195] bias: [16.20046902] loss: 73.89707745458826\nEpoch: 128 / 1000\nw1: [10.12533153] w2: [1.99192535] bias: [16.21570561] loss: 73.87379590307373\nEpoch: 129 / 1000\nw1: [10.11939917] w2: [1.9882183] bias: [16.2040134] loss: 73.88697316373185\nEpoch: 130 / 1000\nw1: [10.10695851] w2: [1.97902193] bias: [16.17743633] loss: 73.91510884995525\nEpoch: 131 / 1000\nw1: [10.08553103] w2: [1.97097197] bias: [16.12929598] loss: 73.98904207834516\nEpoch: 132 / 1000\nw1: [10.11747739] w2: [1.98704082] bias: [16.19862157] loss: 73.89286067160761\nEpoch: 133 / 1000\nw1: [10.41851616] w2: [2.00240878] bias: [16.60219757] loss: 73.31090623276619\nEpoch: 134 / 1000\nw1: [10.50851725] w2: [2.01448093] bias: [16.74999986] loss: 73.29744647402667\nEpoch: 135 / 1000\nw1: [10.5018417] w2: [2.00942808] bias: [16.7316148] loss: 73.28038944828535\nEpoch: 136 / 1000\nw1: [10.35905427] w2: [1.94525053] bias: [16.5329987] loss: 73.24054214605648\nEpoch: 137 / 1000\nw1: [10.25459625] w2: [1.86201542] bias: [16.34906941] loss: 73.254429168531\nEpoch: 138 / 1000\nw1: [10.29877724] w2: [1.87706533] bias: [16.42765857] loss: 73.18329014126023\nEpoch: 139 / 1000\nw1: [10.28377333] w2: [1.79614035] bias: [16.29194727] loss: 73.087534295667\nEpoch: 140 / 1000\nw1: [10.16614579] w2: [1.67720843] bias: [16.0918501] loss: 73.24521904677512\nEpoch: 141 / 1000\nw1: [10.25247147] w2: [1.71331525] bias: [16.22362364] loss: 73.01181241687874\nEpoch: 142 / 1000\nw1: [10.21200683] w2: [1.63786769] bias: [16.11288166] loss: 73.04321390041294\nEpoch: 143 / 1000\nw1: [10.30421348] w2: [1.65879173] bias: [16.25955234] loss: 72.76592118977103\nEpoch: 144 / 1000\nw1: [10.32632466] w2: [1.67310449] bias: [16.30937887] loss: 72.72253587993075\nEpoch: 145 / 1000\nw1: [10.25844545] w2: [1.57303172] bias: [16.13790666] loss: 72.78407491537807\nEpoch: 146 / 1000\nw1: [10.45132925] w2: [1.58720318] bias: [16.41854802] loss: 72.26099121623582\nEpoch: 147 / 1000\nw1: [10.43873132] w2: [1.56530021] bias: [16.38287325] loss: 72.24497266524286\nEpoch: 148 / 1000\nw1: [10.29822696] w2: [1.43408897] bias: [16.13713188] loss: 72.41333841976872\nEpoch: 149 / 1000\nw1: [10.27331036] w2: [1.4140383] bias: [16.07316743] loss: 72.5037217620143\nEpoch: 150 / 1000\nw1: [10.20878144] w2: [1.35591064] bias: [15.93769821] loss: 72.73737877305118\nEpoch: 151 / 1000\nw1: [10.21489686] w2: [1.36119935] bias: [15.951408] loss: 72.7103477021854\nEpoch: 152 / 1000\nw1: [10.38354142] w2: [1.38757189] bias: [16.57607513] loss: 71.84842306346037\nEpoch: 153 / 1000\nw1: [10.61934194] w2: [1.43466417] bias: [16.86880654] loss: 71.71867973000269\nEpoch: 154 / 1000\nw1: [10.69246868] w2: [1.46525036] bias: [16.9804323] loss: 71.81618416793883\nEpoch: 155 / 1000\nw1: [10.68655808] w2: [1.46343608] bias: [16.96923136] loss: 71.80621795994576\nEpoch: 156 / 1000\nw1: [10.63440113] w2: [1.40017666] bias: [16.81013895] loss: 71.5797511262498\nEpoch: 157 / 1000\nw1: [10.62171957] w2: [1.39225212] bias: [16.78514458] loss: 71.5621936625693\nEpoch: 158 / 1000\nw1: [10.59349049] w2: [1.37693609] bias: [16.73094019] loss: 71.53767592540922\nEpoch: 159 / 1000\nw1: [10.51911397] w2: [1.29446472] bias: [16.53724204] loss: 71.43337891330444\nEpoch: 160 / 1000\nw1: [10.4035644] w2: [1.21387686] bias: [16.30889929] loss: 71.56786588485994\nEpoch: 161 / 1000\nw1: [10.3871318] w2: [1.17515374] bias: [16.25612284] loss: 71.56468122449952\nEpoch: 162 / 1000\nw1: [10.28077079] w2: [1.04769662] bias: [16.00218957] loss: 71.87335293454221\nEpoch: 163 / 1000\nw1: [10.28170654] w2: [1.04834753] bias: [16.00409653] loss: 71.86923640569097\nEpoch: 164 / 1000\nw1: [10.34280944] w2: [1.06972107] bias: [16.10482173] loss: 71.62031451907907\nEpoch: 165 / 1000\nw1: [10.57755016] w2: [1.0883186] bias: [16.37658567] loss: 70.93605918655544\nEpoch: 166 / 1000\nw1: [10.49622823] w2: [1.01512269] bias: [16.21484057] loss: 71.06197781688218\nEpoch: 167 / 1000\nw1: [10.48578496] w2: [1.01058844] bias: [16.19350015] loss: 71.09628009214948\nEpoch: 168 / 1000\nw1: [10.4590487] w2: [0.93217114] bias: [16.09127558] loss: 71.12131697659206\nEpoch: 169 / 1000\nw1: [10.47237299] w2: [0.93567485] bias: [16.11597877] loss: 71.06659096609674\nEpoch: 170 / 1000\nw1: [10.4903555] w2: [0.94128072] bias: [16.1509455] loss: 70.99587590780152\nEpoch: 171 / 1000\nw1: [10.49713683] w2: [0.94290138] bias: [16.16476501] loss: 70.96854004284653\nEpoch: 172 / 1000\nw1: [10.56937662] w2: [0.95805424] bias: [16.28572088] loss: 70.73496090445772\nEpoch: 173 / 1000\nw1: [10.57577668] w2: [0.95939162] bias: [16.29712475] loss: 70.71624374067908\nEpoch: 174 / 1000\nw1: [10.56745793] w2: [0.95611567] bias: [16.27894402] loss: 70.74040407633103\nEpoch: 175 / 1000\nw1: [10.46304685] w2: [0.87976884] bias: [16.07308024] loss: 71.03597755436545\nEpoch: 176 / 1000\nw1: [10.48052425] w2: [0.88609054] bias: [16.10342441] loss: 70.96817298005988\nEpoch: 177 / 1000\nw1: [10.47911522] w2: [0.88365963] bias: [16.09930006] loss: 70.97205597046695\nEpoch: 178 / 1000\nw1: [10.46709002] w2: [0.87417877] bias: [16.07037862] loss: 71.0203974698575\nEpoch: 179 / 1000\nw1: [10.75468319] w2: [0.93203062] bias: [16.44950182] loss: 70.26020155456109\nEpoch: 180 / 1000\nw1: [10.76178012] w2: [0.93399223] bias: [16.46206164] loss: 70.2491268260098\nEpoch: 181 / 1000\nw1: [10.77266449] w2: [0.93792918] bias: [16.480959] loss: 70.2354932038141\nEpoch: 182 / 1000\nw1: [10.75720098] w2: [0.92335252] bias: [16.44338729] loss: 70.23890227322468\nEpoch: 183 / 1000\nw1: [10.68576303] w2: [0.88635332] bias: [16.31320759] loss: 70.3500030417712\nEpoch: 184 / 1000\nw1: [10.66105191] w2: [0.86663942] bias: [16.2554524] loss: 70.3999727232688\nEpoch: 185 / 1000\nw1: [10.64456298] w2: [0.85815795] bias: [16.21815039] loss: 70.4483600250258\nEpoch: 186 / 1000\nw1: [10.6440941] w2: [0.85795027] bias: [16.21708133] loss: 70.44988301557332\nEpoch: 187 / 1000\nw1: [10.62435376] w2: [0.84206697] bias: [16.17754763] loss: 70.49667766870982\nEpoch: 188 / 1000\nw1: [10.46366153] w2: [0.68742897] bias: [15.87609086] loss: 71.0143902812426\nEpoch: 189 / 1000\nw1: [10.45707117] w2: [0.68070022] bias: [15.85923877] loss: 71.05198431022981\nEpoch: 190 / 1000\nw1: [10.40771009] w2: [0.62914722] bias: [15.73283276] loss: 71.36381628455625\nEpoch: 191 / 1000\nw1: [10.3337903] w2: [0.52111911] bias: [15.5826625] loss: 71.77860012958679\nEpoch: 192 / 1000\nw1: [10.25176118] w2: [0.47484667] bias: [15.47094277] loss: 72.28075199855654\nEpoch: 193 / 1000\nw1: [10.1513001] w2: [0.404782] bias: [15.27241708] loss: 73.17924027170697\nEpoch: 194 / 1000\nw1: [10.12675931] w2: [0.38823778] bias: [15.21761246] loss: 73.4460651218198\nEpoch: 195 / 1000\nw1: [10.11651604] w2: [0.37915604] bias: [15.19488302] loss: 73.55738758835476\nEpoch: 196 / 1000\nw1: [10.14885005] w2: [0.38484512] bias: [15.25447032] loss: 73.22888658231078\nEpoch: 197 / 1000\nw1: [10.18752336] w2: [0.39324699] bias: [15.31213774] loss: 72.90253767600487\nEpoch: 198 / 1000\nw1: [10.1942524] w2: [0.39706766] bias: [15.32556752] loss: 72.83812849112478\nEpoch: 199 / 1000\nw1: [10.16990988] w2: [0.3608964] bias: [15.27021083] loss: 73.0710505240487\nEpoch: 200 / 1000\nw1: [10.2729389] w2: [0.3842387] bias: [15.43320218] loss: 72.20967640089073\nEpoch: 201 / 1000\nw1: [10.15230005] w2: [0.21025849] bias: [15.13649428] loss: 73.47032779289242\nEpoch: 202 / 1000\nw1: [10.09254183] w2: [0.05325121] bias: [14.93291847] loss: 74.4100158145675\nEpoch: 203 / 1000\nw1: [10.05215179] w2: [-0.00259912] bias: [14.82799281] loss: 75.01615651147075\nEpoch: 204 / 1000\nw1: [10.09853375] w2: [0.03089756] bias: [14.93034693] loss: 74.37819595416144\nEpoch: 205 / 1000\nw1: [10.19235285] w2: [0.04614459] bias: [15.06952881] loss: 73.42865395243987\nEpoch: 206 / 1000\nw1: [10.19216559] w2: [0.04599695] bias: [15.06907844] loss: 73.43108656434272\nEpoch: 207 / 1000\nw1: [10.19885993] w2: [0.05183421] bias: [15.08531346] loss: 73.34458931935163\nEpoch: 208 / 1000\nw1: [10.14092771] w2: [-0.00445138] bias: [14.96047934] loss: 74.0465514318182\nEpoch: 209 / 1000\nw1: [10.02279576] w2: [-0.13933181] bias: [14.74177388] loss: 75.4590042150149\nEpoch: 210 / 1000\nw1: [9.95226548] w2: [-0.31145992] bias: [14.52726485] loss: 76.8238221943041\nEpoch: 211 / 1000\nw1: [9.90775329] w2: [-0.37847188] bias: [14.40025055] loss: 77.74796790339143\nEpoch: 212 / 1000\nw1: [9.89442971] w2: [-0.39086601] bias: [14.37340278] loss: 77.96955195105063\nEpoch: 213 / 1000\nw1: [9.92663583] w2: [-0.38147943] bias: [14.43403905] loss: 77.45446389475221\nEpoch: 214 / 1000\nw1: [9.95453458] w2: [-0.36223313] bias: [14.50188787] loss: 76.93411517767001\nEpoch: 215 / 1000\nw1: [9.91716491] w2: [-0.43380369] bias: [14.38999325] loss: 77.74318755853912\nEpoch: 216 / 1000\nw1: [9.97431933] w2: [-0.41988876] bias: [14.50255511] loss: 76.81172196826384\nEpoch: 217 / 1000\nw1: [10.01911391] w2: [-0.41185178] bias: [14.58001797] loss: 76.17122334372584\nEpoch: 218 / 1000\nw1: [10.02518046] w2: [-0.40622833] bias: [14.59384387] loss: 76.06997302396019\nEpoch: 219 / 1000\nw1: [9.97307329] w2: [-0.48783385] bias: [14.48001076] loss: 76.9162208783004\nEpoch: 220 / 1000\nw1: [9.94598036] w2: [-0.51571876] bias: [14.41430536] loss: 77.4247147182741\nEpoch: 221 / 1000\nw1: [9.95686755] w2: [-0.51106959] bias: [14.43930328] loss: 77.22401276142116\nEpoch: 222 / 1000\nw1: [9.97390008] w2: [-0.50397415] bias: [14.46594979] loss: 76.98902513927129\nEpoch: 223 / 1000\nw1: [10.19389643] w2: [-0.48781069] bias: [14.78603926] loss: 74.2746979717723\nEpoch: 224 / 1000\nw1: [10.13956593] w2: [-0.53301122] bias: [14.68404254] loss: 75.00517685009177\nEpoch: 225 / 1000\nw1: [10.25751811] w2: [-0.5171899] bias: [14.87774689] loss: 73.53848633775408\nEpoch: 226 / 1000\nw1: [10.19921734] w2: [-0.5699628] bias: [14.74693361] loss: 74.39719942692577\nEpoch: 227 / 1000\nw1: [10.07376518] w2: [-0.75643788] bias: [14.47705116] loss: 76.35434459023529\nEpoch: 228 / 1000\nw1: [10.28496838] w2: [-0.7032832] bias: [14.77945706] loss: 73.78449232106067\nEpoch: 229 / 1000\nw1: [10.28985613] w2: [-0.69966037] bias: [14.78957973] loss: 73.71494865706129\nEpoch: 230 / 1000\nw1: [10.25848304] w2: [-0.73771175] bias: [14.69388356] loss: 74.32350306826605\nEpoch: 231 / 1000\nw1: [10.24596888] w2: [-0.7563069] bias: [14.66542544] loss: 74.51919496731904\nEpoch: 232 / 1000\nw1: [10.17426798] w2: [-0.89075197] bias: [14.47547264] loss: 75.85272051983267\nEpoch: 233 / 1000\nw1: [10.14044216] w2: [-0.93001245] bias: [14.39708146] loss: 76.47078115181941\nEpoch: 234 / 1000\nw1: [10.30854059] w2: [-0.8576596] bias: [14.7367275] loss: 73.80820803302156\nEpoch: 235 / 1000\nw1: [10.2556955] w2: [-0.99650351] bias: [14.55670234] loss: 74.96039227980363\nEpoch: 236 / 1000\nw1: [10.28867568] w2: [-0.98015498] bias: [14.6316038] loss: 74.39580450271998\nEpoch: 237 / 1000\nw1: [10.32738191] w2: [-0.96588277] bias: [14.70235961] loss: 73.84760515778548\nEpoch: 238 / 1000\nw1: [10.36478892] w2: [-0.95734095] bias: [14.76921823] loss: 73.3396321313707\nEpoch: 239 / 1000\nw1: [10.3083278] w2: [-1.01703307] bias: [14.65297737] loss: 74.1712054046698\nEpoch: 240 / 1000\nw1: [10.31910387] w2: [-1.01257572] bias: [14.67495483] loss: 74.00546725260754\nEpoch: 241 / 1000\nw1: [10.33277875] w2: [-0.9916763] bias: [14.71397571] loss: 73.74597155535254\nEpoch: 242 / 1000\nw1: [10.3091674] w2: [-1.07442471] bias: [14.60102647] loss: 74.43184936891835\nEpoch: 243 / 1000\nw1: [10.35362445] w2: [-0.97819915] bias: [14.72637565] loss: 73.6007375967357\nEpoch: 244 / 1000\nw1: [10.43789591] w2: [-0.96645452] bias: [14.8696838] loss: 72.52345664703681\nEpoch: 245 / 1000\nw1: [10.41208609] w2: [-1.04278724] bias: [14.76931016] loss: 73.0903070898798\nEpoch: 246 / 1000\nw1: [10.54184781] w2: [-0.99554806] bias: [14.9491366] loss: 71.71121318394816\nEpoch: 247 / 1000\nw1: [10.73840929] w2: [-0.95007037] bias: [15.22110224] loss: 69.85837981642547\nEpoch: 248 / 1000\nw1: [10.70037322] w2: [-0.97097326] bias: [15.13070595] loss: 70.33924533722461\nEpoch: 249 / 1000\nw1: [10.57176339] w2: [-1.16214198] bias: [14.85403046] loss: 71.93364298564404\nEpoch: 250 / 1000\nw1: [10.55230825] w2: [-1.17891458] bias: [14.81572931] loss: 72.19312007802237\nEpoch: 251 / 1000\nw1: [10.61471327] w2: [-1.16516495] bias: [14.91889428] loss: 71.4447178343668\nEpoch: 252 / 1000\nw1: [10.58160266] w2: [-1.19456595] bias: [14.84576509] loss: 71.9134546715533\nEpoch: 253 / 1000\nw1: [10.52848884] w2: [-1.27218773] bias: [14.73786277] loss: 72.63752768259603\nEpoch: 254 / 1000\nw1: [10.45583342] w2: [-1.32940505] bias: [14.59690046] loss: 73.69556382477602\nEpoch: 255 / 1000\nw1: [10.46377532] w2: [-1.32458911] bias: [14.61325751] loss: 73.57001161308447\nEpoch: 256 / 1000\nw1: [10.57769573] w2: [-1.27694044] bias: [14.78715357] loss: 72.1738764142731\nEpoch: 257 / 1000\nw1: [10.7820884] w2: [-1.24844382] bias: [15.09915279] loss: 69.92530409730077\nEpoch: 258 / 1000\nw1: [10.77997345] w2: [-1.24944669] bias: [15.09560705] loss: 69.94744529550121\nEpoch: 259 / 1000\nw1: [10.72028841] w2: [-1.30347262] bias: [14.96168777] loss: 70.73088811893703\nEpoch: 260 / 1000\nw1: [10.83306048] w2: [-1.28960874] bias: [15.15567505] loss: 69.46464231040878\nEpoch: 261 / 1000\nw1: [10.85519342] w2: [-1.2721601] bias: [15.20570183] loss: 69.19698579863665\nEpoch: 262 / 1000\nw1: [10.97146894] w2: [-1.26079517] bias: [15.40859115] loss: 68.05201155310515\nEpoch: 263 / 1000\nw1: [10.97801069] w2: [-1.25708084] bias: [15.42164712] loss: 67.98959987569853\nEpoch: 264 / 1000\nw1: [10.98893222] w2: [-1.24935717] bias: [15.4464835] loss: 67.87805279128332\nEpoch: 265 / 1000\nw1: [11.01002948] w2: [-1.23920801] bias: [15.48565341] loss: 67.69234867882331\nEpoch: 266 / 1000\nw1: [11.01166707] w2: [-1.23842708] bias: [15.48896732] loss: 67.67724952907956\nEpoch: 267 / 1000\nw1: [10.98297737] w2: [-1.32327709] bias: [15.37739387] loss: 68.05764513209084\nEpoch: 268 / 1000\nw1: [10.96501503] w2: [-1.34996785] bias: [15.33654619] loss: 68.23745183501798\nEpoch: 269 / 1000\nw1: [10.95808268] w2: [-1.35372909] bias: [15.32323495] loss: 68.30628206835628\nEpoch: 270 / 1000\nw1: [11.20410327] w2: [-1.32376762] bias: [15.70288648] loss: 66.36247397360499\nEpoch: 271 / 1000\nw1: [11.20743222] w2: [-1.32134518] bias: [15.71025137] loss: 66.33650244051701\nEpoch: 272 / 1000\nw1: [11.20075605] w2: [-1.32412635] bias: [15.69980684] loss: 66.37959792829483\nEpoch: 273 / 1000\nw1: [11.14644958] w2: [-1.4358968] bias: [15.537199] loss: 66.87633833153836\nEpoch: 274 / 1000\nw1: [11.17773599] w2: [-1.42681594] bias: [15.59204743] loss: 66.62384477852737\nEpoch: 275 / 1000\nw1: [11.17701637] w2: [-1.42709722] bias: [15.59054395] loss: 66.63018241141029\nEpoch: 276 / 1000\nw1: [11.21984001] w2: [-1.41744926] bias: [15.67624049] loss: 66.26778075706851\nEpoch: 277 / 1000\nw1: [11.24164843] w2: [-1.40387573] bias: [15.72587772] loss: 66.08816347500469\nEpoch: 278 / 1000\nw1: [11.2529363] w2: [-1.39155422] bias: [15.74850983] loss: 66.01404645638037\nEpoch: 279 / 1000\nw1: [11.23645718] w2: [-1.40413261] bias: [15.70618477] loss: 66.15597617230367\nEpoch: 280 / 1000\nw1: [11.15061031] w2: [-1.47173841] bias: [15.53962908] loss: 66.81247273456344\nEpoch: 281 / 1000\nw1: [11.18386922] w2: [-1.45438172] bias: [15.60837294] loss: 66.52124194773498\nEpoch: 282 / 1000\nw1: [11.10117719] w2: [-1.51823047] bias: [15.47210211] loss: 67.13204937704677\nEpoch: 283 / 1000\nw1: [11.23611655] w2: [-1.50376739] bias: [15.68779844] loss: 66.07520862372789\nEpoch: 284 / 1000\nw1: [11.41519762] w2: [-1.46727563] bias: [15.94211792] loss: 65.02862133175533\nEpoch: 285 / 1000\nw1: [11.35062598] w2: [-1.52504062] bias: [15.81368829] loss: 65.4022818945843\nEpoch: 286 / 1000\nw1: [11.24833591] w2: [-1.53508898] bias: [15.71139822] loss: 65.93284090808827\nEpoch: 287 / 1000\nw1: [11.26873874] w2: [-1.52908597] bias: [15.75161058] loss: 65.77404427928728\nEpoch: 288 / 1000\nw1: [11.24865944] w2: [-1.56847965] bias: [15.70098552] loss: 65.91739724293708\nEpoch: 289 / 1000\nw1: [11.24989928] w2: [-1.56802162] bias: [15.70312178] loss: 65.90846202869268\nEpoch: 290 / 1000\nw1: [11.37369225] w2: [-1.53708727] bias: [15.88630168] loss: 65.1501685239837\nEpoch: 291 / 1000\nw1: [11.37925869] w2: [-1.53365734] bias: [15.89733101] loss: 65.11571953317303\nEpoch: 292 / 1000\nw1: [11.34933079] w2: [-1.54761316] bias: [15.83933113] loss: 65.3094240559476\nEpoch: 293 / 1000\nw1: [11.57673298] w2: [-1.52959703] bias: [16.10259911] loss: 64.25515004463969\nEpoch: 294 / 1000\nw1: [11.61007024] w2: [-1.51282857] bias: [16.17494304] loss: 64.09782811601043\nEpoch: 295 / 1000\nw1: [11.5178609] w2: [-1.58544492] bias: [15.9960432] loss: 64.49307751118972\nEpoch: 296 / 1000\nw1: [11.50041968] w2: [-1.5941952] bias: [15.95904088] loss: 64.5961212434766\nEpoch: 297 / 1000\nw1: [11.59380784] w2: [-1.58166873] bias: [16.1124055] loss: 64.11563486637021\nEpoch: 298 / 1000\nw1: [11.60539844] w2: [-1.57253121] bias: [16.13860357] loss: 64.06295047415117\nEpoch: 299 / 1000\nw1: [11.58384595] w2: [-1.58987474] bias: [16.08327527] loss: 64.17394145695289\nEpoch: 300 / 1000\nw1: [11.92461325] w2: [-1.56382057] bias: [16.5144182] loss: 63.097630903129165\nEpoch: 301 / 1000\nw1: [11.90445352] w2: [-1.57671555] bias: [16.46904793] loss: 63.13016471468152\nEpoch: 302 / 1000\nw1: [11.93115925] w2: [-1.55684813] bias: [16.52473199] loss: 63.096404724995516\nEpoch: 303 / 1000\nw1: [11.82332156] w2: [-1.67361768] bias: [16.31090034] loss: 63.224200183734055\nEpoch: 304 / 1000\nw1: [11.83386861] w2: [-1.6669474] bias: [16.33003978] loss: 63.19659612321867\nEpoch: 305 / 1000\nw1: [11.83467876] w2: [-1.66570923] bias: [16.33235154] loss: 63.19498109547876\nEpoch: 306 / 1000\nw1: [12.16844729] w2: [-1.64019017] bias: [16.75463953] loss: 62.56809719268725\nEpoch: 307 / 1000\nw1: [12.25353113] w2: [-1.60921577] bias: [16.87255046] loss: 62.58658721597004\nEpoch: 308 / 1000\nw1: [12.31849208] w2: [-1.59793316] bias: [16.98612875] loss: 62.62944549802597\nEpoch: 309 / 1000\nw1: [12.22408863] w2: [-1.72069394] bias: [16.76783383] loss: 62.33076123667077\nEpoch: 310 / 1000\nw1: [12.21742535] w2: [-1.72532891] bias: [16.75425491] loss: 62.32579132674289\nEpoch: 311 / 1000\nw1: [12.63855466] w2: [-1.68535305] bias: [17.1816074] loss: 62.43642527388603\nEpoch: 312 / 1000\nw1: [12.7299481] w2: [-1.66251495] bias: [17.31684481] loss: 62.69046142036933\nEpoch: 313 / 1000\nw1: [12.68779716] w2: [-1.68563238] bias: [17.23728396] loss: 62.50280943755863\nEpoch: 314 / 1000\nw1: [12.63280933] w2: [-1.72910933] bias: [17.13382994] loss: 62.26022652563523\nEpoch: 315 / 1000\nw1: [12.61348415] w2: [-1.73165338] bias: [17.10101986] loss: 62.221300757984054\nEpoch: 316 / 1000\nw1: [12.54252193] w2: [-1.83076087] bias: [16.9289233] loss: 61.85311474719179\nEpoch: 317 / 1000\nw1: [12.52265732] w2: [-1.85286478] bias: [16.85550023] loss: 61.76788768462233\nEpoch: 318 / 1000\nw1: [12.42986001] w2: [-1.97353695] bias: [16.64091929] loss: 61.53488516520929\nEpoch: 319 / 1000\nw1: [12.3151482] w2: [-2.05354052] bias: [16.41423207] loss: 61.600839400123576\nEpoch: 320 / 1000\nw1: [12.32999045] w2: [-2.04923257] bias: [16.44025213] loss: 61.571468639569545\nEpoch: 321 / 1000\nw1: [12.30355959] w2: [-2.06363283] bias: [16.38115072] loss: 61.621514494676624\nEpoch: 322 / 1000\nw1: [12.30394889] w2: [-2.06343427] bias: [16.38198069] loss: 61.62069350255773\nEpoch: 323 / 1000\nw1: [12.44154163] w2: [-2.06200378] bias: [16.58934324] loss: 61.33955664319721\nEpoch: 324 / 1000\nw1: [12.43234226] w2: [-2.0674975] bias: [16.56886052] loss: 61.343661279452284\nEpoch: 325 / 1000\nw1: [12.44451765] w2: [-2.06443395] bias: [16.59110958] loss: 61.330501713266536\nEpoch: 326 / 1000\nw1: [12.42774834] w2: [-2.07024744] bias: [16.55581967] loss: 61.346882637499675\nEpoch: 327 / 1000\nw1: [12.60627744] w2: [-2.04814254] bias: [16.81506936] loss: 61.22544702215777\nEpoch: 328 / 1000\nw1: [12.71468066] w2: [-2.02029498] bias: [16.97475222] loss: 61.31441558508171\nEpoch: 329 / 1000\nw1: [12.60336157] w2: [-2.10169304] bias: [16.75526816] loss: 61.08482237038593\nEpoch: 330 / 1000\nw1: [12.5735015] w2: [-2.12401102] bias: [16.69422636] loss: 61.052946837974005\nEpoch: 331 / 1000\nw1: [12.56405805] w2: [-2.13319084] bias: [16.67369078] loss: 61.04111128854795\nEpoch: 332 / 1000\nw1: [12.51087312] w2: [-2.1887375] bias: [16.53749248] loss: 61.00124777889707\nEpoch: 333 / 1000\nw1: [12.50331377] w2: [-2.19516286] bias: [16.52042098] loss: 61.00314813641263\nEpoch: 334 / 1000\nw1: [12.55905209] w2: [-2.15438087] bias: [16.66253055] loss: 61.000919141509456\nEpoch: 335 / 1000\nw1: [12.69033783] w2: [-2.14238156] bias: [16.85159795] loss: 60.955799442964356\nEpoch: 336 / 1000\nw1: [12.65113698] w2: [-2.16280809] bias: [16.78136477] loss: 60.91186321980128\nEpoch: 337 / 1000\nw1: [12.95327552] w2: [-2.14052053] bias: [17.11651059] loss: 61.0484305934263\nEpoch: 338 / 1000\nw1: [12.95327552] w2: [-2.10868666] bias: [17.3305476] loss: 61.477133314138136\nEpoch: 339 / 1000\nw1: [12.93696793] w2: [-2.11209433] bias: [17.30149013] loss: 61.419615525128144\nEpoch: 340 / 1000\nw1: [12.9452654] w2: [-2.10810113] bias: [17.3171518] loss: 61.456033349892635\nEpoch: 341 / 1000\nw1: [12.91106663] w2: [-2.17519585] bias: [17.23092794] loss: 61.14727952978081\nEpoch: 342 / 1000\nw1: [12.92740414] w2: [-2.17229838] bias: [17.25616934] loss: 61.18928575799633\nEpoch: 343 / 1000\nw1: [12.89451274] w2: [-2.28756969] bias: [17.09882727] loss: 60.691462462112256\nEpoch: 344 / 1000\nw1: [12.81360911] w2: [-2.48501382] bias: [16.85276897] loss: 60.07833734711983\nEpoch: 345 / 1000\nw1: [12.73004296] w2: [-2.55547597] bias: [16.70361311] loss: 59.96686260685599\nEpoch: 346 / 1000\nw1: [12.68969868] w2: [-2.59741413] bias: [16.60174722] loss: 59.9345108703154\nEpoch: 347 / 1000\nw1: [12.68836962] w2: [-2.59972485] bias: [16.5979836] loss: 59.93229548594476\nEpoch: 348 / 1000\nw1: [12.64493714] w2: [-2.65240267] bias: [16.46550309] loss: 59.937506433419365\nEpoch: 349 / 1000\nw1: [12.62852243] w2: [-2.70054692] bias: [16.40274238] loss: 59.91141164975391\nEpoch: 350 / 1000\nw1: [12.61545223] w2: [-2.7061283] bias: [16.37273211] loss: 59.94404241892823\nEpoch: 351 / 1000\nw1: [13.00013453] w2: [-2.67083906] bias: [16.84290937] loss: 59.496957176420345\nEpoch: 352 / 1000\nw1: [12.99218003] w2: [-2.67637221] bias: [16.82669908] loss: 59.4826475731685\nEpoch: 353 / 1000\nw1: [12.985388] w2: [-2.68250213] bias: [16.80746546] loss: 59.46547583087211\nEpoch: 354 / 1000\nw1: [12.96142656] w2: [-2.70475658] bias: [16.75516318] loss: 59.41834330063232\nEpoch: 355 / 1000\nw1: [13.11742158] w2: [-2.66549645] bias: [16.97852065] loss: 59.52610583636919\nEpoch: 356 / 1000\nw1: [13.05554378] w2: [-2.75672148] bias: [16.82220882] loss: 59.24807664293204\nEpoch: 357 / 1000\nw1: [13.02585079] w2: [-2.76728201] bias: [16.75946886] loss: 59.22336052464057\nEpoch: 358 / 1000\nw1: [12.99979477] w2: [-2.79088992] bias: [16.70380519] loss: 59.18234731348399\nEpoch: 359 / 1000\nw1: [13.13755031] w2: [-2.774322] bias: [16.8987472] loss: 59.19265451279911\nEpoch: 360 / 1000\nw1: [13.08707065] w2: [-2.81146637] bias: [16.79191292] loss: 59.08507237369956\nEpoch: 361 / 1000\nw1: [13.04974512] w2: [-2.84096796] bias: [16.70772885] loss: 59.02673076774961\nEpoch: 362 / 1000\nw1: [13.14578993] w2: [-2.83318038] bias: [16.86451892] loss: 59.02952328244924\nEpoch: 363 / 1000\nw1: [13.07693773] w2: [-2.87941817] bias: [16.73843484] loss: 58.927320442821966\nEpoch: 364 / 1000\nw1: [13.07995429] w2: [-2.8777675] bias: [16.74457023] loss: 58.92991022863097\nEpoch: 365 / 1000\nw1: [13.08127955] w2: [-2.87751726] bias: [16.74665983] loss: 58.92987104677379\nEpoch: 366 / 1000\nw1: [13.08585181] w2: [-2.87453085] bias: [16.75638378] loss: 58.93531584928985\nEpoch: 367 / 1000\nw1: [13.09332603] w2: [-2.87236146] bias: [16.76948689] loss: 58.93798106210481\nEpoch: 368 / 1000\nw1: [13.09464655] w2: [-2.87112958] bias: [16.77235969] loss: 58.94056883297017\nEpoch: 369 / 1000\nw1: [13.16262249] w2: [-2.86256737] bias: [16.88437939] loss: 58.96457103874446\nEpoch: 370 / 1000\nw1: [13.1630555] w2: [-2.86219289] bias: [16.88535014] loss: 58.9657685545133\nEpoch: 371 / 1000\nw1: [13.28604598] w2: [-2.85095175] bias: [17.06247136] loss: 59.08302443905117\nEpoch: 372 / 1000\nw1: [13.32992903] w2: [-2.81629042] bias: [17.13206286] loss: 59.23759984686131\nEpoch: 373 / 1000\nw1: [13.19977355] w2: [-2.90865798] bias: [16.91898838] loss: 58.85930788285936\nEpoch: 374 / 1000\nw1: [13.00679266] w2: [-3.11625365] bias: [16.60630062] loss: 58.511145706778116\nEpoch: 375 / 1000\nw1: [13.0220204] w2: [-3.11311416] bias: [16.63446277] loss: 58.49557638814171\nEpoch: 376 / 1000\nw1: [13.28713212] w2: [-3.09958027] bias: [16.98987455] loss: 58.4271187441664\nEpoch: 377 / 1000\nw1: [13.27342333] w2: [-3.10776693] bias: [16.95935144] loss: 58.39128801083468\nEpoch: 378 / 1000\nw1: [13.32739938] w2: [-3.10099763] bias: [17.05192489] loss: 58.460886141739174\nEpoch: 379 / 1000\nw1: [13.3243104] w2: [-3.10167094] bias: [17.04548664] loss: 58.45421207158259\nEpoch: 380 / 1000\nw1: [13.30327963] w2: [-3.11625963] bias: [17.00271204] loss: 58.39058703784542\nEpoch: 381 / 1000\nw1: [13.31582415] w2: [-3.1039039] bias: [17.02900516] loss: 58.43692539112935\nEpoch: 382 / 1000\nw1: [13.31542239] w2: [-3.10412374] bias: [17.02818801] loss: 58.435829926142134\nEpoch: 383 / 1000\nw1: [13.46513086] w2: [-3.06644578] bias: [17.24254426] loss: 58.74264879570647\nEpoch: 384 / 1000\nw1: [13.593281] w2: [-3.01128745] bias: [17.50147403] loss: 59.38146142235692\nEpoch: 385 / 1000\nw1: [13.50368196] w2: [-3.0795716] bias: [17.33081074] loss: 58.85351299956324\nEpoch: 386 / 1000\nw1: [13.47570943] w2: [-3.09653406] bias: [17.27319881] loss: 58.71553783505386\nEpoch: 387 / 1000\nw1: [13.48857778] w2: [-3.0934011] bias: [17.29854217] loss: 58.763739396054746\nEpoch: 388 / 1000\nw1: [13.46688103] w2: [-3.12937119] bias: [17.22750369] loss: 58.562724347467025\nEpoch: 389 / 1000\nw1: [13.47423638] w2: [-3.12667646] bias: [17.2400078] loss: 58.58680972538571\nEpoch: 390 / 1000\nw1: [13.46219732] w2: [-3.13000408] bias: [17.21870164] loss: 58.54905942094798\nEpoch: 391 / 1000\nw1: [13.42634028] w2: [-3.15675236] bias: [17.14855999] loss: 58.396938707008296\nEpoch: 392 / 1000\nw1: [13.72874452] w2: [-3.13363133] bias: [17.5311655] loss: 59.13686162982195\nEpoch: 393 / 1000\nw1: [13.78194292] w2: [-3.12639218] bias: [17.62386749] loss: 59.40556897236252\nEpoch: 394 / 1000\nw1: [13.79183556] w2: [-3.11721088] bias: [17.64954114] loss: 59.5039312392563\nEpoch: 395 / 1000\nw1: [13.8240157] w2: [-3.11320245] bias: [17.69780209] loss: 59.66246619119166\nEpoch: 396 / 1000\nw1: [13.74152014] w2: [-3.31453166] bias: [17.44690213] loss: 58.461321864252\nEpoch: 397 / 1000\nw1: [13.73343796] w2: [-3.32143049] bias: [17.42669087] loss: 58.399876934159536\nEpoch: 398 / 1000\nw1: [13.74086493] w2: [-3.3175546] bias: [17.44204192] loss: 58.44286723608774\nEpoch: 399 / 1000\nw1: [13.61613159] w2: [-3.4045475] bias: [17.19555069] loss: 57.79537934754374\nEpoch: 400 / 1000\nw1: [13.6211231] w2: [-3.40368246] bias: [17.20438144] loss: 57.80901392683978\nEpoch: 401 / 1000\nw1: [13.52943429] w2: [-3.46545672] bias: [17.0519853] loss: 57.50798376626412\nEpoch: 402 / 1000\nw1: [13.50343057] w2: [-3.48291642] bias: [16.98877458] loss: 57.42170552328757\nEpoch: 403 / 1000\nw1: [13.49019737] w2: [-3.49355431] bias: [16.95773455] loss: 57.380034216758176\nEpoch: 404 / 1000\nw1: [13.73964762] w2: [-3.44337523] bias: [17.28657537] loss: 57.816095599328285\nEpoch: 405 / 1000\nw1: [13.70928823] w2: [-3.46115899] bias: [17.22577504] loss: 57.67640645829476\nEpoch: 406 / 1000\nw1: [13.72688199] w2: [-3.44807033] bias: [17.26245975] loss: 57.76506094386091\nEpoch: 407 / 1000\nw1: [13.71354315] w2: [-3.4517572] bias: [17.23885331] loss: 57.71955529917466\nEpoch: 408 / 1000\nw1: [13.74853191] w2: [-3.446071] bias: [17.2907596] loss: 57.81522787582424\nEpoch: 409 / 1000\nw1: [13.83684454] w2: [-3.42933667] bias: [17.44429179] loss: 58.15799678440327\nEpoch: 410 / 1000\nw1: [13.84452687] w2: [-3.4279742] bias: [17.45616097] loss: 58.18822217708577\nEpoch: 411 / 1000\nw1: [14.11934114] w2: [-3.40911051] bias: [17.79291976] loss: 59.241937601882114\nEpoch: 412 / 1000\nw1: [14.00628633] w2: [-3.54458911] bias: [17.52300526] loss: 58.05561324204488\nEpoch: 413 / 1000\nw1: [13.89145725] w2: [-3.63325174] bias: [17.33377477] loss: 57.40662346464791\nEpoch: 414 / 1000\nw1: [13.88563015] w2: [-3.63961244] bias: [17.32209146] loss: 57.36919063591377\nEpoch: 415 / 1000\nw1: [13.86064753] w2: [-3.64922863] bias: [17.28075212] loss: 57.27376447413438\nEpoch: 416 / 1000\nw1: [14.13831224] w2: [-3.63016928] bias: [17.62100385] loss: 58.113286246018006\nEpoch: 417 / 1000\nw1: [14.12353943] w2: [-3.63739379] bias: [17.58887914] loss: 58.001904975386665\nEpoch: 418 / 1000\nw1: [14.12024131] w2: [-3.63900744] bias: [17.58263808] loss: 57.97997960048087\nEpoch: 419 / 1000\nw1: [14.07982767] w2: [-3.6800669] bias: [17.49299984] loss: 57.6345954217112\nEpoch: 420 / 1000\nw1: [14.0303523] w2: [-3.74007393] bias: [17.34208694] loss: 57.14389203714871\nEpoch: 421 / 1000\nw1: [14.02586965] w2: [-3.74425565] bias: [17.33233498] loss: 57.114104580430606\nEpoch: 422 / 1000\nw1: [14.00200998] w2: [-3.86730742] bias: [17.17803061] loss: 56.54172775566589\nEpoch: 423 / 1000\nw1: [14.12456223] w2: [-3.85249199] bias: [17.36576181] loss: 56.905866003576975\nEpoch: 424 / 1000\nw1: [14.09232414] w2: [-3.88170114] bias: [17.29689132] loss: 56.696783955904735\nEpoch: 425 / 1000\nw1: [14.09152657] w2: [-3.88193579] bias: [17.29513794] loss: 56.692954621628914\nEpoch: 426 / 1000\nw1: [14.14009835] w2: [-3.84639733] bias: [17.41897583] loss: 57.03643078119131\nEpoch: 427 / 1000\nw1: [14.13479596] w2: [-3.85561615] bias: [17.40396054] loss: 56.97949255922541\nEpoch: 428 / 1000\nw1: [14.14565234] w2: [-3.85277988] bias: [17.42511001] loss: 57.03400627744588\nEpoch: 429 / 1000\nw1: [14.26373943] w2: [-3.83850425] bias: [17.60600128] loss: 57.54420758753034\nEpoch: 430 / 1000\nw1: [14.25445936] w2: [-3.8486341] bias: [17.58739478] loss: 57.463503940496004\nEpoch: 431 / 1000\nw1: [14.2162019] w2: [-3.87415412] bias: [17.51316963] loss: 57.194746507772784\nEpoch: 432 / 1000\nw1: [14.1996802] w2: [-3.87849861] bias: [17.48253846] loss: 57.106011405919006\nEpoch: 433 / 1000\nw1: [14.18741876] w2: [-3.88390053] bias: [17.45985414] loss: 57.03672605346526\nEpoch: 434 / 1000\nw1: [14.12403898] w2: [-3.91873117] bias: [17.30922615] loss: 56.625785342181466\nEpoch: 435 / 1000\nw1: [13.97931569] w2: [-4.13385131] bias: [16.99788614] loss: 55.704517979711405\nEpoch: 436 / 1000\nw1: [13.78946272] w2: [-4.33808219] bias: [16.69026656] loss: 55.26098311052681\nEpoch: 437 / 1000\nw1: [13.76631001] w2: [-4.35769556] bias: [16.63682383] loss: 55.24751579560729\nEpoch: 438 / 1000\nw1: [13.71408599] w2: [-4.38607193] bias: [16.54392774] loss: 55.2735089156164\nEpoch: 439 / 1000\nw1: [13.81756824] w2: [-4.37498049] bias: [16.70934086] loss: 55.16385195834392\nEpoch: 440 / 1000\nw1: [13.77560296] w2: [-4.42812838] bias: [16.59196854] loss: 55.11190032514732\nEpoch: 441 / 1000\nw1: [13.76900995] w2: [-4.43134988] bias: [16.57966644] loss: 55.11611769799856\nEpoch: 442 / 1000\nw1: [13.71802278] w2: [-4.46505098] bias: [16.48513642] loss: 55.15209349045325\nEpoch: 443 / 1000\nw1: [13.56073199] w2: [-4.48050228] bias: [16.32784563] loss: 55.46688990871612\nEpoch: 444 / 1000\nw1: [13.54659753] w2: [-4.49442084] bias: [16.29602175] loss: 55.49943268725885\nEpoch: 445 / 1000\nw1: [13.49173834] w2: [-4.55577807] bias: [16.16223197] loss: 55.66512793869313\nEpoch: 446 / 1000\nw1: [13.48409884] w2: [-4.55828094] bias: [16.14821279] loss: 55.69542190373816\nEpoch: 447 / 1000\nw1: [13.41174253] w2: [-4.60703016] bias: [16.02794923] loss: 55.96141542029517\nEpoch: 448 / 1000\nw1: [13.424484] w2: [-4.60233198] bias: [16.0512409] loss: 55.90132048351918\nEpoch: 449 / 1000\nw1: [13.4275442] w2: [-4.59935721] bias: [16.05789557] loss: 55.887879451945075\nEpoch: 450 / 1000\nw1: [13.49754075] w2: [-4.5872] bias: [16.18027814] loss: 55.58374351348438\nEpoch: 451 / 1000\nw1: [13.49845367] w2: [-4.58690657] bias: [16.18193192] loss: 55.580244317580004\nEpoch: 452 / 1000\nw1: [13.50205171] w2: [-4.58600578] bias: [16.18792177] loss: 55.56689361738797\nEpoch: 453 / 1000\nw1: [13.43195163] w2: [-4.68786178] bias: [15.98330638] loss: 55.89621102327983\nEpoch: 454 / 1000\nw1: [13.30152841] w2: [-4.79356208] bias: [15.74858955] loss: 56.589526154094024\nEpoch: 455 / 1000\nw1: [13.27867346] w2: [-4.82544135] bias: [15.69000411] loss: 56.76754204611904\nEpoch: 456 / 1000\nw1: [13.2701457] w2: [-4.83300211] bias: [15.67108134] loss: 56.83349688777463\nEpoch: 457 / 1000\nw1: [13.58496185] w2: [-4.81879256] bias: [16.07338923] loss: 55.25642346291317\nEpoch: 458 / 1000\nw1: [13.51846536] w2: [-4.85323247] bias: [15.95221424] loss: 55.564886966763424\nEpoch: 459 / 1000\nw1: [13.55204208] w2: [-4.83634356] bias: [16.02507782] loss: 55.38055974442645\nEpoch: 460 / 1000\nw1: [13.65389487] w2: [-4.82808505] bias: [16.19134923] loss: 54.93981559953993\nEpoch: 461 / 1000\nw1: [13.66780851] w2: [-4.82404662] bias: [16.21574134] loss: 54.88939469042478\nEpoch: 462 / 1000\nw1: [13.65340262] w2: [-4.82904077] bias: [16.18542506] loss: 54.9472979059042\nEpoch: 463 / 1000\nw1: [13.67222796] w2: [-4.82180348] bias: [16.22371402] loss: 54.87496385377538\nEpoch: 464 / 1000\nw1: [13.65034107] w2: [-4.83078473] bias: [16.17949115] loss: 54.95832401502084\nEpoch: 465 / 1000\nw1: [13.65217014] w2: [-4.83027917] bias: [16.18272815] loss: 54.9513685094531\nEpoch: 466 / 1000\nw1: [13.68779349] w2: [-4.82225541] bias: [16.24330758] loss: 54.823677838711546\nEpoch: 467 / 1000\nw1: [13.64261994] w2: [-4.87468682] bias: [16.13861808] loss: 54.96850036799299\nEpoch: 468 / 1000\nw1: [13.60680112] w2: [-4.9200504] bias: [16.03843672] loss: 55.137704181276256\nEpoch: 469 / 1000\nw1: [13.67605996] w2: [-4.90140656] bias: [16.16218265] loss: 54.83560611776656\nEpoch: 470 / 1000\nw1: [13.68217028] w2: [-4.90045856] bias: [16.17386817] loss: 54.80952752937374\nEpoch: 471 / 1000\nw1: [13.62702503] w2: [-4.9501334] bias: [16.05809863] loss: 55.02408089352527\nEpoch: 472 / 1000\nw1: [13.69067759] w2: [-4.94215053] bias: [16.16726811] loss: 54.744383656298105\nEpoch: 473 / 1000\nw1: [14.05239455] w2: [-4.91747204] bias: [16.57012786] loss: 53.9152483840172\nEpoch: 474 / 1000\nw1: [14.06535225] w2: [-4.91338262] bias: [16.59612796] loss: 53.90271186970887\nEpoch: 475 / 1000\nw1: [14.02394407] w2: [-4.93575779] bias: [16.52153034] loss: 53.931778872469195\nEpoch: 476 / 1000\nw1: [14.02500995] w2: [-4.93533578] bias: [16.52393121] loss: 53.93032851192446\nEpoch: 477 / 1000\nw1: [14.1107835] w2: [-4.93064688] bias: [16.65987174] loss: 53.81752293545517\nEpoch: 478 / 1000\nw1: [14.13114884] w2: [-4.91746422] bias: [16.70576412] loss: 53.82453562763729\nEpoch: 479 / 1000\nw1: [14.0665017] w2: [-4.9608781] bias: [16.58738045] loss: 53.81863468554155\nEpoch: 480 / 1000\nw1: [14.36885335] w2: [-4.93298424] bias: [17.09281711] loss: 53.87102601490925\nEpoch: 481 / 1000\nw1: [14.3649353] w2: [-4.93442895] bias: [17.08565484] loss: 53.861532073659276\nEpoch: 482 / 1000\nw1: [14.41157468] w2: [-4.92857976] bias: [17.16564528] loss: 53.95431238284741\nEpoch: 483 / 1000\nw1: [14.39997688] w2: [-4.93618343] bias: [17.14102] loss: 53.90953877915644\nEpoch: 484 / 1000\nw1: [14.39147233] w2: [-4.93767978] bias: [17.12534724] loss: 53.889936776292174\nEpoch: 485 / 1000\nw1: [14.36890157] w2: [-4.95353965] bias: [17.07630613] loss: 53.808175258409754\nEpoch: 486 / 1000\nw1: [14.22187535] w2: [-5.0726957] bias: [16.81170962] loss: 53.47254862333306\nEpoch: 487 / 1000\nw1: [14.1173301] w2: [-5.16607968] bias: [16.61985953] loss: 53.3930865170666\nEpoch: 488 / 1000\nw1: [14.04340524] w2: [-5.30469485] bias: [16.42401494] loss: 53.352685979359826\nEpoch: 489 / 1000\nw1: [14.0321563] w2: [-5.30985667] bias: [16.3986331] loss: 53.38100466263248\nEpoch: 490 / 1000\nw1: [13.97439145] w2: [-5.35252644] bias: [16.2822785] loss: 53.520796807717495\nEpoch: 491 / 1000\nw1: [13.88513597] w2: [-5.42144301] bias: [16.13519156] loss: 53.78054076195699\nEpoch: 492 / 1000\nw1: [13.96445728] w2: [-5.38826585] bias: [16.25627314] loss: 53.51653283734017\nEpoch: 493 / 1000\nw1: [13.95460519] w2: [-5.39149362] bias: [16.23819365] loss: 53.551209032236954\nEpoch: 494 / 1000\nw1: [13.91702529] w2: [-5.40901768] bias: [16.16536428] loss: 53.69284464482578\nEpoch: 495 / 1000\nw1: [13.89759878] w2: [-5.42220597] bias: [16.12394805] loss: 53.77404275931554\nEpoch: 496 / 1000\nw1: [13.73167358] w2: [-5.5760641] bias: [15.85855925] loss: 54.45914331441239\nEpoch: 497 / 1000\nw1: [13.87761668] w2: [-5.55138244] bias: [16.04298469] loss: 53.78633537938325\nEpoch: 498 / 1000\nw1: [13.8635018] w2: [-5.564917] bias: [16.00629864] loss: 53.8677279963758\nEpoch: 499 / 1000\nw1: [13.85265519] w2: [-5.56962637] bias: [15.98413402] loss: 53.928079398730816\nEpoch: 500 / 1000\nw1: [13.80190815] w2: [-5.63198152] bias: [15.86878176] loss: 54.2158317068535\nEpoch: 501 / 1000\nw1: [13.79577111] w2: [-5.63500209] bias: [15.85549712] loss: 54.2574272476234\nEpoch: 502 / 1000\nw1: [13.74668702] w2: [-5.6953139] bias: [15.74392489] loss: 54.59043906333144\nEpoch: 503 / 1000\nw1: [13.78137895] w2: [-5.67483419] bias: [15.81874191] loss: 54.336892280181566\nEpoch: 504 / 1000\nw1: [13.79966085] w2: [-5.66289318] bias: [15.85762261] loss: 54.213710295867386\nEpoch: 505 / 1000\nw1: [13.81810746] w2: [-5.65979234] bias: [15.89116716] loss: 54.09795921625965\nEpoch: 506 / 1000\nw1: [13.82170101] w2: [-5.65719133] bias: [15.8992861] loss: 54.074493951541974\nEpoch: 507 / 1000\nw1: [13.8106284] w2: [-5.66276687] bias: [15.87424798] loss: 54.15023216870894\nEpoch: 508 / 1000\nw1: [13.8304628] w2: [-5.65491384] bias: [15.91672469] loss: 54.01890666235246\nEpoch: 509 / 1000\nw1: [13.82654709] w2: [-5.6576193] bias: [15.908287] loss: 54.04318412421367\nEpoch: 510 / 1000\nw1: [13.89712997] w2: [-5.63861904] bias: [16.03439862] loss: 53.65666218560112\nEpoch: 511 / 1000\nw1: [13.93797504] w2: [-5.62470548] bias: [16.10705384] loss: 53.46328534882465\nEpoch: 512 / 1000\nw1: [14.0804427] w2: [-5.56338462] bias: [16.39491242] loss: 52.925490219694986\nEpoch: 513 / 1000\nw1: [14.26224117] w2: [-5.54898148] bias: [16.60538408] loss: 52.57159651161666\nEpoch: 514 / 1000\nw1: [14.2582648] w2: [-5.54959896] bias: [16.59849865] loss: 52.57751974053225\nEpoch: 515 / 1000\nw1: [14.25311438] w2: [-5.5508884] bias: [16.58992447] loss: 52.5845022018693\nEpoch: 516 / 1000\nw1: [14.18177991] w2: [-5.61545921] bias: [16.42986661] loss: 52.66695168175587\nEpoch: 517 / 1000\nw1: [14.16505284] w2: [-5.63061467] bias: [16.39413244] loss: 52.698295223193995\nEpoch: 518 / 1000\nw1: [14.18084209] w2: [-5.62454459] bias: [16.42624627] loss: 52.65726178101862\nEpoch: 519 / 1000\nw1: [14.22327558] w2: [-5.62158171] bias: [16.49335547] loss: 52.55491527348946\nEpoch: 520 / 1000\nw1: [14.14450142] w2: [-5.75773942] bias: [16.27148699] loss: 52.67641074182616\nEpoch: 521 / 1000\nw1: [14.45060032] w2: [-5.72949985] bias: [16.78318787] loss: 52.0412483685635\nEpoch: 522 / 1000\nw1: [14.45705767] w2: [-5.71963104] bias: [16.80161372] loss: 52.05587129702766\nEpoch: 523 / 1000\nw1: [14.39518792] w2: [-5.77129865] bias: [16.69114562] loss: 52.024428327972885\nEpoch: 524 / 1000\nw1: [14.3884268] w2: [-5.77276752] bias: [16.68106383] loss: 52.030814282302195\nEpoch: 525 / 1000\nw1: [14.26535374] w2: [-5.9132896] bias: [16.45321058] loss: 52.076509754816406\nEpoch: 526 / 1000\nw1: [14.28601363] w2: [-5.9082597] bias: [16.49389888] loss: 52.02301601170755\nEpoch: 527 / 1000\nw1: [14.24846696] w2: [-5.93289968] bias: [16.40250145] loss: 52.118218265384385\nEpoch: 528 / 1000\nw1: [14.23517048] w2: [-5.93813586] bias: [16.37344186] loss: 52.16020211384444\nEpoch: 529 / 1000\nw1: [14.18467131] w2: [-6.00866418] bias: [16.25097198] loss: 52.29415625654926\nEpoch: 530 / 1000\nw1: [14.20121694] w2: [-5.99826203] bias: [16.28620317] loss: 52.23287822599816\nEpoch: 531 / 1000\nw1: [14.09539893] w2: [-6.0720628] bias: [16.0770914] loss: 52.662212370138015\nEpoch: 532 / 1000\nw1: [14.09474555] w2: [-6.07223708] bias: [16.07597154] loss: 52.6653868815408\nEpoch: 533 / 1000\nw1: [14.02333314] w2: [-6.16510066] bias: [15.91084022] loss: 53.05853336717394\nEpoch: 534 / 1000\nw1: [14.00209317] w2: [-6.18458919] bias: [15.86428357] loss: 53.19782721908642\nEpoch: 535 / 1000\nw1: [14.00669212] w2: [-6.18126047] bias: [15.87467401] loss: 53.16533574388024\nEpoch: 536 / 1000\nw1: [14.12195582] w2: [-6.16191096] bias: [16.0369949] loss: 52.58145981430605\nEpoch: 537 / 1000\nw1: [14.22562626] w2: [-6.14226648] bias: [16.21722676] loss: 52.096445940600844\nEpoch: 538 / 1000\nw1: [14.17065685] w2: [-6.17918127] bias: [16.11656523] loss: 52.31630770291695\nEpoch: 539 / 1000\nw1: [14.13010197] w2: [-6.19745817] bias: [16.04214333] loss: 52.51433711730867\nEpoch: 540 / 1000\nw1: [14.03791027] w2: [-6.28422621] bias: [15.87290361] loss: 52.99604235538584\nEpoch: 541 / 1000\nw1: [14.04439446] w2: [-6.28260286] bias: [15.88369818] loss: 52.95650623245707\nEpoch: 542 / 1000\nw1: [14.01558402] w2: [-6.29817076] bias: [15.83179563] loss: 53.13761598916529\nEpoch: 543 / 1000\nw1: [14.02274838] w2: [-6.29637711] bias: [15.84372253] loss: 53.09210813753943\nEpoch: 544 / 1000\nw1: [14.16100022] w2: [-6.27626939] bias: [16.04282195] loss: 52.36107609582071\nEpoch: 545 / 1000\nw1: [14.02878777] w2: [-6.33569389] bias: [15.85891557] loss: 53.00185054742073\nEpoch: 546 / 1000\nw1: [13.99250198] w2: [-6.39659246] bias: [15.76224609] loss: 53.28973942541889\nEpoch: 547 / 1000\nw1: [13.9961529] w2: [-6.39362629] bias: [15.77091494] loss: 53.2592735603485\nEpoch: 548 / 1000\nw1: [14.05911377] w2: [-6.38041976] bias: [15.87633451] loss: 52.845007824435065\nEpoch: 549 / 1000\nw1: [14.08713224] w2: [-6.36864903] bias: [15.93374651] loss: 52.6550174730156\nEpoch: 550 / 1000\nw1: [14.15135207] w2: [-6.34685239] bias: [16.04345626] loss: 52.30096786844478\nEpoch: 551 / 1000\nw1: [14.16782568] w2: [-6.33609249] bias: [16.07849121] loss: 52.20979770545153\nEpoch: 552 / 1000\nw1: [14.30586416] w2: [-6.31940491] bias: [16.28994493] loss: 51.62400842222529\nEpoch: 553 / 1000\nw1: [14.59621131] w2: [-6.29947502] bias: [16.64573779] loss: 50.93397320501417\nEpoch: 554 / 1000\nw1: [14.49971901] w2: [-6.40395947] bias: [16.45440294] loss: 51.01780671191591\nEpoch: 555 / 1000\nw1: [14.61741288] w2: [-6.40273587] bias: [16.63177641] loss: 50.75004595282839\nEpoch: 556 / 1000\nw1: [14.48104491] w2: [-6.51325402] bias: [16.38636109] loss: 50.96079961191086\nEpoch: 557 / 1000\nw1: [14.49488621] w2: [-6.50841552] bias: [16.40892832] loss: 50.92184078662725\nEpoch: 558 / 1000\nw1: [14.50407014] w2: [-6.45761376] bias: [16.56764004] loss: 50.84450222645542\nEpoch: 559 / 1000\nw1: [14.51010214] w2: [-6.45469439] bias: [16.57938231] loss: 50.832971905969444\nEpoch: 560 / 1000\nw1: [14.57023477] w2: [-6.43850724] bias: [16.68682229] loss: 50.728115763652674\nEpoch: 561 / 1000\nw1: [14.57271575] w2: [-6.43658008] bias: [16.69216176] loss: 50.726360722199054\nEpoch: 562 / 1000\nw1: [14.52895445] w2: [-6.56910495] bias: [16.53344724] loss: 50.67198324545381\nEpoch: 563 / 1000\nw1: [14.63416437] w2: [-6.54207771] bias: [16.68842624] loss: 50.489032452070056\nEpoch: 564 / 1000\nw1: [14.64133421] w2: [-6.53764963] bias: [16.70309474] loss: 50.482813451776714\nEpoch: 565 / 1000\nw1: [14.71997642] w2: [-6.50902029] bias: [16.81207873] loss: 50.42896883998888\nEpoch: 566 / 1000\nw1: [14.80591524] w2: [-6.46221067] bias: [16.94197005] loss: 50.45960779968945\nEpoch: 567 / 1000\nw1: [14.70385304] w2: [-6.5533767] bias: [16.75467658] loss: 50.37635671554231\nEpoch: 568 / 1000\nw1: [14.63159337] w2: [-6.61349349] bias: [16.61902074] loss: 50.41291279236696\nEpoch: 569 / 1000\nw1: [14.56280333] w2: [-6.71344599] bias: [16.41822925] loss: 50.52824017492795\nEpoch: 570 / 1000\nw1: [14.54335945] w2: [-6.72797872] bias: [16.37848087] loss: 50.58420344288302\nEpoch: 571 / 1000\nw1: [14.54346993] w2: [-6.7279589] bias: [16.37867191] loss: 50.583827344583156\nEpoch: 572 / 1000\nw1: [14.54754663] w2: [-6.72714529] bias: [16.38559328] loss: 50.57029827894872\nEpoch: 573 / 1000\nw1: [14.52069538] w2: [-6.74876388] bias: [16.33287068] loss: 50.65102466632517\nEpoch: 574 / 1000\nw1: [14.47119437] w2: [-6.77596739] bias: [16.21522691] loss: 50.87579685711825\nEpoch: 575 / 1000\nw1: [14.4430198] w2: [-6.80525506] bias: [16.14408849] loss: 51.016205716678755\nEpoch: 576 / 1000\nw1: [14.41219523] w2: [-6.86429042] bias: [16.05179161] loss: 51.191342031124535\nEpoch: 577 / 1000\nw1: [14.41198055] w2: [-6.86454346] bias: [16.05124349] loss: 51.19264472705084\nEpoch: 578 / 1000\nw1: [14.38503865] w2: [-6.8757943] bias: [16.00361144] loss: 51.33836325122691\nEpoch: 579 / 1000\nw1: [14.34738207] w2: [-6.98983191] bias: [15.86703765] loss: 51.63879455072076\nEpoch: 580 / 1000\nw1: [14.41626014] w2: [-6.97786896] bias: [15.98746466] loss: 51.206163417185\nEpoch: 581 / 1000\nw1: [14.38843498] w2: [-6.99986934] bias: [15.93511444] loss: 51.36747582648715\nEpoch: 582 / 1000\nw1: [14.39096396] w2: [-6.99781468] bias: [15.94111934] loss: 51.349447858454475\nEpoch: 583 / 1000\nw1: [14.76132519] w2: [-6.98248383] bias: [16.38917573] loss: 49.896733024570395\nEpoch: 584 / 1000\nw1: [14.7751815] w2: [-6.97568347] bias: [16.41858446] loss: 49.85401574929166\nEpoch: 585 / 1000\nw1: [14.78289313] w2: [-6.97438717] bias: [16.43260779] loss: 49.83044840308053\nEpoch: 586 / 1000\nw1: [14.76259913] w2: [-6.99071797] bias: [16.38051023] loss: 49.89348079668587\nEpoch: 587 / 1000\nw1: [14.74032707] w2: [-7.00411039] bias: [16.3369428] loss: 49.96202864894063\nEpoch: 588 / 1000\nw1: [14.74917632] w2: [-7.00262285] bias: [16.35303487] loss: 49.93045061134388\nEpoch: 589 / 1000\nw1: [14.71731303] w2: [-7.00936467] bias: [16.30159828] loss: 50.03795860328441\nEpoch: 590 / 1000\nw1: [14.72364377] w2: [-7.00631917] bias: [16.31335215] loss: 50.0159669535413\nEpoch: 591 / 1000\nw1: [14.69737193] w2: [-7.0251924] bias: [16.24943012] loss: 50.1248965111115\nEpoch: 592 / 1000\nw1: [14.71218793] w2: [-7.0193263] bias: [16.28115959] loss: 50.06179926782033\nEpoch: 593 / 1000\nw1: [14.68039402] w2: [-7.04076023] bias: [16.21015727] loss: 50.19554016262707\nEpoch: 594 / 1000\nw1: [14.63752401] w2: [-7.10789936] bias: [16.11650361] loss: 50.35837248902606\nEpoch: 595 / 1000\nw1: [14.73149262] w2: [-7.10028011] bias: [16.26990436] loss: 49.94627589135389\nEpoch: 596 / 1000\nw1: [14.77568198] w2: [-7.09309867] bias: [16.33545986] loss: 49.79429145194367\nEpoch: 597 / 1000\nw1: [14.74406683] w2: [-7.15364815] bias: [16.2407958] loss: 49.903659670512155\nEpoch: 598 / 1000\nw1: [14.72939278] w2: [-7.15906904] bias: [16.21551225] loss: 49.962094673589974\nEpoch: 599 / 1000\nw1: [14.73110073] w2: [-7.15880406] bias: [16.21877858] loss: 49.95423741325814\nEpoch: 600 / 1000\nw1: [14.71070599] w2: [-7.22313503] bias: [16.13808095] loss: 50.054528258694454\nEpoch: 601 / 1000\nw1: [14.61325927] w2: [-7.35702479] bias: [15.94179596] loss: 50.526540319915625\nEpoch: 602 / 1000\nw1: [14.61623967] w2: [-7.35462891] bias: [15.94878683] loss: 50.50576627489137\nEpoch: 603 / 1000\nw1: [14.6134113] w2: [-7.369884] bias: [15.92320403] loss: 50.558968314592214\nEpoch: 604 / 1000\nw1: [14.60189899] w2: [-7.37761371] bias: [15.89521954] loss: 50.646202699839364\nEpoch: 605 / 1000\nw1: [14.56478009] w2: [-7.40214827] bias: [15.82640123] loss: 50.88808183365116\nEpoch: 606 / 1000\nw1: [14.49794459] w2: [-7.45129784] bias: [15.69387293] loss: 51.39233530851987\nEpoch: 607 / 1000\nw1: [14.52080259] w2: [-7.4289988] bias: [15.74378752] loss: 51.20313753417262\nEpoch: 608 / 1000\nw1: [14.51110816] w2: [-7.43235961] bias: [15.72338623] loss: 51.284404792692975\nEpoch: 609 / 1000\nw1: [14.49413652] w2: [-7.44602389] bias: [15.69006231] loss: 51.41730524263056\nEpoch: 610 / 1000\nw1: [14.50162398] w2: [-7.44414936] bias: [15.70252707] loss: 51.361909664853485\nEpoch: 611 / 1000\nw1: [14.52482492] w2: [-7.42250595] bias: [15.75300049] loss: 51.17152275333362\nEpoch: 612 / 1000\nw1: [14.42699481] w2: [-7.49193313] bias: [15.59284511] loss: 51.87326151504632\nEpoch: 613 / 1000\nw1: [14.43231301] w2: [-7.49051455] bias: [15.60196028] loss: 51.829520845925735\nEpoch: 614 / 1000\nw1: [14.39900248] w2: [-7.54268249] bias: [15.52919021] loss: 52.13892618879382\nEpoch: 615 / 1000\nw1: [14.41293907] w2: [-7.53282658] bias: [15.56088305] loss: 51.99549939050287\nEpoch: 616 / 1000\nw1: [14.3373567] w2: [-7.59234892] bias: [15.41424203] loss: 52.71514210169648\nEpoch: 617 / 1000\nw1: [14.23292676] w2: [-7.74757607] bias: [15.1895842] loss: 53.92569751081944\nEpoch: 618 / 1000\nw1: [14.24035358] w2: [-7.7447752] bias: [15.20236803] loss: 53.844640307664115\nEpoch: 619 / 1000\nw1: [14.2588099] w2: [-7.74076551] bias: [15.22988904] loss: 53.66365315069463\nEpoch: 620 / 1000\nw1: [14.35191563] w2: [-7.71475043] bias: [15.36692199] loss: 52.80515148703963\nEpoch: 621 / 1000\nw1: [14.39443527] w2: [-7.69527144] bias: [15.45428812] loss: 52.34738461845148\nEpoch: 622 / 1000\nw1: [14.38915878] w2: [-7.69767226] bias: [15.44395491] loss: 52.40081608685561\nEpoch: 623 / 1000\nw1: [14.39245688] w2: [-7.69507198] bias: [15.45188707] loss: 52.36231492211449\nEpoch: 624 / 1000\nw1: [14.38712258] w2: [-7.69775803] bias: [15.43982482] loss: 52.42246363040597\nEpoch: 625 / 1000\nw1: [14.33607065] w2: [-7.74023095] bias: [15.34398307] loss: 52.936496832751374\nEpoch: 626 / 1000\nw1: [14.33190505] w2: [-7.74446311] bias: [15.3347437] loss: 52.985603228647385\nEpoch: 627 / 1000\nw1: [14.30480405] w2: [-7.77591832] bias: [15.27193726] loss: 53.323268114663485\nEpoch: 628 / 1000\nw1: [14.38429885] w2: [-7.76510081] bias: [15.41046257] loss: 52.51470763212247\nEpoch: 629 / 1000\nw1: [14.37153657] w2: [-7.76845708] bias: [15.38623328] loss: 52.6472341048093\nEpoch: 630 / 1000\nw1: [14.39318338] w2: [-7.75478599] bias: [15.43102896] loss: 52.41284142576776\nEpoch: 631 / 1000\nw1: [14.45260244] w2: [-7.73363833] bias: [15.52449131] loss: 51.903147907947734\nEpoch: 632 / 1000\nw1: [14.43899886] w2: [-7.74020439] bias: [15.49456] loss: 52.04721304222564\nEpoch: 633 / 1000\nw1: [14.41172339] w2: [-7.75429586] bias: [15.43652873] loss: 52.33725227012016\nEpoch: 634 / 1000\nw1: [14.51945835] w2: [-7.73045235] bias: [15.61109215] loss: 51.42010278357536\nEpoch: 635 / 1000\nw1: [14.49903042] w2: [-7.75168733] bias: [15.55951335] loss: 51.64002187004562\nEpoch: 636 / 1000\nw1: [14.43343156] w2: [-7.80054076] bias: [15.44886032] loss: 52.20582138144005\nEpoch: 637 / 1000\nw1: [14.57479719] w2: [-7.77372891] bias: [15.64756413] loss: 51.126608133613324\nEpoch: 638 / 1000\nw1: [14.76474273] w2: [-7.75868031] bias: [15.86746781] loss: 50.02778144988423\nEpoch: 639 / 1000\nw1: [14.75813264] w2: [-7.76119901] bias: [15.85480796] loss: 50.0740425392424\nEpoch: 640 / 1000\nw1: [14.75953242] w2: [-7.7608191] bias: [15.85709879] loss: 50.065099995386774\nEpoch: 641 / 1000\nw1: [15.12917381] w2: [-7.74336771] bias: [16.29936345] loss: 48.47965650118274\nEpoch: 642 / 1000\nw1: [14.9682537] w2: [-7.89258475] bias: [16.04198003] loss: 49.06136314366486\nEpoch: 643 / 1000\nw1: [14.95667676] w2: [-7.89737336] bias: [16.01836923] loss: 49.13128374510178\nEpoch: 644 / 1000\nw1: [14.95009725] w2: [-7.89874823] bias: [16.00664562] loss: 49.169352192268015\nEpoch: 645 / 1000\nw1: [14.96871134] w2: [-7.8922413] bias: [16.03699455] loss: 49.07132997527599\nEpoch: 646 / 1000\nw1: [14.98609483] w2: [-7.8808871] bias: [16.07396457] loss: 48.97045694796454\nEpoch: 647 / 1000\nw1: [15.00149863] w2: [-7.87469804] bias: [16.10239188] loss: 48.88978081647246\nEpoch: 648 / 1000\nw1: [15.02047864] w2: [-7.8628849] bias: [16.14559148] loss: 48.78358069275848\nEpoch: 649 / 1000\nw1: [15.03652718] w2: [-7.85240264] bias: [16.17972243] loss: 48.70349513172345\nEpoch: 650 / 1000\nw1: [15.02611075] w2: [-7.85722845] bias: [16.15996112] loss: 48.75272313640611\nEpoch: 651 / 1000\nw1: [14.9641677] w2: [-7.89740824] bias: [16.03121529] loss: 49.08813086891491\nEpoch: 652 / 1000\nw1: [14.96001264] w2: [-7.90048799] bias: [16.02261004] loss: 49.11236076721662\nEpoch: 653 / 1000\nw1: [14.96522957] w2: [-7.8914731] bias: [16.03585282] loss: 49.0815465078239\nEpoch: 654 / 1000\nw1: [14.99670756] w2: [-7.8780862] bias: [16.09853256] loss: 48.90331588627236\nEpoch: 655 / 1000\nw1: [14.90791224] w2: [-7.92817543] bias: [15.9775976] loss: 49.298052350549156\nEpoch: 656 / 1000\nw1: [14.92793162] w2: [-7.92374077] bias: [16.01420628] loss: 49.17616916943062\nEpoch: 657 / 1000\nw1: [14.90565515] w2: [-7.9397438] bias: [15.96000537] loss: 49.334328713791315\nEpoch: 658 / 1000\nw1: [14.89344197] w2: [-7.95016906] bias: [15.93307654] loss: 49.41708608544873\nEpoch: 659 / 1000\nw1: [14.97533996] w2: [-7.9393885] bias: [16.04406734] loss: 48.99912649713602\nEpoch: 660 / 1000\nw1: [14.96070858] w2: [-7.95447653] bias: [15.99144075] loss: 49.1313909478794\nEpoch: 661 / 1000\nw1: [14.91203925] w2: [-7.99042761] bias: [15.8934071] loss: 49.44170348131694\nEpoch: 662 / 1000\nw1: [15.02332889] w2: [-7.97074162] bias: [16.04391775] loss: 48.87334883806178\nEpoch: 663 / 1000\nw1: [15.38389963] w2: [-7.95581604] bias: [16.48012978] loss: 47.64199180756252\nEpoch: 664 / 1000\nw1: [15.58450884] w2: [-7.93138497] bias: [16.78970385] loss: 47.320964483834494\nEpoch: 665 / 1000\nw1: [15.50835924] w2: [-7.99398867] bias: [16.64710435] loss: 47.33324537714619\nEpoch: 666 / 1000\nw1: [15.47593481] w2: [-7.99909722] bias: [16.59799724] loss: 47.38631529885553\nEpoch: 667 / 1000\nw1: [15.46142671] w2: [-8.0122422] bias: [16.56700348] loss: 47.40376481352015\nEpoch: 668 / 1000\nw1: [15.6793206] w2: [-8.00154733] bias: [16.84786004] loss: 47.13530967683153\nEpoch: 669 / 1000\nw1: [15.66927124] w2: [-8.00384208] bias: [16.82989853] loss: 47.13720372331391\nEpoch: 670 / 1000\nw1: [15.64816595] w2: [-8.01801282] bias: [16.77859508] loss: 47.130401171086454\nEpoch: 671 / 1000\nw1: [15.6081642] w2: [-8.06410427] bias: [16.67803191] loss: 47.11478567797785\nEpoch: 672 / 1000\nw1: [15.84665946] w2: [-8.05192915] bias: [16.99776137] loss: 46.99696251610426\nEpoch: 673 / 1000\nw1: [15.8221392] w2: [-8.06629249] bias: [16.94865499] loss: 46.960268464657474\nEpoch: 674 / 1000\nw1: [15.70007203] w2: [-8.24773603] bias: [16.68605459] loss: 46.75257005311459\nEpoch: 675 / 1000\nw1: [15.68278102] w2: [-8.25822124] bias: [16.65044221] loss: 46.76750885965058\nEpoch: 676 / 1000\nw1: [15.68238641] w2: [-8.25856533] bias: [16.6494852] loss: 46.767810274829266\nEpoch: 677 / 1000\nw1: [15.59805139] w2: [-8.3431653] bias: [16.50695136] loss: 46.838808319642915\nEpoch: 678 / 1000\nw1: [15.59290285] w2: [-8.34407117] bias: [16.4974633] loss: 46.85210686842455\nEpoch: 679 / 1000\nw1: [15.68159254] w2: [-8.33601558] bias: [16.62169075] loss: 46.673202328486\nEpoch: 680 / 1000\nw1: [15.66053736] w2: [-8.35172212] bias: [16.58050372] loss: 46.69853221761772\nEpoch: 681 / 1000\nw1: [15.90197998] w2: [-8.30315384] bias: [16.8987884] loss: 46.4982952652547\nEpoch: 682 / 1000\nw1: [15.85597146] w2: [-8.37520822] bias: [16.79827837] loss: 46.40392775989471\nEpoch: 683 / 1000\nw1: [15.78026303] w2: [-8.43744922] bias: [16.65650503] loss: 46.41016874410839\nEpoch: 684 / 1000\nw1: [15.80823672] w2: [-8.4279202] bias: [16.70626464] loss: 46.37980634721754\nEpoch: 685 / 1000\nw1: [15.79957424] w2: [-8.42989826] bias: [16.69078194] loss: 46.38964057550577\nEpoch: 686 / 1000\nw1: [15.79876373] w2: [-8.43072579] bias: [16.68870939] loss: 46.38986831392938\nEpoch: 687 / 1000\nw1: [16.02811739] w2: [-8.41864278] bias: [16.99492499] loss: 46.27263019612651\nEpoch: 688 / 1000\nw1: [15.99918753] w2: [-8.42933006] bias: [16.94507852] loss: 46.24690287519367\nEpoch: 689 / 1000\nw1: [15.98921637] w2: [-8.4520493] bias: [16.91676541] loss: 46.20536426532727\nEpoch: 690 / 1000\nw1: [15.94323069] w2: [-8.52127955] bias: [16.78554657] loss: 46.10923781490838\nEpoch: 691 / 1000\nw1: [15.95029831] w2: [-8.51904903] bias: [16.79972801] loss: 46.106964689170184\nEpoch: 692 / 1000\nw1: [15.95866204] w2: [-8.51685627] bias: [16.81565296] loss: 46.104410433978046\nEpoch: 693 / 1000\nw1: [15.91860424] w2: [-8.54314417] bias: [16.71814286] loss: 46.104711431038865\nEpoch: 694 / 1000\nw1: [15.94523804] w2: [-8.53407157] bias: [16.76551908] loss: 46.088761521897844\nEpoch: 695 / 1000\nw1: [15.96365628] w2: [-8.51718376] bias: [16.80831729] loss: 46.09920334676039\nEpoch: 696 / 1000\nw1: [16.04307923] w2: [-8.50847895] bias: [16.92429612] loss: 46.08181920880831\nEpoch: 697 / 1000\nw1: [16.01999683] w2: [-8.51750125] bias: [16.87607074] loss: 46.06714655491755\nEpoch: 698 / 1000\nw1: [15.91861406] w2: [-8.59471753] bias: [16.69424369] loss: 46.03605830270568\nEpoch: 699 / 1000\nw1: [15.85534593] w2: [-8.66160619] bias: [16.56398872] loss: 46.06974272636895\nEpoch: 700 / 1000\nw1: [15.79695996] w2: [-8.70081537] bias: [16.45707069] loss: 46.175407146129636\nEpoch: 701 / 1000\nw1: [15.75552815] w2: [-8.72013564] bias: [16.37677637] loss: 46.29212765780586\nEpoch: 702 / 1000\nw1: [15.84870499] w2: [-8.66938358] bias: [16.5176075] loss: 46.09852365889885\nEpoch: 703 / 1000\nw1: [15.80654493] w2: [-8.73099727] bias: [16.43195809] loss: 46.14987162222736\nEpoch: 704 / 1000\nw1: [15.82259853] w2: [-8.71602139] bias: [16.46688253] loss: 46.11420589148063\nEpoch: 705 / 1000\nw1: [16.13181164] w2: [-8.70016829] bias: [16.93396885] loss: 45.721623045750384\nEpoch: 706 / 1000\nw1: [16.10555993] w2: [-8.7099634] bias: [16.88606406] loss: 45.708266695216004\nEpoch: 707 / 1000\nw1: [16.08482442] w2: [-8.7188181] bias: [16.83845359] loss: 45.700421282268955\nEpoch: 708 / 1000\nw1: [16.42250431] w2: [-8.69688561] bias: [17.24192231] loss: 45.87441644374239\nEpoch: 709 / 1000\nw1: [16.39507044] w2: [-8.73515182] bias: [17.17159942] loss: 45.724301239496704\nEpoch: 710 / 1000\nw1: [16.23495913] w2: [-8.8071158] bias: [16.94888596] loss: 45.49328275986738\nEpoch: 711 / 1000\nw1: [16.1824695] w2: [-8.84180999] bias: [16.85157038] loss: 45.442078876630404\nEpoch: 712 / 1000\nw1: [16.17135534] w2: [-8.84721284] bias: [16.83115334] loss: 45.43887497772829\nEpoch: 713 / 1000\nw1: [16.21938214] w2: [-8.8388714] bias: [16.91512382] loss: 45.438725038250695\nEpoch: 714 / 1000\nw1: [16.14991866] w2: [-8.91044984] bias: [16.76500792] loss: 45.36057910597823\nEpoch: 715 / 1000\nw1: [16.10030484] w2: [-8.9432432] bias: [16.67302407] loss: 45.37767339226601\nEpoch: 716 / 1000\nw1: [16.32681317] w2: [-8.93131009] bias: [16.97544079] loss: 45.254413429712585\nEpoch: 717 / 1000\nw1: [16.30829486] w2: [-8.93418577] bias: [16.94337474] loss: 45.24448957700217\nEpoch: 718 / 1000\nw1: [16.32013056] w2: [-8.93104147] bias: [16.96255814] loss: 45.251832917281206\nEpoch: 719 / 1000\nw1: [16.28812755] w2: [-8.94648841] bias: [16.89214342] loss: 45.21797593615323\nEpoch: 720 / 1000\nw1: [16.36132481] w2: [-8.94055337] bias: [17.0116356] loss: 45.24270462109578\nEpoch: 721 / 1000\nw1: [16.35345614] w2: [-8.94687883] bias: [16.99317871] loss: 45.2249662061424\nEpoch: 722 / 1000\nw1: [16.2620631] w2: [-9.02760706] bias: [16.78360916] loss: 45.09644605857053\nEpoch: 723 / 1000\nw1: [16.26040943] w2: [-9.02901266] bias: [16.77987463] loss: 45.095665429815305\nEpoch: 724 / 1000\nw1: [16.23502891] w2: [-9.04056086] bias: [16.73017072] loss: 45.10317319137132\nEpoch: 725 / 1000\nw1: [16.16534255] w2: [-9.17706136] bias: [16.57697226] loss: 45.04405323278222\nEpoch: 726 / 1000\nw1: [16.13517608] w2: [-9.20306839] bias: [16.5175838] loss: 45.08632938104737\nEpoch: 727 / 1000\nw1: [16.12615926] w2: [-9.20822654] bias: [16.49996539] loss: 45.10466246768229\nEpoch: 728 / 1000\nw1: [16.14564417] w2: [-9.17434975] bias: [16.55514269] loss: 45.08326105299712\nEpoch: 729 / 1000\nw1: [16.09074566] w2: [-9.2352232] bias: [16.41217096] loss: 45.198575078468515\nEpoch: 730 / 1000\nw1: [16.0732001] w2: [-9.24831166] bias: [16.37784927] loss: 45.24487203399748\nEpoch: 731 / 1000\nw1: [16.05870643] w2: [-9.29402891] bias: [16.32050094] loss: 45.28516033101559\nEpoch: 732 / 1000\nw1: [16.18681519] w2: [-9.28821412] bias: [16.50535015] loss: 44.93001809684691\nEpoch: 733 / 1000\nw1: [16.24645823] w2: [-9.27467959] bias: [16.60022276] loss: 44.81790997354473\nEpoch: 734 / 1000\nw1: [16.24800171] w2: [-9.27328657] bias: [16.60459357] loss: 44.81584864699029\nEpoch: 735 / 1000\nw1: [16.56158531] w2: [-9.25140109] bias: [16.98590598] loss: 44.62359211565958\nEpoch: 736 / 1000\nw1: [16.52434828] w2: [-9.30289152] bias: [16.88917124] loss: 44.517703533104026\nEpoch: 737 / 1000\nw1: [16.43126126] w2: [-9.37619906] bias: [16.70856858] loss: 44.46536277768384\nEpoch: 738 / 1000\nw1: [16.42711094] w2: [-9.38307969] bias: [16.6949798] loss: 44.46208324621616\nEpoch: 739 / 1000\nw1: [16.38672231] w2: [-9.39994583] bias: [16.62357457] loss: 44.499156491707424\nEpoch: 740 / 1000\nw1: [16.35245487] w2: [-9.40895759] bias: [16.55851755] loss: 44.55460803707709\nEpoch: 741 / 1000\nw1: [16.35488824] w2: [-9.40723672] bias: [16.56405121] loss: 44.551038387396176\nEpoch: 742 / 1000\nw1: [16.34243721] w2: [-9.41851791] bias: [16.53745197] loss: 44.56635703500346\nEpoch: 743 / 1000\nw1: [16.59139393] w2: [-9.30773681] bias: [17.09894943] loss: 44.60090387862949\nEpoch: 744 / 1000\nw1: [16.56611855] w2: [-9.32807619] bias: [17.03406394] loss: 44.52938607017829\nEpoch: 745 / 1000\nw1: [16.57254226] w2: [-9.30734093] bias: [17.057058] loss: 44.57623649095216\nEpoch: 746 / 1000\nw1: [16.84295955] w2: [-9.28666554] bias: [17.39919323] loss: 44.98522135122226\nEpoch: 747 / 1000\nw1: [16.91687169] w2: [-9.27265995] bias: [17.52769006] loss: 45.2682176328319\nEpoch: 748 / 1000\nw1: [16.82087742] w2: [-9.38769415] bias: [17.29850702] loss: 44.63761279786605\nEpoch: 749 / 1000\nw1: [16.79940495] w2: [-9.39324572] bias: [17.26186058] loss: 44.5788468758498\nEpoch: 750 / 1000\nw1: [16.77294912] w2: [-9.41636741] bias: [17.20730784] loss: 44.47342732579831\nEpoch: 751 / 1000\nw1: [16.76166693] w2: [-9.41968691] bias: [17.18507154] loss: 44.44439748045087\nEpoch: 752 / 1000\nw1: [16.83693544] w2: [-9.40976937] bias: [17.29989947] loss: 44.59841337923415\nEpoch: 753 / 1000\nw1: [16.83570016] w2: [-9.41219288] bias: [17.29678501] loss: 44.58960984306662\nEpoch: 754 / 1000\nw1: [16.82460437] w2: [-9.41498478] bias: [17.27650877] loss: 44.55699003870488\nEpoch: 755 / 1000\nw1: [16.8268748] w2: [-9.41341849] bias: [17.28203038] loss: 44.56724818872959\nEpoch: 756 / 1000\nw1: [16.86724756] w2: [-9.40833316] bias: [17.34856195] loss: 44.67344485118749\nEpoch: 757 / 1000\nw1: [16.88306504] w2: [-9.39899562] bias: [17.38267409] loss: 44.74649161314984\nEpoch: 758 / 1000\nw1: [16.95335207] w2: [-9.38719641] bias: [17.48165629] loss: 44.95642275164021\nEpoch: 759 / 1000\nw1: [17.00987528] w2: [-9.33288799] bias: [17.59463776] loss: 45.31916199638422\nEpoch: 760 / 1000\nw1: [17.00595929] w2: [-9.33480395] bias: [17.58722749] loss: 45.297142034123915\nEpoch: 761 / 1000\nw1: [16.97973864] w2: [-9.34158313] bias: [17.54247747] loss: 45.17739470621869\nEpoch: 762 / 1000\nw1: [16.96722047] w2: [-9.35604515] bias: [17.51572384] loss: 45.08925126350283\nEpoch: 763 / 1000\nw1: [16.94088354] w2: [-9.36372113] bias: [17.46613782] loss: 44.970374204339706\nEpoch: 764 / 1000\nw1: [16.92563372] w2: [-9.41716568] bias: [17.39318752] loss: 44.73298553659857\nEpoch: 765 / 1000\nw1: [16.89208674] w2: [-9.48141501] bias: [17.29273905] loss: 44.45479058636392\nEpoch: 766 / 1000\nw1: [17.02682787] w2: [-9.46473179] bias: [17.48840236] loss: 44.83286020446514\nEpoch: 767 / 1000\nw1: [17.00248484] w2: [-9.47036435] bias: [17.44222051] loss: 44.728321051966745\nEpoch: 768 / 1000\nw1: [16.97118991] w2: [-9.47885797] bias: [17.39100437] loss: 44.616286257517814\nEpoch: 769 / 1000\nw1: [16.92881672] w2: [-9.49477689] bias: [17.29580607] loss: 44.433443619599075\nEpoch: 770 / 1000\nw1: [16.8984323] w2: [-9.50137799] bias: [17.25049856] loss: 44.35961237195842\nEpoch: 771 / 1000\nw1: [16.85918435] w2: [-9.55564906] bias: [17.14853984] loss: 44.14851123707089\nEpoch: 772 / 1000\nw1: [16.85210317] w2: [-9.56738865] bias: [17.12535496] loss: 44.10641345581012\nEpoch: 773 / 1000\nw1: [16.79258575] w2: [-9.62100194] bias: [17.00040668] loss: 43.936321865513776\nEpoch: 774 / 1000\nw1: [16.81394299] w2: [-9.61834163] bias: [17.03243641] loss: 43.95401226667682\nEpoch: 775 / 1000\nw1: [16.82973021] w2: [-9.59089371] bias: [17.07714261] loss: 44.02891261353923\nEpoch: 776 / 1000\nw1: [16.80443109] w2: [-9.59776003] bias: [17.03573899] loss: 43.993383617000255\nEpoch: 777 / 1000\nw1: [16.807164] w2: [-9.59704353] bias: [17.0409426] loss: 43.99743545051083\nEpoch: 778 / 1000\nw1: [16.77390206] w2: [-9.62382353] bias: [16.97563257] loss: 43.925171331644364\nEpoch: 779 / 1000\nw1: [16.77661346] w2: [-9.62281225] bias: [16.98034321] loss: 43.92824497043142\nEpoch: 780 / 1000\nw1: [16.74538279] w2: [-9.63975683] bias: [16.92037527] loss: 43.887285475389\nEpoch: 781 / 1000\nw1: [16.64637187] w2: [-9.75217262] bias: [16.70470834] loss: 43.769563237063196\nEpoch: 782 / 1000\nw1: [16.6056793] w2: [-9.75858384] bias: [16.64307905] loss: 43.81412475761621\nEpoch: 783 / 1000\nw1: [16.68261984] w2: [-9.7501511] bias: [16.7554329] loss: 43.73703962606376\nEpoch: 784 / 1000\nw1: [16.66160852] w2: [-9.75489216] bias: [16.71832358] loss: 43.7507592336876\nEpoch: 785 / 1000\nw1: [16.65118733] w2: [-9.7610033] bias: [16.69913226] loss: 43.75455870291186\nEpoch: 786 / 1000\nw1: [16.67716936] w2: [-9.75738228] bias: [16.7433161] loss: 43.73211761418921\nEpoch: 787 / 1000\nw1: [16.60895918] w2: [-9.81413008] bias: [16.61526252] loss: 43.751374574212974\nEpoch: 788 / 1000\nw1: [16.5830036] w2: [-9.8282126] bias: [16.56542361] loss: 43.78554424212521\nEpoch: 789 / 1000\nw1: [16.58849787] w2: [-9.82690249] bias: [16.57583559] loss: 43.775596986017675\nEpoch: 790 / 1000\nw1: [16.54742485] w2: [-9.86337381] bias: [16.48512035] loss: 43.838227005889046\nEpoch: 791 / 1000\nw1: [16.47809512] w2: [-9.92037078] bias: [16.35529193] loss: 43.99047894330432\nEpoch: 792 / 1000\nw1: [16.46411057] w2: [-9.93259291] bias: [16.32645535] loss: 44.031834418849634\nEpoch: 793 / 1000\nw1: [16.45818092] w2: [-9.93536684] bias: [16.31308738] loss: 44.05413171339547\nEpoch: 794 / 1000\nw1: [16.46974852] w2: [-9.93078688] bias: [16.33786017] loss: 44.01189969954152\nEpoch: 795 / 1000\nw1: [16.41949607] w2: [-9.95840334] bias: [16.21843051] loss: 44.22084204719618\nEpoch: 796 / 1000\nw1: [16.76399378] w2: [-9.93602803] bias: [16.63004534] loss: 43.44138144912223\nEpoch: 797 / 1000\nw1: [16.64953111] w2: [-10.04292002] bias: [16.42985074] loss: 43.5597010736848\nEpoch: 798 / 1000\nw1: [16.61428311] w2: [-10.08383118] bias: [16.34816365] loss: 43.64420393124479\nEpoch: 799 / 1000\nw1: [16.55941304] w2: [-10.13714165] bias: [16.22992794] loss: 43.82273298775691\nEpoch: 800 / 1000\nw1: [16.53163228] w2: [-10.17555618] bias: [16.15775881] loss: 43.94349642234905\nEpoch: 801 / 1000\nw1: [16.56551643] w2: [-10.1700495] bias: [16.2080264] loss: 43.815566091279855\nEpoch: 802 / 1000\nw1: [16.57635605] w2: [-10.16812708] bias: [16.22477358] loss: 43.77587631149006\nEpoch: 803 / 1000\nw1: [16.58340661] w2: [-10.16623637] bias: [16.23834175] loss: 43.74743991627855\nEpoch: 804 / 1000\nw1: [16.71921125] w2: [-10.13856315] bias: [16.43120281] loss: 43.368548842034244\nEpoch: 805 / 1000\nw1: [16.71614793] w2: [-10.13940543] bias: [16.42562254] loss: 43.37630357434717\nEpoch: 806 / 1000\nw1: [16.62644902] w2: [-10.22382732] bias: [16.26095896] loss: 43.59821856500541\nEpoch: 807 / 1000\nw1: [16.58306898] w2: [-10.27234571] bias: [16.15516435] loss: 43.78155743518949\nEpoch: 808 / 1000\nw1: [16.92748595] w2: [-10.24075025] bias: [16.57612739] loss: 42.92728802460017\nEpoch: 809 / 1000\nw1: [16.92910401] w2: [-10.23937492] bias: [16.5797815] loss: 42.92582501264786\nEpoch: 810 / 1000\nw1: [16.9271863] w2: [-10.24008203] bias: [16.57627589] loss: 42.928323842534425\nEpoch: 811 / 1000\nw1: [16.89581273] w2: [-10.29273642] bias: [16.4926931] loss: 42.944791118882534\nEpoch: 812 / 1000\nw1: [17.01392878] w2: [-10.2873752] bias: [16.66312375] loss: 42.76352790582462\nEpoch: 813 / 1000\nw1: [16.96152667] w2: [-10.3109913] bias: [16.5669611] loss: 42.813639099319786\nEpoch: 814 / 1000\nw1: [17.09861678] w2: [-10.29187807] bias: [16.776225] loss: 42.688465200477246\nEpoch: 815 / 1000\nw1: [17.14944709] w2: [-10.28034337] bias: [16.85707943] loss: 42.691367987960774\nEpoch: 816 / 1000\nw1: [17.05237995] w2: [-10.39055223] bias: [16.64564645] loss: 42.60456011063643\nEpoch: 817 / 1000\nw1: [17.04878578] w2: [-10.39167268] bias: [16.63865763] loss: 42.608130028029315\nEpoch: 818 / 1000\nw1: [17.02079898] w2: [-10.41379306] bias: [16.57553615] loss: 42.629478221250835\nEpoch: 819 / 1000\nw1: [17.01386439] w2: [-10.42062173] bias: [16.55992285] loss: 42.63538932921149\nEpoch: 820 / 1000\nw1: [17.00998412] w2: [-10.42127399] bias: [16.55286673] loss: 42.6419442397991\nEpoch: 821 / 1000\nw1: [16.95895897] w2: [-10.48359818] bias: [16.44810658] loss: 42.69537658782044\nEpoch: 822 / 1000\nw1: [16.96386015] w2: [-10.48272895] bias: [16.45567889] loss: 42.68468005858289\nEpoch: 823 / 1000\nw1: [16.96315879] w2: [-10.48292948] bias: [16.45426234] loss: 42.68637249159791\nEpoch: 824 / 1000\nw1: [16.88035257] w2: [-10.62992419] bias: [16.26562606] loss: 42.84770176022748\nEpoch: 825 / 1000\nw1: [16.89356988] w2: [-10.62484287] bias: [16.29250882] loss: 42.79871993161053\nEpoch: 826 / 1000\nw1: [16.92349046] w2: [-10.62067294] bias: [16.34339039] loss: 42.70103226679144\nEpoch: 827 / 1000\nw1: [16.95132133] w2: [-10.60667417] bias: [16.4037851] loss: 42.615818352369836\nEpoch: 828 / 1000\nw1: [16.93968148] w2: [-10.61783546] bias: [16.37353192] loss: 42.649863480414446\nEpoch: 829 / 1000\nw1: [16.95564523] w2: [-10.61366487] bias: [16.40463114] loss: 42.60252319850385\nEpoch: 830 / 1000\nw1: [16.95945381] w2: [-10.60498703] bias: [16.41544561] loss: 42.59634542444252\nEpoch: 831 / 1000\nw1: [16.95492612] w2: [-10.60944556] bias: [16.40525147] loss: 42.60708395969661\nEpoch: 832 / 1000\nw1: [16.94887664] w2: [-10.61076419] bias: [16.39264276] loss: 42.62566013210844\nEpoch: 833 / 1000\nw1: [16.89203484] w2: [-10.72210498] bias: [16.26768177] loss: 42.74607318195267\nEpoch: 834 / 1000\nw1: [16.8766103] w2: [-10.72558542] bias: [16.24043958] loss: 42.804147014964926\nEpoch: 835 / 1000\nw1: [16.92825741] w2: [-10.70805605] bias: [16.32867076] loss: 42.6291206312752\nEpoch: 836 / 1000\nw1: [16.92819003] w2: [-10.70809042] bias: [16.32852711] loss: 42.629353462975374\nEpoch: 837 / 1000\nw1: [16.97320053] w2: [-10.7002279] bias: [16.41040575] loss: 42.488248856190616\nEpoch: 838 / 1000\nw1: [16.94599553] w2: [-10.71317426] bias: [16.35183194] loss: 42.57315806962629\nEpoch: 839 / 1000\nw1: [17.01319797] w2: [-10.70950057] bias: [16.45833955] loss: 42.3857850077344\nEpoch: 840 / 1000\nw1: [17.34330742] w2: [-10.66841803] bias: [16.81451925] loss: 42.02511128569397\nEpoch: 841 / 1000\nw1: [17.41058456] w2: [-10.66296301] bias: [16.92434702] loss: 42.03788646575086\nEpoch: 842 / 1000\nw1: [17.32333037] w2: [-10.75118478] bias: [16.77591821] loss: 41.9225827033207\nEpoch: 843 / 1000\nw1: [17.27682705] w2: [-10.80832549] bias: [16.67021227] loss: 41.89346994353531\nEpoch: 844 / 1000\nw1: [17.28647172] w2: [-10.80202597] bias: [16.6907239] loss: 41.889348447578385\nEpoch: 845 / 1000\nw1: [17.25852717] w2: [-10.81234926] bias: [16.64257513] loss: 41.91141800710586\nEpoch: 846 / 1000\nw1: [17.21389879] w2: [-10.84293922] bias: [16.5614199] loss: 41.95107072650308\nEpoch: 847 / 1000\nw1: [17.20568736] w2: [-10.84775453] bias: [16.54629801] loss: 41.962188016659795\nEpoch: 848 / 1000\nw1: [17.28442487] w2: [-10.79779574] bias: [16.70483623] loss: 41.89424852672146\nEpoch: 849 / 1000\nw1: [17.35913075] w2: [-10.78534875] bias: [16.805975] loss: 41.8574442513879\nEpoch: 850 / 1000\nw1: [17.3420698] w2: [-10.80268234] bias: [16.76813348] loss: 41.84231587932895\nEpoch: 851 / 1000\nw1: [17.28790279] w2: [-10.83211446] bias: [16.67178119] loss: 41.85505145991637\nEpoch: 852 / 1000\nw1: [17.23589872] w2: [-10.87895972] bias: [16.56260611] loss: 41.88818226055304\nEpoch: 853 / 1000\nw1: [17.20302269] w2: [-10.91684067] bias: [16.47995678] loss: 41.93361721001635\nEpoch: 854 / 1000\nw1: [17.17580903] w2: [-10.94393625] bias: [16.42243225] loss: 41.98437812108423\nEpoch: 855 / 1000\nw1: [17.15817675] w2: [-10.95459182] bias: [16.38795384] loss: 42.02798395484253\nEpoch: 856 / 1000\nw1: [17.1549108] w2: [-10.95659798] bias: [16.38156517] loss: 42.036515685832526\nEpoch: 857 / 1000\nw1: [17.18277831] w2: [-10.95312674] bias: [16.42335843] loss: 41.966024206381206\nEpoch: 858 / 1000\nw1: [17.36170272] w2: [-10.94913432] bias: [16.61887905] loss: 41.66579140817651\nEpoch: 859 / 1000\nw1: [17.36391447] w2: [-10.94757018] bias: [16.62390872] loss: 41.66413088558548\nEpoch: 860 / 1000\nw1: [17.30801755] w2: [-10.98886015] bias: [16.51131667] loss: 41.72248299514062\nEpoch: 861 / 1000\nw1: [17.40064704] w2: [-10.98789712] bias: [16.65091624] loss: 41.57781161537177\nEpoch: 862 / 1000\nw1: [17.47920289] w2: [-10.97754644] bias: [16.77075926] loss: 41.52322197287377\nEpoch: 863 / 1000\nw1: [17.42561106] w2: [-11.07803551] bias: [16.62878172] loss: 41.45665748178372\nEpoch: 864 / 1000\nw1: [17.37175821] w2: [-11.10763059] bias: [16.50079537] loss: 41.535554955855375\nEpoch: 865 / 1000\nw1: [17.33581032] w2: [-11.11708428] bias: [16.43254799] loss: 41.61456389053251\nEpoch: 866 / 1000\nw1: [17.33419966] w2: [-11.11736341] bias: [16.42969849] loss: 41.61845672608899\nEpoch: 867 / 1000\nw1: [17.60674687] w2: [-11.09652518] bias: [16.77452853] loss: 41.28756879668962\nEpoch: 868 / 1000\nw1: [17.60290571] w2: [-11.09720641] bias: [16.76859393] loss: 41.28816900628695\nEpoch: 869 / 1000\nw1: [17.5988982] w2: [-11.10042796] bias: [16.75919385] loss: 41.285302450665306\nEpoch: 870 / 1000\nw1: [17.65592998] w2: [-11.09731026] bias: [16.8495822] loss: 41.278474495097775\nEpoch: 871 / 1000\nw1: [17.62384494] w2: [-11.12497132] bias: [16.78641669] loss: 41.242374376482985\nEpoch: 872 / 1000\nw1: [17.64744793] w2: [-11.12020573] bias: [16.83222702] loss: 41.2464763278226\nEpoch: 873 / 1000\nw1: [17.73520979] w2: [-11.11218443] bias: [16.95861475] loss: 41.27884605264587\nEpoch: 874 / 1000\nw1: [17.73203668] w2: [-11.11786979] bias: [16.95054436] loss: 41.26665665587785\nEpoch: 875 / 1000\nw1: [17.7200669] w2: [-11.12501794] bias: [16.92389322] loss: 41.245189327759384\nEpoch: 876 / 1000\nw1: [17.76264922] w2: [-11.11537047] bias: [16.99125822] loss: 41.28795896248109\nEpoch: 877 / 1000\nw1: [17.76231538] w2: [-11.11549499] bias: [16.99067821] loss: 41.28744657522837\nEpoch: 878 / 1000\nw1: [17.7274222] w2: [-11.14468128] bias: [16.90981344] loss: 41.20678405639929\nEpoch: 879 / 1000\nw1: [17.70367914] w2: [-11.16239289] bias: [16.86336852] loss: 41.17156563043016\nEpoch: 880 / 1000\nw1: [17.69944472] w2: [-11.16352841] bias: [16.85521975] loss: 41.16892501538796\nEpoch: 881 / 1000\nw1: [17.67663215] w2: [-11.18057896] bias: [16.80858489] loss: 41.14361049700408\nEpoch: 882 / 1000\nw1: [17.65348346] w2: [-11.19130348] bias: [16.76466886] loss: 41.135041710660126\nEpoch: 883 / 1000\nw1: [17.58360453] w2: [-11.24455878] bias: [16.63156735] loss: 41.12771228964525\nEpoch: 884 / 1000\nw1: [17.80883757] w2: [-11.1992512] bias: [16.92848355] loss: 41.10799192233082\nEpoch: 885 / 1000\nw1: [17.74020526] w2: [-11.35001702] bias: [16.73916431] loss: 40.87647965402625\nEpoch: 886 / 1000\nw1: [17.72768066] w2: [-11.35118312] bias: [16.71644415] loss: 40.882239560966056\nEpoch: 887 / 1000\nw1: [17.81224571] w2: [-11.32945932] bias: [16.84101231] loss: 40.88432558667699\nEpoch: 888 / 1000\nw1: [17.733246] w2: [-11.38268445] bias: [16.70970674] loss: 40.839956983652854\nEpoch: 889 / 1000\nw1: [17.69156221] w2: [-11.38955755] bias: [16.64600317] loss: 40.8687140547111\nEpoch: 890 / 1000\nw1: [17.61454426] w2: [-11.42944674] bias: [16.50565517] loss: 40.94564774715005\nEpoch: 891 / 1000\nw1: [17.59338637] w2: [-11.4403298] bias: [16.45779083] loss: 40.98614421136928\nEpoch: 892 / 1000\nw1: [17.673101] w2: [-11.42694797] bias: [16.5700495] loss: 40.862803395228084\nEpoch: 893 / 1000\nw1: [17.64648515] w2: [-11.44798478] bias: [16.51002007] loss: 40.892194213391946\nEpoch: 894 / 1000\nw1: [17.81624185] w2: [-11.44419692] bias: [16.69552263] loss: 40.707412322193214\nEpoch: 895 / 1000\nw1: [17.82467068] w2: [-11.43838219] bias: [16.71602125] loss: 40.70978803671169\nEpoch: 896 / 1000\nw1: [17.66886274] w2: [-11.60598974] bias: [16.46356499] loss: 40.73695459472983\nEpoch: 897 / 1000\nw1: [17.89650015] w2: [-11.5601985] bias: [16.76365078] loss: 40.51982222035378\nEpoch: 898 / 1000\nw1: [17.88405154] w2: [-11.56602204] bias: [16.73558629] loss: 40.51660924088465\nEpoch: 899 / 1000\nw1: [17.93522682] w2: [-11.5596154] bias: [16.81620318] loss: 40.51149687223387\nEpoch: 900 / 1000\nw1: [17.93883515] w2: [-11.55782672] bias: [16.82439808] loss: 40.5146109335801\nEpoch: 901 / 1000\nw1: [17.94619889] w2: [-11.55590292] bias: [16.8387435] loss: 40.518511709476186\nEpoch: 902 / 1000\nw1: [17.92910826] w2: [-11.57038091] bias: [16.7992937] loss: 40.49605736835842\nEpoch: 903 / 1000\nw1: [17.8338004] w2: [-11.67376035] bias: [16.61243729] loss: 40.43854930416705\nEpoch: 904 / 1000\nw1: [17.86469218] w2: [-11.66295453] bias: [16.66336093] loss: 40.41629436937862\nEpoch: 905 / 1000\nw1: [17.89964589] w2: [-11.62507603] bias: [16.74985852] loss: 40.43609269458896\nEpoch: 906 / 1000\nw1: [17.95110496] w2: [-11.58064067] bias: [16.88131464] loss: 40.50132959447366\nEpoch: 907 / 1000\nw1: [18.28964948] w2: [-11.52185936] bias: [17.25438556] loss: 40.916815328993074\nEpoch: 908 / 1000\nw1: [18.27037559] w2: [-11.52715881] bias: [17.21927546] loss: 40.851675192695865\nEpoch: 909 / 1000\nw1: [18.54660408] w2: [-11.50831285] bias: [17.52692303] loss: 41.57912415601086\nEpoch: 910 / 1000\nw1: [18.47316888] w2: [-11.55532347] bias: [17.39444521] loss: 41.16217307632746\nEpoch: 911 / 1000\nw1: [18.4934702] w2: [-11.55062885] bias: [17.42902505] loss: 41.25113662718241\nEpoch: 912 / 1000\nw1: [18.46437301] w2: [-11.55727314] bias: [17.3770188] loss: 41.12025073488389\nEpoch: 913 / 1000\nw1: [18.37511913] w2: [-11.64680742] bias: [17.22617164] loss: 40.678704826516594\nEpoch: 914 / 1000\nw1: [18.32230556] w2: [-11.66886217] bias: [17.13279969] loss: 40.50783760390825\nEpoch: 915 / 1000\nw1: [18.30554558] w2: [-11.67492437] bias: [17.1037011] loss: 40.46251163744994\nEpoch: 916 / 1000\nw1: [18.30489418] w2: [-11.6755182] bias: [17.10217591] loss: 40.4597891182136\nEpoch: 917 / 1000\nw1: [18.31147059] w2: [-11.66415413] bias: [17.11886962] loss: 40.497579119993595\nEpoch: 918 / 1000\nw1: [18.35855999] w2: [-11.659529] bias: [17.1943718] loss: 40.60859544441361\nEpoch: 919 / 1000\nw1: [18.29909214] w2: [-11.66889831] bias: [17.10430717] loss: 40.47278196797542\nEpoch: 920 / 1000\nw1: [18.21061084] w2: [-11.75836079] bias: [16.9537909] loss: 40.20808796220971\nEpoch: 921 / 1000\nw1: [18.1857174] w2: [-11.76600193] bias: [16.9066163] loss: 40.17242711353695\nEpoch: 922 / 1000\nw1: [18.15329396] w2: [-11.79395473] bias: [16.84278457] loss: 40.11309681814887\nEpoch: 923 / 1000\nw1: [18.11734264] w2: [-11.80870729] bias: [16.77014425] loss: 40.085650863946164\nEpoch: 924 / 1000\nw1: [18.09519787] w2: [-11.82347918] bias: [16.7271801] loss: 40.071295946042916\nEpoch: 925 / 1000\nw1: [18.16833394] w2: [-11.81564031] bias: [16.84408582] loss: 40.07887901616548\nEpoch: 926 / 1000\nw1: [18.12638291] w2: [-11.853418] bias: [16.7671289] loss: 40.024550066275246\nEpoch: 927 / 1000\nw1: [18.08877445] w2: [-11.91003657] bias: [16.65981421] loss: 39.970232944689144\nEpoch: 928 / 1000\nw1: [18.08877445] w2: [-11.87252201] bias: [16.91204568] loss: 40.07113611345855\nEpoch: 929 / 1000\nw1: [18.06844239] w2: [-11.87786845] bias: [16.87435011] loss: 40.05718707332443\nEpoch: 930 / 1000\nw1: [18.03833381] w2: [-11.88910264] bias: [16.81940721] loss: 40.041824680161845\nEpoch: 931 / 1000\nw1: [17.80253727] w2: [-11.91226588] bias: [16.58361067] loss: 40.22442652878149\nEpoch: 932 / 1000\nw1: [17.71549607] w2: [-11.99001455] bias: [16.42388211] loss: 40.359824779220936\nEpoch: 933 / 1000\nw1: [17.70902274] w2: [-11.99316138] bias: [16.41199042] loss: 40.37637434864707\nEpoch: 934 / 1000\nw1: [17.67359655] w2: [-12.03016071] bias: [16.32126948] loss: 40.49111382416492\nEpoch: 935 / 1000\nw1: [17.75053371] w2: [-12.02626002] bias: [16.43809667] loss: 40.27352267940261\nEpoch: 936 / 1000\nw1: [17.75694214] w2: [-12.02261626] bias: [16.45329229] loss: 40.25582722269652\nEpoch: 937 / 1000\nw1: [17.72280443] w2: [-12.0533579] bias: [16.3906685] loss: 40.329952989363896\nEpoch: 938 / 1000\nw1: [17.7554732] w2: [-12.04173086] bias: [16.44205436] loss: 40.25050717257696\nEpoch: 939 / 1000\nw1: [18.05395331] w2: [-12.02089947] bias: [16.80500118] loss: 39.870355228513766\nEpoch: 940 / 1000\nw1: [18.04183739] w2: [-12.02547286] bias: [16.78030078] loss: 39.86973505436824\nEpoch: 941 / 1000\nw1: [18.10493294] w2: [-12.01974196] bias: [16.86867858] loss: 39.85837974936446\nEpoch: 942 / 1000\nw1: [18.10493294] w2: [-11.98280016] bias: [17.1170591] loss: 40.05198521936576\nEpoch: 943 / 1000\nw1: [18.09529474] w2: [-11.98751364] bias: [17.09610002] loss: 40.03073305691662\nEpoch: 944 / 1000\nw1: [18.08350705] w2: [-11.99093501] bias: [17.07543494] loss: 40.01360234087066\nEpoch: 945 / 1000\nw1: [18.06879401] w2: [-11.99745171] bias: [17.04188872] loss: 39.986306769613996\nEpoch: 946 / 1000\nw1: [18.04020024] w2: [-12.01048751] bias: [16.97881563] loss: 39.94447947305957\nEpoch: 947 / 1000\nw1: [18.13480437] w2: [-11.99910943] bias: [17.11269279] loss: 40.01846398416506\nEpoch: 948 / 1000\nw1: [18.26319536] w2: [-11.98321247] bias: [17.2991348] loss: 40.237973445006226\nEpoch: 949 / 1000\nw1: [18.23502681] w2: [-12.01262863] bias: [17.23017998] loss: 40.1048254796606\nEpoch: 950 / 1000\nw1: [18.21445631] w2: [-12.01454385] bias: [17.19286422] loss: 40.06108568766639\nEpoch: 951 / 1000\nw1: [18.20745898] w2: [-12.01963574] bias: [17.17738348] loss: 40.03786468699332\nEpoch: 952 / 1000\nw1: [18.19152013] w2: [-12.03407705] bias: [17.14333319] loss: 39.985250733814404\nEpoch: 953 / 1000\nw1: [18.17499921] w2: [-12.04214953] bias: [17.11250635] loss: 39.94887672815988\nEpoch: 954 / 1000\nw1: [18.19037123] w2: [-11.98776847] bias: [17.17215434] loss: 40.0811168624181\nEpoch: 955 / 1000\nw1: [18.17121736] w2: [-12.00221838] bias: [17.12629929] loss: 40.01849858823709\nEpoch: 956 / 1000\nw1: [18.20562159] w2: [-11.99768961] bias: [17.17292502] loss: 40.06549993922807\nEpoch: 957 / 1000\nw1: [18.22392408] w2: [-11.98090793] bias: [17.21545425] loss: 40.13573488396725\nEpoch: 958 / 1000\nw1: [18.21594182] w2: [-11.98267615] bias: [17.20085741] loss: 40.11696613821805\nEpoch: 959 / 1000\nw1: [18.18551913] w2: [-11.99526] bias: [17.13881129] loss: 40.037635132251445\nEpoch: 960 / 1000\nw1: [18.19564376] w2: [-11.93925469] bias: [17.31377963] loss: 40.323721660135696\nEpoch: 961 / 1000\nw1: [18.18852861] w2: [-11.94269827] bias: [17.29992884] loss: 40.2996568651564\nEpoch: 962 / 1000\nw1: [18.2087166] w2: [-11.91725407] bias: [17.37507944] loss: 40.447255094665444\nEpoch: 963 / 1000\nw1: [18.14852299] w2: [-11.92673772] bias: [17.28391564] loss: 40.30371106267868\nEpoch: 964 / 1000\nw1: [18.12799491] w2: [-11.94412768] bias: [17.23653127] loss: 40.22202031767518\nEpoch: 965 / 1000\nw1: [18.11114698] w2: [-11.95200925] bias: [17.19854883] loss: 40.17126108204598\nEpoch: 966 / 1000\nw1: [18.02588758] w2: [-12.07116483] bias: [17.00798404] loss: 39.88606556908987\nEpoch: 967 / 1000\nw1: [18.19043476] w2: [-12.03830272] bias: [17.21225895] loss: 40.049633467374335\nEpoch: 968 / 1000\nw1: [18.09219531] w2: [-12.14486202] bias: [17.01965501] loss: 39.76960676072307\nEpoch: 969 / 1000\nw1: [18.1000269] w2: [-12.1422071] bias: [17.03171195] loss: 39.776760435228006\nEpoch: 970 / 1000\nw1: [18.10905] w2: [-12.13841644] bias: [17.05020098] loss: 39.78950726317287\nEpoch: 971 / 1000\nw1: [18.08752223] w2: [-12.15026873] bias: [16.99972775] loss: 39.75428875928857\nEpoch: 972 / 1000\nw1: [18.09685134] w2: [-12.14440358] bias: [17.01959254] loss: 39.76836246276179\nEpoch: 973 / 1000\nw1: [18.05719976] w2: [-12.20763621] bias: [16.914812] loss: 39.66566084158683\nEpoch: 974 / 1000\nw1: [18.05296265] w2: [-12.21041411] bias: [16.90581546] loss: 39.66244924351365\nEpoch: 975 / 1000\nw1: [18.0033038] w2: [-12.30352853] bias: [16.77425731] loss: 39.5823023890599\nEpoch: 976 / 1000\nw1: [17.92125446] w2: [-12.39237376] bias: [16.61156145] loss: 39.62409376790737\nEpoch: 977 / 1000\nw1: [17.94156656] w2: [-12.37255835] bias: [16.65591663] loss: 39.599532198256696\nEpoch: 978 / 1000\nw1: [17.84553491] w2: [-12.44277803] bias: [16.46657429] loss: 39.77922140314688\nEpoch: 979 / 1000\nw1: [17.84834564] w2: [-12.44238574] bias: [16.47114555] loss: 39.77162105182115\nEpoch: 980 / 1000\nw1: [17.82149781] w2: [-12.45534441] bias: [16.41207355] loss: 39.857527661757146\nEpoch: 981 / 1000\nw1: [17.86737978] w2: [-12.4501238] bias: [16.47513859] loss: 39.738814284994675\nEpoch: 982 / 1000\nw1: [17.85238737] w2: [-12.45426772] bias: [16.44860574] loss: 39.780035521045704\nEpoch: 983 / 1000\nw1: [17.77456595] w2: [-12.51627804] bias: [16.31157807] loss: 40.00576976374514\nEpoch: 984 / 1000\nw1: [17.77797798] w2: [-12.51353518] bias: [16.31958139] loss: 39.9914430125992\nEpoch: 985 / 1000\nw1: [17.73379356] w2: [-12.60008288] bias: [16.2224464] loss: 40.15246900842903\nEpoch: 986 / 1000\nw1: [17.76863213] w2: [-12.5620541] bias: [16.29229754] loss: 40.01174869009326\nEpoch: 987 / 1000\nw1: [17.75565673] w2: [-12.56919782] bias: [16.26187587] loss: 40.07330595499894\nEpoch: 988 / 1000\nw1: [17.78318624] w2: [-12.53450064] bias: [16.36435556] loss: 39.907949006432325\nEpoch: 989 / 1000\nw1: [17.83149792] w2: [-12.52792647] bias: [16.4485421] loss: 39.747129550587154\nEpoch: 990 / 1000\nw1: [17.81960035] w2: [-12.53477682] bias: [16.41926654] loss: 39.79045906984652\nEpoch: 991 / 1000\nw1: [18.03988651] w2: [-12.52353127] bias: [16.71458466] loss: 39.338237717692095\nEpoch: 992 / 1000\nw1: [17.99734796] w2: [-12.5412952] bias: [16.63937847] loss: 39.39378048934076\nEpoch: 993 / 1000\nw1: [18.02200136] w2: [-12.53631753] bias: [16.68722751] loss: 39.351930888749685\nEpoch: 994 / 1000\nw1: [17.99502549] w2: [-12.57574069] bias: [16.63242523] loss: 39.36784711783042\nEpoch: 995 / 1000\nw1: [17.97708867] w2: [-12.58229238] bias: [16.59461739] loss: 39.404281703164685\nEpoch: 996 / 1000\nw1: [17.91846458] w2: [-12.68362159] bias: [16.42950181] loss: 39.5399028689925\nEpoch: 997 / 1000\nw1: [17.8424977] w2: [-12.77465607] bias: [16.2481335] loss: 39.83721580946657\nEpoch: 998 / 1000\nw1: [17.76479966] w2: [-12.84328738] bias: [16.0699674] loss: 40.26472612830842\nEpoch: 999 / 1000\nw1: [17.75331654] w2: [-12.85396945] bias: [16.04682825] loss: 40.330413722335166\nEpoch: 1000 / 1000\nw1: [17.68174465] w2: [-12.90923203] bias: [15.92888265] loss: 40.72337937459625\n##### 최종 w1, w2, bias #######\n[17.68174465] [-12.90923203] [15.92888265]\n","output_type":"stream"}]},{"cell_type":"code","source":"predicted = scaled_features[:, 0]*w1 + scaled_features[:, 1]*w2 + bias\nbostonDF['PREDICTED_PRICE_SGD'] = predicted\nbostonDF.head(10)","metadata":{"execution":{"iopub.status.busy":"2024-05-18T06:47:05.677259Z","iopub.execute_input":"2024-05-18T06:47:05.677637Z","iopub.status.idle":"2024-05-18T06:47:05.708336Z","shell.execute_reply.started":"2024-05-18T06:47:05.677608Z","shell.execute_reply":"2024-05-18T06:47:05.707186Z"},"trusted":true},"execution_count":9,"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"      CRIM    ZN  INDUS  CHAS    NOX     RM    AGE     DIS  RAD    TAX  \\\n0  0.00632  18.0   2.31   0.0  0.538  6.575   65.2  4.0900  1.0  296.0   \n1  0.02731   0.0   7.07   0.0  0.469  6.421   78.9  4.9671  2.0  242.0   \n2  0.02729   0.0   7.07   0.0  0.469  7.185   61.1  4.9671  2.0  242.0   \n3  0.03237   0.0   2.18   0.0  0.458  6.998   45.8  6.0622  3.0  222.0   \n4  0.06905   0.0   2.18   0.0  0.458  7.147   54.2  6.0622  3.0  222.0   \n5  0.02985   0.0   2.18   0.0  0.458  6.430   58.7  6.0622  3.0  222.0   \n6  0.08829  12.5   7.87   0.0  0.524  6.012   66.6  5.5605  5.0  311.0   \n7  0.14455  12.5   7.87   0.0  0.524  6.172   96.1  5.9505  5.0  311.0   \n8  0.21124  12.5   7.87   0.0  0.524  5.631  100.0  6.0821  5.0  311.0   \n9  0.17004  12.5   7.87   0.0  0.524  6.004   85.9  6.5921  5.0  311.0   \n\n   PTRATIO       B  LSTAT  PRICE  PREDICTED_PRICE_SGD  \n0     15.3  396.90   4.98   24.0            24.982485  \n1     17.8  396.90   9.14   21.6            22.978885  \n2     17.8  392.83   4.03   34.7            27.387542  \n3     18.7  394.63   2.94   33.4            27.142269  \n4     18.7  396.90   5.33   36.2            26.795720  \n5     18.7  394.12   5.21   28.7            24.409301  \n6     15.2  395.60  12.43   22.9            20.421263  \n7     15.2  396.90  19.15   27.1            18.569571  \n8     15.2  386.63  29.93   16.5            12.896689  \n9     15.2  386.71  17.10   18.9            18.730636  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>CRIM</th>\n      <th>ZN</th>\n      <th>INDUS</th>\n      <th>CHAS</th>\n      <th>NOX</th>\n      <th>RM</th>\n      <th>AGE</th>\n      <th>DIS</th>\n      <th>RAD</th>\n      <th>TAX</th>\n      <th>PTRATIO</th>\n      <th>B</th>\n      <th>LSTAT</th>\n      <th>PRICE</th>\n      <th>PREDICTED_PRICE_SGD</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.00632</td>\n      <td>18.0</td>\n      <td>2.31</td>\n      <td>0.0</td>\n      <td>0.538</td>\n      <td>6.575</td>\n      <td>65.2</td>\n      <td>4.0900</td>\n      <td>1.0</td>\n      <td>296.0</td>\n      <td>15.3</td>\n      <td>396.90</td>\n      <td>4.98</td>\n      <td>24.0</td>\n      <td>24.982485</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.02731</td>\n      <td>0.0</td>\n      <td>7.07</td>\n      <td>0.0</td>\n      <td>0.469</td>\n      <td>6.421</td>\n      <td>78.9</td>\n      <td>4.9671</td>\n      <td>2.0</td>\n      <td>242.0</td>\n      <td>17.8</td>\n      <td>396.90</td>\n      <td>9.14</td>\n      <td>21.6</td>\n      <td>22.978885</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.02729</td>\n      <td>0.0</td>\n      <td>7.07</td>\n      <td>0.0</td>\n      <td>0.469</td>\n      <td>7.185</td>\n      <td>61.1</td>\n      <td>4.9671</td>\n      <td>2.0</td>\n      <td>242.0</td>\n      <td>17.8</td>\n      <td>392.83</td>\n      <td>4.03</td>\n      <td>34.7</td>\n      <td>27.387542</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.03237</td>\n      <td>0.0</td>\n      <td>2.18</td>\n      <td>0.0</td>\n      <td>0.458</td>\n      <td>6.998</td>\n      <td>45.8</td>\n      <td>6.0622</td>\n      <td>3.0</td>\n      <td>222.0</td>\n      <td>18.7</td>\n      <td>394.63</td>\n      <td>2.94</td>\n      <td>33.4</td>\n      <td>27.142269</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.06905</td>\n      <td>0.0</td>\n      <td>2.18</td>\n      <td>0.0</td>\n      <td>0.458</td>\n      <td>7.147</td>\n      <td>54.2</td>\n      <td>6.0622</td>\n      <td>3.0</td>\n      <td>222.0</td>\n      <td>18.7</td>\n      <td>396.90</td>\n      <td>5.33</td>\n      <td>36.2</td>\n      <td>26.795720</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>0.02985</td>\n      <td>0.0</td>\n      <td>2.18</td>\n      <td>0.0</td>\n      <td>0.458</td>\n      <td>6.430</td>\n      <td>58.7</td>\n      <td>6.0622</td>\n      <td>3.0</td>\n      <td>222.0</td>\n      <td>18.7</td>\n      <td>394.12</td>\n      <td>5.21</td>\n      <td>28.7</td>\n      <td>24.409301</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>0.08829</td>\n      <td>12.5</td>\n      <td>7.87</td>\n      <td>0.0</td>\n      <td>0.524</td>\n      <td>6.012</td>\n      <td>66.6</td>\n      <td>5.5605</td>\n      <td>5.0</td>\n      <td>311.0</td>\n      <td>15.2</td>\n      <td>395.60</td>\n      <td>12.43</td>\n      <td>22.9</td>\n      <td>20.421263</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>0.14455</td>\n      <td>12.5</td>\n      <td>7.87</td>\n      <td>0.0</td>\n      <td>0.524</td>\n      <td>6.172</td>\n      <td>96.1</td>\n      <td>5.9505</td>\n      <td>5.0</td>\n      <td>311.0</td>\n      <td>15.2</td>\n      <td>396.90</td>\n      <td>19.15</td>\n      <td>27.1</td>\n      <td>18.569571</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>0.21124</td>\n      <td>12.5</td>\n      <td>7.87</td>\n      <td>0.0</td>\n      <td>0.524</td>\n      <td>5.631</td>\n      <td>100.0</td>\n      <td>6.0821</td>\n      <td>5.0</td>\n      <td>311.0</td>\n      <td>15.2</td>\n      <td>386.63</td>\n      <td>29.93</td>\n      <td>16.5</td>\n      <td>12.896689</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>0.17004</td>\n      <td>12.5</td>\n      <td>7.87</td>\n      <td>0.0</td>\n      <td>0.524</td>\n      <td>6.004</td>\n      <td>85.9</td>\n      <td>6.5921</td>\n      <td>5.0</td>\n      <td>311.0</td>\n      <td>15.2</td>\n      <td>386.71</td>\n      <td>17.10</td>\n      <td>18.9</td>\n      <td>18.730636</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]}]}